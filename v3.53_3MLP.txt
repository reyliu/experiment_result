liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.53_3MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [132.0 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [65.8], Init: NDCG = 
[ 0.55336379  0.5607913   0.56926372  0.58059525  0.59366502  0.60827967
  0.62477642  0.64336341  0.66393062  0.68600377]
Iteration 0 [96.7 s]: loss = 0.692124 [68.60 s], NDCG = 
[ 0.70212885  0.69582885  0.69536268  0.69848147  0.70475722  0.71380653
  0.72433338  0.73665851  0.75122727  0.76824285]
Iteration 1 [100.9 s]: loss = 0.681646 [69.48 s], NDCG = 
[ 0.71721826  0.71368112  0.71423291  0.71803773  0.72439083  0.73382532
  0.7449142   0.75689193  0.77052777  0.78590319]
Iteration 2 [100.1 s]: loss = 0.653774 [71.07 s], NDCG = 
[ 0.71752514  0.71602444  0.71772784  0.72231443  0.72954357  0.7387322
  0.75015625  0.76299614  0.77644381  0.79103671]
Iteration 3 [100.5 s]: loss = 0.630378 [68.12 s], NDCG = 
[ 0.73140436  0.7269717   0.72741168  0.72995351  0.73646554  0.74533692
  0.7556564   0.76833671  0.78163911  0.79593041]
Iteration 4 [97.1 s]: loss = 0.621013 [67.54 s], NDCG = 
[ 0.74306622  0.73614037  0.73451976  0.73646363  0.74165854  0.75020371
  0.76037269  0.7721326   0.78511491  0.79931578]
Iteration 5 [94.3 s]: loss = 0.617773 [65.10 s], NDCG = 
[ 0.75213278  0.7418286   0.73902498  0.74014441  0.74477458  0.75273346
  0.76308481  0.77445035  0.78769791  0.80173746]
Iteration 6 [96.5 s]: loss = 0.615360 [67.30 s], NDCG = 
[ 0.75436334  0.74315623  0.74032432  0.74029493  0.74531975  0.75345866
  0.76359814  0.77500762  0.78802028  0.80213718]
Iteration 7 [95.7 s]: loss = 0.614679 [65.72 s], NDCG = 
[ 0.75704039  0.74578538  0.74200819  0.74182495  0.74634722  0.75421375
  0.76467591  0.77584853  0.78874635  0.80300132]
Iteration 8 [94.6 s]: loss = 0.613859 [64.98 s], NDCG = 
[ 0.76068776  0.74723282  0.74263698  0.74274396  0.747511    0.75488401
  0.76541574  0.77664727  0.7893868   0.80356569]
Iteration 9 [93.5 s]: loss = 0.613603 [64.89 s], NDCG = 
[ 0.76019285  0.74674245  0.74266002  0.74283708  0.74757912  0.75490052
  0.76451565  0.77649515  0.78916187  0.80347206]
Iteration 10 [95.3 s]: loss = 0.613314 [66.97 s], NDCG = 
[ 0.76227312  0.74706866  0.74308287  0.74351115  0.74799296  0.75549707
  0.76501161  0.77670712  0.78954055  0.80366277]
Iteration 11 [94.4 s]: loss = 0.613476 [65.11 s], NDCG = 
[ 0.75908543  0.74605904  0.74164027  0.74264912  0.74664772  0.75465651
  0.76450073  0.77620018  0.78928333  0.80332328]
Iteration 12 [94.7 s]: loss = 0.612540 [64.71 s], NDCG = 
[ 0.760756    0.74690406  0.74190923  0.74282053  0.74757179  0.75546727
  0.76503482  0.77673347  0.78970736  0.80365938]
Iteration 13 [94.3 s]: loss = 0.613153 [66.12 s], NDCG = 
[ 0.76182788  0.74761849  0.74343434  0.74366938  0.74788739  0.75526314
  0.76536715  0.77708616  0.7901273   0.80420364]
Iteration 14 [94.7 s]: loss = 0.612735 [66.29 s], NDCG = 
[ 0.76184455  0.74736389  0.74316498  0.74348739  0.74786987  0.75570017
  0.7654531   0.77709441  0.78992978  0.80411333]
Iteration 15 [96.2 s]: loss = 0.612657 [67.49 s], NDCG = 
[ 0.76087328  0.74819561  0.74371513  0.74411173  0.74801068  0.75526729
  0.76517171  0.77705549  0.79009897  0.80424504]
Iteration 16 [98.7 s]: loss = 0.612392 [67.33 s], NDCG = 
[ 0.76078932  0.74747555  0.74304164  0.74371806  0.74782258  0.75528465
  0.76526849  0.77686706  0.79003476  0.8041777 ]
Iteration 17 [96.3 s]: loss = 0.612263 [65.42 s], NDCG = 
[ 0.76211181  0.7482841   0.74357127  0.74371912  0.74847138  0.75586985
  0.76565679  0.77705349  0.79004657  0.80430141]
Iteration 18 [95.3 s]: loss = 0.612651 [65.58 s], NDCG = 
[ 0.76289663  0.74840512  0.74403625  0.74424183  0.74863601  0.75627981
  0.76595497  0.77704334  0.79004435  0.80440306]
Iteration 19 [96.9 s]: loss = 0.612081 [66.12 s], NDCG = 
[ 0.7620178   0.74782622  0.7435266   0.7440907   0.7481058   0.75580973
  0.76589604  0.7768934   0.79001792  0.80421319]
Iteration 20 [95.8 s]: loss = 0.612513 [67.04 s], NDCG = 
[ 0.76220929  0.74841224  0.74419178  0.74387267  0.74855017  0.75595819
  0.76585295  0.7772661   0.7899563   0.8043336 ]
Iteration 21 [95.7 s]: loss = 0.612858 [66.24 s], NDCG = 
[ 0.76220237  0.74783852  0.74381714  0.7440465   0.74834943  0.75575109
  0.76580599  0.7771828   0.79006877  0.80441188]
Iteration 22 [94.3 s]: loss = 0.612186 [66.24 s], NDCG = 
[ 0.76133297  0.74742921  0.74371676  0.74338002  0.74746464  0.75560869
  0.76549712  0.77692105  0.78999865  0.80392774]
Iteration 23 [94.2 s]: loss = 0.612123 [64.93 s], NDCG = 
[ 0.76205144  0.74814902  0.74434135  0.74413114  0.74808196  0.75608859
  0.76583559  0.77731479  0.79048041  0.80420034]
Iteration 24 [95.9 s]: loss = 0.612405 [66.91 s], NDCG = 
[ 0.76304976  0.74768005  0.74336959  0.74361028  0.74807782  0.75560602
  0.76565133  0.77702555  0.79011011  0.80405068]
Iteration 25 [96.4 s]: loss = 0.612618 [66.92 s], NDCG = 
[ 0.76286676  0.74739255  0.74333838  0.7439532   0.74842881  0.75559608
  0.76551857  0.77713041  0.7901446   0.80442242]
Iteration 26 [97.1 s]: loss = 0.612505 [67.79 s], NDCG = 
[ 0.76360346  0.74878698  0.74435026  0.74415454  0.74844632  0.75586077
  0.76591939  0.77727507  0.79032877  0.80450028]
Iteration 27 [95.7 s]: loss = 0.612418 [66.82 s], NDCG = 
[ 0.7627762   0.74785911  0.74411601  0.74400411  0.74799937  0.75588566
  0.76566707  0.77694697  0.79014876  0.80422922]
Iteration 28 [98.6 s]: loss = 0.612063 [67.56 s], NDCG = 
[ 0.7624624   0.74719447  0.74401631  0.74380379  0.74793346  0.7554428
  0.7658088   0.77727718  0.79018912  0.80423418]

