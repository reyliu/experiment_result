liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.0001 --path Data_item/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data_item/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.3 s]. #user=12922, #item=13679, #train=323272, #test=316871
Init: NDCG = 
[ 0.53648407  0.54428385  0.55532678  0.56803221  0.58117615  0.59412963
  0.60912776  0.62452325  0.64110002  0.65902879]
Iteration 0 [17.5 s]: loss = 6.3597 [20.4 s], NDCG = 
[ 0.60404651  0.60304372  0.60655713  0.61287678  0.62156068  0.6322754
  0.64483168  0.65823744  0.67341719  0.68958375]
Iteration 1 [15.5 s]: loss = 1.0960 [20.5 s], NDCG = 
[ 0.69142071  0.68305664  0.68246688  0.68571133  0.69125394  0.69892854
  0.70903251  0.72024452  0.73219248  0.74538053]
Iteration 2 [15.3 s]: loss = 1.0009 [19.0 s], NDCG = 
[ 0.69984995  0.69344423  0.6922233   0.69585869  0.70103063  0.70835431
  0.71757951  0.72780895  0.73924316  0.7524571 ]
Iteration 3 [14.8 s]: loss = 0.9802 [19.1 s], NDCG = 
[ 0.70007927  0.69514311  0.69525823  0.69728318  0.70320974  0.71087402
  0.71986367  0.73014726  0.74169645  0.75435075]
Iteration 4 [14.9 s]: loss = 0.9718 [18.8 s], NDCG = 
[ 0.70357951  0.6965844   0.69468572  0.69733142  0.7033102   0.71085766
  0.71998965  0.73039126  0.74206032  0.75461245]
Iteration 5 [15.0 s]: loss = 0.9669 [18.8 s], NDCG = 
[ 0.69925618  0.69555887  0.69446446  0.69691563  0.70256288  0.71048407
  0.71972748  0.72984805  0.74142827  0.7546122 ]
Iteration 6 [15.0 s]: loss = 0.9634 [19.0 s], NDCG = 
[ 0.70003921  0.69671412  0.69535619  0.69795465  0.70373328  0.71106614
  0.72015714  0.73051203  0.74231455  0.75470019]
Iteration 7 [14.8 s]: loss = 0.9606 [19.9 s], NDCG = 
[ 0.6998331   0.6950963   0.69492677  0.69739573  0.70290256  0.71113612
  0.72026255  0.73085875  0.74235241  0.75501033]
Iteration 8 [14.8 s]: loss = 0.9581 [20.0 s], NDCG = 
[ 0.70068868  0.69497446  0.69510282  0.69799012  0.70336164  0.71109535
  0.72047165  0.73065711  0.74221547  0.75476571]
Iteration 9 [15.6 s]: loss = 0.9558 [20.4 s], NDCG = 
[ 0.70075361  0.69674459  0.6954313   0.69711918  0.70286962  0.71123754
  0.72029826  0.73051961  0.74237277  0.7551597 ]
Iteration 10 [15.1 s]: loss = 0.9537 [20.6 s], NDCG = 
[ 0.7008747   0.69544817  0.6955609   0.69811865  0.70314259  0.71090151
  0.72030845  0.73089714  0.74252049  0.7551527 ]
Iteration 11 [16.1 s]: loss = 0.9514 [20.6 s], NDCG = 
[ 0.70230542  0.69578094  0.69527182  0.69813165  0.70291671  0.7115337
  0.72075534  0.73115187  0.74253645  0.75505983]
Iteration 12 [15.5 s]: loss = 0.9486 [20.4 s], NDCG = 
[ 0.70342193  0.6962626   0.69571116  0.69841548  0.70372635  0.71182596
  0.72089391  0.73128543  0.74263398  0.75545089]
Iteration 13 [15.7 s]: loss = 0.9449 [20.5 s], NDCG = 
[ 0.70231661  0.69594758  0.6960558   0.69819949  0.70360649  0.7113615
  0.7205965   0.73123683  0.74272723  0.75525822]
Iteration 14 [15.1 s]: loss = 0.9394 [18.4 s], NDCG = 
[ 0.70264825  0.69664286  0.69606388  0.69807251  0.70375262  0.71154525
  0.72086736  0.73124671  0.74268078  0.7553515 ]
Iteration 15 [14.9 s]: loss = 0.9306 [20.3 s], NDCG = 
[ 0.70327433  0.69669727  0.69611794  0.6989147   0.70403896  0.71172464
  0.72101352  0.73141668  0.74284174  0.75547774]
Iteration 16 [15.2 s]: loss = 0.9188 [20.8 s], NDCG = 
[ 0.70138961  0.69633744  0.69616384  0.69838515  0.7038476   0.71161372
  0.72078985  0.73116848  0.74241318  0.75527007]
Iteration 17 [15.7 s]: loss = 0.9048 [20.8 s], NDCG = 
[ 0.70176373  0.69585355  0.69551368  0.69778813  0.70327814  0.71097661
  0.72056079  0.73058382  0.74208063  0.75496908]
Iteration 18 [16.0 s]: loss = 0.8902 [20.7 s], NDCG = 
[ 0.7024141   0.694045    0.69401834  0.69673118  0.70248027  0.71043577
  0.71967704  0.72999277  0.74135374  0.75410861]
Iteration 19 [15.8 s]: loss = 0.8750 [20.8 s], NDCG = 
[ 0.70199547  0.69251191  0.6931193   0.69581773  0.70178592  0.70923512
  0.7180507   0.72895854  0.74029512  0.75290471]
Iteration 20 [16.0 s]: loss = 0.8589 [20.8 s], NDCG = 
[ 0.69978362  0.68994342  0.69084478  0.69410382  0.70035379  0.70802638
  0.71698994  0.72741773  0.73897707  0.75185147]
Iteration 21 [15.7 s]: loss = 0.8412 [20.5 s], NDCG = 
[ 0.6976542   0.68884412  0.68904947  0.69268491  0.69858969  0.70620431
  0.71520863  0.72580944  0.73756659  0.75050483]
Iteration 22 [15.9 s]: loss = 0.8211 [20.5 s], NDCG = 
[ 0.69384858  0.68569285  0.68588969  0.68987794  0.69580047  0.70344331
  0.71312874  0.723897    0.73531824  0.748374  ]
Iteration 23 [15.5 s]: loss = 0.7982 [20.5 s], NDCG = 
[ 0.68937612  0.68382651  0.68351332  0.68703324  0.6930708   0.70067346
  0.71081528  0.72199621  0.73381258  0.74713553]
Iteration 24 [15.4 s]: loss = 0.7726 [20.5 s], NDCG = 
[ 0.68615734  0.68081002  0.67993019  0.68356943  0.69061907  0.69874176
  0.70812544  0.71912209  0.73115282  0.74463785]
Iteration 25 [15.1 s]: loss = 0.7455 [20.6 s], NDCG = 
[ 0.6777475   0.67441001  0.6742412   0.67837818  0.68448615  0.69318593
  0.70330689  0.71446004  0.72661036  0.74000861]
Iteration 26 [15.5 s]: loss = 0.7171 [20.5 s], NDCG = 
[ 0.67563314  0.67245131  0.67242996  0.67624855  0.68278586  0.69133836
  0.70160616  0.71287583  0.72533516  0.73874731]
Iteration 27 [15.8 s]: loss = 0.6901 [20.7 s], NDCG = 
[ 0.66928773  0.66542527  0.66696308  0.67142624  0.67822723  0.68710778
  0.69767276  0.7090355   0.72142919  0.73512881]
Iteration 28 [15.2 s]: loss = 0.6636 [20.2 s], NDCG = 
[ 0.66412064  0.66035459  0.66130257  0.66643081  0.67433548  0.6835007
  0.69446125  0.70576195  0.71821744  0.73204431]
Iteration 29 [15.3 s]: loss = 0.6386 [20.5 s], NDCG = 
[ 0.65484319  0.65252585  0.65548824  0.66177931  0.66933954  0.67913778
  0.6901566   0.70150089  0.71445793  0.7287991 ]
Iteration 30 [15.4 s]: loss = 0.6145 [20.6 s], NDCG = 
[ 0.64808449  0.64476101  0.64939767  0.65584552  0.66508445  0.6749776
  0.68614907  0.69800862  0.71131785  0.7257214 ]
Iteration 31 [15.7 s]: loss = 0.5914 [20.5 s], NDCG = 
[ 0.6424297   0.64166994  0.64597287  0.65334167  0.6620908   0.67205217
  0.68288105  0.69567888  0.7092762   0.72366549]
Iteration 32 [15.5 s]: loss = 0.5697 [20.5 s], NDCG = 
[ 0.63361114  0.63561501  0.64194916  0.64881986  0.65846145  0.66838982
  0.67977297  0.69225663  0.70613109  0.72073965]
Iteration 33 [15.3 s]: loss = 0.5487 [20.5 s], NDCG = 
[ 0.62728061  0.62919023  0.637035    0.64469597  0.65488761  0.66515344
  0.67666941  0.68938001  0.70333755  0.71839332]
Iteration 34 [15.5 s]: loss = 0.5284 [20.2 s], NDCG = 
[ 0.62187365  0.62667464  0.63430722  0.64365474  0.65301633  0.66336149
  0.67509205  0.68789295  0.70144383  0.71664964]
Iteration 35 [15.2 s]: loss = 0.5092 [20.2 s], NDCG = 
[ 0.61428168  0.62085081  0.62904221  0.63788858  0.64802658  0.65906392
  0.67080042  0.68386659  0.69804606  0.71312282]
Iteration 36 [14.9 s]: loss = 0.4908 [20.1 s], NDCG = 
[ 0.6114545   0.61862102  0.62751664  0.63633726  0.64672463  0.65756248
  0.66968127  0.68292587  0.69706235  0.71201336]
Iteration 37 [15.2 s]: loss = 0.4736 [20.1 s], NDCG = 
[ 0.60697028  0.61455034  0.62278284  0.63189052  0.64243096  0.65380235
  0.6664642   0.68022587  0.69433211  0.7096243 ]
Iteration 38 [14.8 s]: loss = 0.4573 [20.1 s], NDCG = 
[ 0.60755699  0.61459956  0.62310691  0.63301141  0.64295839  0.65449435
  0.66696031  0.6803581   0.69432703  0.70987562]
Iteration 39 [15.4 s]: loss = 0.4422 [20.1 s], NDCG = 
[ 0.60068694  0.61074928  0.61833425  0.6282505   0.63980399  0.65197342
  0.66420369  0.67757167  0.69202515  0.70759003]
Iteration 40 [15.9 s]: loss = 0.4279 [20.1 s], NDCG = 
[ 0.59842555  0.60758244  0.61598244  0.62633157  0.63741901  0.64897323
  0.66219438  0.67607436  0.69085363  0.70592536]
Iteration 41 [15.1 s]: loss = 0.4144 [20.1 s], NDCG = 
[ 0.59586064  0.6038633   0.61319908  0.62344701  0.63509883  0.64700724
  0.65943892  0.6739801   0.68866526  0.70429524]
Iteration 42 [15.2 s]: loss = 0.4016 [20.5 s], NDCG = 
[ 0.59346126  0.60498169  0.61283648  0.62350819  0.63496156  0.64676092
  0.65925472  0.67345995  0.68800839  0.70359887]
Iteration 43 [15.2 s]: loss = 0.3900 [20.6 s], NDCG = 
[ 0.59313834  0.60144042  0.60955601  0.62145772  0.63309383  0.64563173
  0.65809936  0.67192349  0.6867155   0.70238067]
Iteration 44 [15.8 s]: loss = 0.3786 [20.5 s], NDCG = 
[ 0.59187385  0.60275983  0.61111756  0.62175167  0.63356349  0.64550745
  0.65851991  0.67261069  0.68706901  0.7025753 ]
Iteration 45 [15.8 s]: loss = 0.3680 [21.1 s], NDCG = 
[ 0.5924942   0.59949627  0.60871083  0.62017616  0.63169498  0.64375288
  0.65667453  0.67074609  0.6858723   0.70140373]
Iteration 46 [16.2 s]: loss = 0.3579 [21.0 s], NDCG = 
[ 0.58975536  0.59798467  0.60827774  0.61875061  0.63031681  0.64256023
  0.6554466   0.66943697  0.68486964  0.70067855]
Iteration 47 [16.0 s]: loss = 0.3482 [20.4 s], NDCG = 
[ 0.5906154   0.5984611   0.60863097  0.61897115  0.63080033  0.64275605
  0.65604247  0.66975498  0.68484912  0.70112708]
Iteration 48 [15.1 s]: loss = 0.3392 [20.2 s], NDCG = 
[ 0.58591597  0.59692283  0.60593656  0.61669486  0.62918878  0.64151047
  0.65441414  0.66875183  0.68349694  0.69955363]
Iteration 49 [14.8 s]: loss = 0.3306 [20.0 s], NDCG = 
[ 0.58597588  0.59595565  0.60546491  0.61700466  0.62882469  0.64089357
  0.65362382  0.66802162  0.68315505  0.69911553]
Iteration 50 [15.1 s]: loss = 0.3220 [20.2 s], NDCG = 
[ 0.58256778  0.59187019  0.60235918  0.61418896  0.62595957  0.63834483
  0.65171202  0.66570961  0.68104781  0.69710461]



liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [192.0 s]. #user=12922, #item=13679, #train=19937524, #test=316871
Time: [80.6], Init: NDCG = 
[ 0.53943633  0.54956983  0.56064187  0.57119663  0.58352027  0.59677291
  0.61134144  0.6261819   0.64279635  0.66059015]
Iteration 0 [124.3 s]: loss = 0.692841 [79.94 s], NDCG = 
[ 0.60216771  0.60553237  0.61254179  0.62134094  0.63147298  0.64276899
  0.6558021   0.66932421  0.68426735  0.70002605]
Iteration 1 [125.7 s]: loss = 0.689246 [79.13 s], NDCG = 
[ 0.63909798  0.64187271  0.64707589  0.65453067  0.66394099  0.67386614
  0.68546677  0.69786924  0.71127723  0.72551196]
Iteration 2 [126.3 s]: loss = 0.674995 [77.46 s], NDCG = 
[ 0.64642572  0.64960106  0.65615412  0.6628978   0.67285316  0.68266263
  0.69420016  0.70620314  0.7191135   0.73300611]
Iteration 3 [125.9 s]: loss = 0.656430 [74.35 s], NDCG = 
[ 0.65955724  0.66163708  0.66523013  0.67194693  0.67935902  0.6893381
  0.6997235   0.71204523  0.72461903  0.73833616]
Iteration 4 [131.7 s]: loss = 0.646929 [82.58 s], NDCG = 
[ 0.66636072  0.66744028  0.66947783  0.67555267  0.6830329   0.69237379
  0.70299841  0.71461207  0.72737415  0.74133553]
Iteration 5 [126.6 s]: loss = 0.643486 [82.06 s], NDCG = 
[ 0.67406862  0.67052673  0.67284681  0.67829938  0.68485869  0.69411962
  0.7050802   0.7165439   0.72943564  0.74284144]
Iteration 6 [129.4 s]: loss = 0.641275 [82.46 s], NDCG = 
[ 0.6760383   0.67537749  0.67658485  0.68072977  0.68738879  0.69643058
  0.70714318  0.71818217  0.7308345   0.74396939]
Iteration 7 [125.2 s]: loss = 0.639235 [82.54 s], NDCG = 
[ 0.67766044  0.67558272  0.67725645  0.68094231  0.68773085  0.69670935
  0.70701983  0.71829005  0.73069656  0.74426355]
Iteration 8 [130.0 s]: loss = 0.637212 [81.16 s], NDCG = 
[ 0.68015533  0.67770066  0.67852283  0.68232798  0.68869158  0.69773157
  0.70744751  0.71914748  0.7314491   0.74473168]
Iteration 9 [121.3 s]: loss = 0.636104 [73.02 s], NDCG = 
[ 0.68161296  0.67910337  0.67820963  0.6828012   0.68906198  0.69812816
  0.70795705  0.71934668  0.73182367  0.74495364]
Iteration 10 [120.3 s]: loss = 0.634441 [76.17 s], NDCG = 
[ 0.68313125  0.6778576   0.67881175  0.6828585   0.68957616  0.69852258
  0.70822067  0.71963346  0.7321296   0.74544336]
Iteration 11 [121.0 s]: loss = 0.632454 [73.28 s], NDCG = 
[ 0.68665273  0.67997637  0.67986668  0.68423215  0.69090334  0.69859664
  0.70896941  0.71995953  0.73258172  0.74539134]
Iteration 12 [119.6 s]: loss = 0.630725 [73.72 s], NDCG = 
[ 0.68772816  0.67965708  0.68055357  0.68459079  0.69110537  0.69899872
  0.70935172  0.72003495  0.73255192  0.74569972]
Iteration 13 [124.5 s]: loss = 0.628444 [75.35 s], NDCG = 
[ 0.68857268  0.68144083  0.68082732  0.68532082  0.69107535  0.698982
  0.70938343  0.72030152  0.73252409  0.74589133]
Iteration 14 [120.7 s]: loss = 0.626558 [74.12 s], NDCG = 
[ 0.68943768  0.68236162  0.68145369  0.6848928   0.69114507  0.69939533
  0.70925086  0.72033877  0.73237373  0.74555719]
Iteration 15 [120.6 s]: loss = 0.623599 [76.43 s], NDCG = 
[ 0.69097607  0.68206616  0.68108304  0.68535     0.69125695  0.69934914
  0.70939126  0.72055132  0.7325721   0.74593116]
Iteration 16 [120.9 s]: loss = 0.621076 [73.51 s], NDCG = 
[ 0.69114255  0.68207943  0.68091676  0.68485172  0.6913475   0.69935867
  0.70931574  0.72028911  0.73231707  0.74561129]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [217.8 s]. #user=12922, #item=13679, #train=19937524, #test=316871
Time: [81.8], Init: NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 0 [106.5 s]: loss = 2.024863 [72.10 s], NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 1 [112.1 s]: loss = 2.006048 [71.94 s], NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 2 [111.5 s]: loss = 1.926693 [71.37 s], NDCG = 
[ 0.52339947  0.5366544   0.55134468  0.56349965  0.57696325  0.59094224
  0.60628184  0.6225195   0.6395734   0.65769797]
Iteration 3 [110.8 s]: loss = 1.817727 [71.76 s], NDCG = 
[ 0.63588715  0.63895803  0.64521647  0.65320458  0.66210397  0.67147448
  0.6821646   0.69422681  0.70754587  0.72255853]
Iteration 4 [111.2 s]: loss = 1.750724 [71.82 s], NDCG = 
[ 0.65740842  0.6566358   0.66027006  0.66759969  0.67558359  0.68463877
  0.69527197  0.70618485  0.71865355  0.73284018]
Iteration 5 [110.9 s]: loss = 1.716819 [71.86 s], NDCG = 
[ 0.66710921  0.66488639  0.6668975   0.67302494  0.68083005  0.68904663
  0.69955466  0.71099517  0.72299128  0.73713076]
Iteration 6 [110.6 s]: loss = 1.698641 [72.16 s], NDCG = 
[ 0.6683421   0.66735696  0.67036387  0.67557965  0.6833042   0.69164181
  0.70179757  0.71262316  0.72466242  0.73847939]
Iteration 7 [111.2 s]: loss = 1.686951 [72.10 s], NDCG = 
[ 0.67499912  0.67213071  0.67355185  0.67823344  0.68545191  0.69347946
  0.70358502  0.71437728  0.72615701  0.74019764]
Iteration 8 [110.9 s]: loss = 1.676469 [71.97 s], NDCG = 
[ 0.68046739  0.67608006  0.67650057  0.6809586   0.68674635  0.69432407
  0.70494166  0.71556169  0.72750327  0.74144951]
Iteration 9 [110.2 s]: loss = 1.663264 [71.44 s], NDCG = 
[ 0.68027483  0.67431062  0.67532563  0.68000184  0.68636691  0.69409993
  0.70440133  0.71502195  0.72696556  0.74051157]
Iteration 10 [110.5 s]: loss = 1.654861 [71.76 s], NDCG = 
[ 0.68258199  0.67688361  0.67630635  0.68046586  0.68748854  0.69480013
  0.70470716  0.71538468  0.72755718  0.74119767]
Iteration 11 [110.2 s]: loss = 1.639614 [71.52 s], NDCG = 
[ 0.68338728  0.67642868  0.67676089  0.68125557  0.687773    0.69502802
  0.70523364  0.71574522  0.72758096  0.74142428]
Iteration 12 [110.5 s]: loss = 1.632520 [71.45 s], NDCG = 
[ 0.68262492  0.67641307  0.6774604   0.68163338  0.68711446  0.69479376
  0.70465762  0.71529571  0.72739898  0.74137234]
Iteration 13 [110.4 s]: loss = 1.614364 [71.58 s], NDCG = 
[ 0.68569006  0.67805108  0.67786375  0.68155874  0.6873975   0.69450198
  0.70435545  0.71573556  0.72753218  0.74157764]
Iteration 14 [111.6 s]: loss = 1.596567 [71.62 s], NDCG = 
[ 0.68529139  0.67819127  0.67808176  0.68181607  0.68758114  0.6949839
  0.70477692  0.7155611   0.72754698  0.74128291]
Iteration 15 [110.9 s]: loss = 1.587757 [71.95 s], NDCG = 
[ 0.68507994  0.67811815  0.67800732  0.68195938  0.68727971  0.6944588
  0.70453473  0.71495465  0.72711606  0.74120987]
Iteration 16 [110.0 s]: loss = 1.564807 [71.55 s], NDCG = 
[ 0.68415122  0.67679726  0.67809041  0.68183111  0.68768388  0.69446376
  0.70418637  0.71486867  0.72664915  0.74050104]
Iteration 17 [110.0 s]: loss = 1.546997 [71.31 s], NDCG = 
[ 0.68536815  0.67824718  0.67812376  0.68187772  0.68745939  0.69486535
  0.70427972  0.71503046  0.72691699  0.74081214]
Iteration 18 [110.0 s]: loss = 1.520967 [71.73 s], NDCG = 
[ 0.68448687  0.67772504  0.67807904  0.6815384   0.68727059  0.69476413
  0.70392132  0.71435147  0.7264617   0.73993553]
Iteration 19 [109.9 s]: loss = 1.498398 [71.58 s], NDCG = 
[ 0.68591232  0.67739477  0.67867348  0.68191456  0.68710389  0.694728
  0.70382576  0.71394612  0.72578504  0.73959876]
Iteration 20 [110.3 s]: loss = 1.473190 [71.61 s], NDCG = 
[ 0.68963507  0.67990377  0.67958894  0.6830207   0.68799977  0.69525033
  0.70439238  0.71539021  0.72706669  0.74073424]
Iteration 21 [109.9 s]: loss = 1.443342 [71.75 s], NDCG = 
[ 0.68474932  0.67895969  0.67959676  0.68212367  0.68760502  0.69562247
  0.70469419  0.71450139  0.72661589  0.74030325]
Iteration 22 [126.7 s]: loss = 1.411820 [76.07 s], NDCG = 
[ 0.68585705  0.67861772  0.67813517  0.68161037  0.68752105  0.69469967
  0.70355516  0.71403713  0.72609931  0.739898  ]
Iteration 23 [122.1 s]: loss = 1.376155 [82.51 s], NDCG = 
[ 0.68602329  0.67747057  0.67816276  0.68068355  0.6867426   0.69471933
  0.70332244  0.71389322  0.72592107  0.73989536]
Iteration 24 [125.5 s]: loss = 1.342618 [82.65 s], NDCG = 
[ 0.6845801   0.67833585  0.67716935  0.67998265  0.68598484  0.69337271
  0.70265184  0.71338725  0.72514765  0.73899135]
Iteration 25 [125.6 s]: loss = 1.306473 [81.79 s], NDCG = 
[ 0.68591314  0.67789032  0.67815534  0.68053305  0.68655852  0.69420425
  0.70319701  0.71362462  0.7258296   0.73913064]
Iteration 26 [125.0 s]: loss = 1.272878 [83.33 s], NDCG = 
[ 0.68424134  0.6760396   0.67655093  0.6794011   0.68549245  0.6929524
  0.70194995  0.71262509  0.72479417  0.73824963]
Iteration 27 [129.4 s]: loss = 1.232589 [82.16 s], NDCG = 
[ 0.68083025  0.67467576  0.67491162  0.67894988  0.68411305  0.69201184
  0.70084265  0.71158853  0.72362259  0.73773444]
Iteration 28 [125.8 s]: loss = 1.198529 [82.31 s], NDCG = 
[ 0.67869059  0.67313797  0.67354545  0.67688292  0.68306602  0.68989522
  0.69948498  0.71031246  0.72222964  0.73636444]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [233.0 s]. #user=12922, #item=13679, #train=19937524, #test=316871
Time: [87.4], Init: NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 0 [131.2 s]: loss = 0.998892 [84.96 s], NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 1 [141.1 s]: loss = 0.985933 [83.54 s], NDCG = 
[ 0.52315909  0.53616379  0.55074112  0.5629081   0.57639967  0.59047863
  0.6057711   0.6220467   0.63911933  0.65723803]
Iteration 2 [144.6 s]: loss = 0.923005 [84.83 s], NDCG = 
[ 0.6416683   0.63950561  0.64348741  0.64993667  0.65753578  0.66634198
  0.67681738  0.68885371  0.70207406  0.71730848]
Iteration 3 [146.4 s]: loss = 0.856159 [75.59 s], NDCG = 
[ 0.65895928  0.65740561  0.66047063  0.66577423  0.67367881  0.68284933
  0.69269464  0.7040177   0.71651784  0.73072385]
Iteration 4 [146.1 s]: loss = 0.829389 [76.47 s], NDCG = 
[ 0.66956131  0.66601391  0.66764376  0.67312002  0.67999392  0.6893535
  0.69923028  0.71044014  0.72268662  0.73644019]
Iteration 5 [149.9 s]: loss = 0.819440 [72.84 s], NDCG = 
[ 0.67529031  0.67157428  0.67205364  0.67668198  0.68337543  0.69198928
  0.7018448   0.71358315  0.7252293   0.73898779]
Iteration 6 [149.7 s]: loss = 0.814255 [75.48 s], NDCG = 
[ 0.67776015  0.67225648  0.67293201  0.67765317  0.6851939   0.69339263
  0.70375435  0.71471669  0.72663561  0.74033873]
Iteration 7 [149.9 s]: loss = 0.810561 [73.55 s], NDCG = 
[ 0.68025511  0.67535499  0.6758293   0.68049959  0.68680784  0.69510529
  0.70467121  0.71575082  0.72784887  0.74159914]
Iteration 8 [152.4 s]: loss = 0.808785 [76.26 s], NDCG = 
[ 0.68084818  0.67499291  0.67696411  0.68028282  0.68740523  0.69588316
  0.70530814  0.71598689  0.7279697   0.74169463]
Iteration 9 [152.4 s]: loss = 0.805467 [73.51 s], NDCG = 
[ 0.68263719  0.67813699  0.67833056  0.68205129  0.68794898  0.69580435
  0.70531323  0.71643176  0.72855178  0.74233267]
Iteration 10 [153.3 s]: loss = 0.803652 [77.57 s], NDCG = 
[ 0.68428668  0.67741639  0.67903807  0.68259991  0.68818498  0.69576206
  0.70581026  0.71665809  0.7289115   0.74239579]
Iteration 11 [159.2 s]: loss = 0.801250 [83.29 s], NDCG = 
[ 0.68469018  0.67697656  0.67890966  0.68294486  0.68866364  0.69630738
  0.70626373  0.71712787  0.72883983  0.74254911]
Iteration 12 [153.7 s]: loss = 0.798641 [80.70 s], NDCG = 
[ 0.6835041   0.67646209  0.67810622  0.68242579  0.68884458  0.69702197
  0.70643307  0.71716666  0.72869613  0.74261699]
Iteration 13 [159.4 s]: loss = 0.797932 [83.55 s], NDCG = 
[ 0.68364737  0.67664441  0.67841445  0.68231539  0.68890055  0.69683305
  0.70646512  0.7170453   0.72871694  0.74276988]
Iteration 14 [158.4 s]: loss = 0.795359 [79.32 s], NDCG = 
[ 0.68484496  0.67624346  0.67873881  0.68250037  0.68868037  0.69686778
  0.70635397  0.71698368  0.72893995  0.74254101]
Iteration 15 [157.5 s]: loss = 0.793805 [77.39 s], NDCG = 
[ 0.6852402   0.67671153  0.67865868  0.68196941  0.68921923  0.69708281
  0.70621939  0.71707662  0.72851048  0.74235328]
Iteration 16 [156.6 s]: loss = 0.790113 [84.17 s], NDCG = 
[ 0.68511174  0.6771098   0.6781709   0.68280819  0.68904551  0.69665614
  0.70602473  0.71676617  0.72856797  0.74214845]



