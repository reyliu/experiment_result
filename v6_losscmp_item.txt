liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [193.1 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [76.3], Init: NDCG = 
[ 0.52380357  0.53764783  0.55136499  0.56336371  0.5760786   0.59062634
  0.6052734   0.62060153  0.63726401  0.65494755]
Iteration 0 [107.0 s]: loss = 0.692788 [74.15 s], NDCG = 
[ 0.56960543  0.57863628  0.58950511  0.60054759  0.61324669  0.62640644
  0.63968897  0.65447157  0.6694493   0.68562481]
Iteration 1 [112.7 s]: loss = 0.688762 [73.17 s], NDCG = 
[ 0.62015626  0.62860858  0.63626638  0.64526417  0.65531803  0.66617346
  0.67807151  0.69114526  0.70455256  0.71878641]
Iteration 2 [114.2 s]: loss = 0.672618 [75.39 s], NDCG = 
[ 0.64247937  0.64769369  0.65299157  0.66228784  0.67173366  0.68188236
  0.69199221  0.70407086  0.71694408  0.7306785 ]
Iteration 3 [118.2 s]: loss = 0.653624 [77.99 s], NDCG = 
[ 0.65997394  0.66048022  0.66526143  0.67233121  0.68074534  0.6896675
  0.69992828  0.71130697  0.72333739  0.73648365]
Iteration 4 [121.9 s]: loss = 0.644838 [79.62 s], NDCG = 
[ 0.67291176  0.67100364  0.67285366  0.6791494   0.68620306  0.6945934
  0.70438488  0.71505224  0.72705418  0.74008187]
Iteration 5 [119.2 s]: loss = 0.641551 [73.59 s], NDCG = 
[ 0.67708341  0.67293287  0.67477063  0.67999308  0.687048    0.69559488
  0.70531964  0.71638681  0.7282518   0.74119139]
Iteration 6 [118.4 s]: loss = 0.639614 [74.40 s], NDCG = 
[ 0.68006552  0.6742557   0.67695828  0.68200094  0.68850402  0.69685258
  0.70595737  0.71717791  0.72909002  0.74190382]
Iteration 7 [112.9 s]: loss = 0.637296 [76.58 s], NDCG = 
[ 0.6817502   0.67557239  0.67746706  0.68254794  0.68941362  0.69738843
  0.70713501  0.71812737  0.72973687  0.74254012]
Iteration 8 [129.2 s]: loss = 0.635422 [81.75 s], NDCG = 
[ 0.68344344  0.67758439  0.6786941   0.6834081   0.68967972  0.6977906
  0.70736176  0.71845199  0.73010781  0.74311021]
Iteration 9 [118.1 s]: loss = 0.633481 [83.61 s], NDCG = 
[ 0.68438579  0.67778676  0.68036563  0.68401727  0.69005566  0.69860618
  0.70782325  0.71824649  0.73025007  0.74318273]
Iteration 10 [117.8 s]: loss = 0.631317 [82.81 s], NDCG = 
[ 0.68463324  0.67792091  0.67983951  0.68418397  0.690716    0.69849622
  0.70790724  0.71858766  0.73042893  0.74323178]
Iteration 11 [120.3 s]: loss = 0.629075 [81.87 s], NDCG = 
[ 0.68467448  0.67871848  0.67972796  0.68484661  0.69091825  0.69887917
  0.70778209  0.71829466  0.73032522  0.74322241]
Iteration 12 [121.6 s]: loss = 0.626753 [83.51 s], NDCG = 
[ 0.68484122  0.67911737  0.67941303  0.68488602  0.69076025  0.69842997
  0.70809088  0.71809234  0.73001432  0.74294751]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [231.4 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [78.2], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [105.3 s]: loss = 2.023731 [73.23 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [111.7 s]: loss = 1.989152 [72.65 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [112.0 s]: loss = 1.871648 [73.77 s], NDCG = 
[ 0.64031777  0.63952973  0.64231019  0.64900813  0.65699655  0.66580544
  0.67563427  0.68746035  0.70110792  0.71537024]
Iteration 3 [111.8 s]: loss = 1.764724 [71.93 s], NDCG = 
[ 0.65827855  0.65618224  0.65894412  0.66505097  0.6730928   0.68215455
  0.69222678  0.70394092  0.71577768  0.72970737]
Iteration 4 [112.6 s]: loss = 1.720380 [71.71 s], NDCG = 
[ 0.66727784  0.6634727   0.66531023  0.67065731  0.67807999  0.68725251
  0.6968639   0.7084574   0.72019155  0.73348381]
Iteration 5 [112.3 s]: loss = 1.696202 [71.87 s], NDCG = 
[ 0.67230986  0.66674771  0.66951712  0.67480547  0.68164571  0.68990723
  0.6999277   0.71051133  0.72264293  0.73573383]
Iteration 6 [113.9 s]: loss = 1.686928 [79.41 s], NDCG = 
[ 0.6754978   0.67253202  0.67369116  0.67733663  0.68440605  0.69200245
  0.70101004  0.71224637  0.72405624  0.73689646]
Iteration 7 [114.7 s]: loss = 1.669442 [79.66 s], NDCG = 
[ 0.6792182   0.67498059  0.67501058  0.67926474  0.68535394  0.69301463
  0.70212657  0.71266434  0.72438091  0.73753791]
Iteration 8 [115.2 s]: loss = 1.658563 [78.53 s], NDCG = 
[ 0.67732395  0.67475116  0.67404042  0.67873171  0.68520006  0.69319853
  0.70230773  0.71268714  0.72449201  0.73766216]
Iteration 9 [111.6 s]: loss = 1.642993 [71.69 s], NDCG = 
[ 0.67879711  0.675489    0.67547311  0.67919735  0.68581455  0.69376477
  0.70284839  0.71355366  0.72494622  0.73790279]
Iteration 10 [112.7 s]: loss = 1.633837 [71.83 s], NDCG = 
[ 0.68177998  0.67710066  0.67636938  0.67980655  0.68609568  0.69366604
  0.70279684  0.71371187  0.72545346  0.73846646]
Iteration 11 [111.2 s]: loss = 1.618133 [72.16 s], NDCG = 
[ 0.68245714  0.67656939  0.67609128  0.68036379  0.6860376   0.69333639
  0.70250138  0.71345323  0.7251693   0.73818215]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [230.1 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [77.4], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [121.7 s]: loss = 0.998686 [81.65 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [133.1 s]: loss = 0.981132 [80.12 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [131.6 s]: loss = 0.904670 [83.26 s], NDCG = 
[ 0.63894585  0.64217495  0.64623906  0.65219765  0.66079267  0.66929421
  0.67926179  0.69102162  0.7043823   0.71818645]
Iteration 3 [146.8 s]: loss = 0.845266 [78.82 s], NDCG = 
[ 0.65169139  0.654036    0.65828759  0.66452224  0.67254913  0.68156637
  0.69177185  0.7029323   0.71534718  0.72884542]
Iteration 4 [142.0 s]: loss = 0.825068 [80.67 s], NDCG = 
[ 0.66238311  0.66162422  0.66372856  0.67048588  0.67825519  0.68689328
  0.697107    0.70807946  0.72006234  0.73347719]
Iteration 5 [143.1 s]: loss = 0.817402 [78.07 s], NDCG = 
[ 0.66999559  0.66412618  0.6676437   0.67337916  0.68105893  0.6895521
  0.69917987  0.7101827   0.72225105  0.7352074 ]
Iteration 6 [146.1 s]: loss = 0.812675 [81.96 s], NDCG = 
[ 0.67214844  0.66722709  0.67001874  0.67546209  0.68241798  0.69114594
  0.7006485   0.71155697  0.72383605  0.73683192]
Iteration 7 [144.4 s]: loss = 0.809661 [74.36 s], NDCG = 
[ 0.67379563  0.66838661  0.67132509  0.67585953  0.68320904  0.69198983
  0.70198281  0.71248948  0.7243025   0.7373154 ]
Iteration 8 [149.5 s]: loss = 0.808468 [80.41 s], NDCG = 
[ 0.67905818  0.67243909  0.67370187  0.67833239  0.68562996  0.69374828
  0.70362867  0.71391792  0.72545105  0.73821718]
Iteration 9 [155.7 s]: loss = 0.804734 [83.31 s], NDCG = 
[ 0.67857728  0.67273497  0.6737296   0.6795036   0.68621147  0.69414589
  0.70315099  0.7140493   0.72543118  0.7384854 ]
Iteration 10 [154.0 s]: loss = 0.803770 [76.46 s], NDCG = 
[ 0.67882612  0.67373446  0.67549252  0.67974769  0.68710196  0.6951719
  0.70426945  0.71464342  0.72607935  0.73940894]
Iteration 11 [149.5 s]: loss = 0.801162 [81.06 s], NDCG = 
[ 0.67847053  0.67359619  0.67503559  0.67977612  0.68711362  0.69453717
  0.70413502  0.71431553  0.72577179  0.73889371]


