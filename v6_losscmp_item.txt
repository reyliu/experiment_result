liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [193.1 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [76.3], Init: NDCG = 
[ 0.52380357  0.53764783  0.55136499  0.56336371  0.5760786   0.59062634
  0.6052734   0.62060153  0.63726401  0.65494755]
Iteration 0 [107.0 s]: loss = 0.692788 [74.15 s], NDCG = 
[ 0.56960543  0.57863628  0.58950511  0.60054759  0.61324669  0.62640644
  0.63968897  0.65447157  0.6694493   0.68562481]
Iteration 1 [112.7 s]: loss = 0.688762 [73.17 s], NDCG = 
[ 0.62015626  0.62860858  0.63626638  0.64526417  0.65531803  0.66617346
  0.67807151  0.69114526  0.70455256  0.71878641]
Iteration 2 [114.2 s]: loss = 0.672618 [75.39 s], NDCG = 
[ 0.64247937  0.64769369  0.65299157  0.66228784  0.67173366  0.68188236
  0.69199221  0.70407086  0.71694408  0.7306785 ]
Iteration 3 [118.2 s]: loss = 0.653624 [77.99 s], NDCG = 
[ 0.65997394  0.66048022  0.66526143  0.67233121  0.68074534  0.6896675
  0.69992828  0.71130697  0.72333739  0.73648365]
Iteration 4 [121.9 s]: loss = 0.644838 [79.62 s], NDCG = 
[ 0.67291176  0.67100364  0.67285366  0.6791494   0.68620306  0.6945934
  0.70438488  0.71505224  0.72705418  0.74008187]
Iteration 5 [119.2 s]: loss = 0.641551 [73.59 s], NDCG = 
[ 0.67708341  0.67293287  0.67477063  0.67999308  0.687048    0.69559488
  0.70531964  0.71638681  0.7282518   0.74119139]
Iteration 6 [118.4 s]: loss = 0.639614 [74.40 s], NDCG = 
[ 0.68006552  0.6742557   0.67695828  0.68200094  0.68850402  0.69685258
  0.70595737  0.71717791  0.72909002  0.74190382]
Iteration 7 [112.9 s]: loss = 0.637296 [76.58 s], NDCG = 
[ 0.6817502   0.67557239  0.67746706  0.68254794  0.68941362  0.69738843
  0.70713501  0.71812737  0.72973687  0.74254012]
Iteration 8 [129.2 s]: loss = 0.635422 [81.75 s], NDCG = 
[ 0.68344344  0.67758439  0.6786941   0.6834081   0.68967972  0.6977906
  0.70736176  0.71845199  0.73010781  0.74311021]
Iteration 9 [118.1 s]: loss = 0.633481 [83.61 s], NDCG = 
[ 0.68438579  0.67778676  0.68036563  0.68401727  0.69005566  0.69860618
  0.70782325  0.71824649  0.73025007  0.74318273]
Iteration 10 [117.8 s]: loss = 0.631317 [82.81 s], NDCG = 
[ 0.68463324  0.67792091  0.67983951  0.68418397  0.690716    0.69849622
  0.70790724  0.71858766  0.73042893  0.74323178]
Iteration 11 [120.3 s]: loss = 0.629075 [81.87 s], NDCG = 
[ 0.68467448  0.67871848  0.67972796  0.68484661  0.69091825  0.69887917
  0.70778209  0.71829466  0.73032522  0.74322241]
Iteration 12 [121.6 s]: loss = 0.626753 [83.51 s], NDCG = 
[ 0.68484122  0.67911737  0.67941303  0.68488602  0.69076025  0.69842997
  0.70809088  0.71809234  0.73001432  0.74294751]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [231.4 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [78.2], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [105.3 s]: loss = 2.023731 [73.23 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [111.7 s]: loss = 1.989152 [72.65 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [112.0 s]: loss = 1.871648 [73.77 s], NDCG = 
[ 0.64031777  0.63952973  0.64231019  0.64900813  0.65699655  0.66580544
  0.67563427  0.68746035  0.70110792  0.71537024]
Iteration 3 [111.8 s]: loss = 1.764724 [71.93 s], NDCG = 
[ 0.65827855  0.65618224  0.65894412  0.66505097  0.6730928   0.68215455
  0.69222678  0.70394092  0.71577768  0.72970737]
Iteration 4 [112.6 s]: loss = 1.720380 [71.71 s], NDCG = 
[ 0.66727784  0.6634727   0.66531023  0.67065731  0.67807999  0.68725251
  0.6968639   0.7084574   0.72019155  0.73348381]
Iteration 5 [112.3 s]: loss = 1.696202 [71.87 s], NDCG = 
[ 0.67230986  0.66674771  0.66951712  0.67480547  0.68164571  0.68990723
  0.6999277   0.71051133  0.72264293  0.73573383]
Iteration 6 [113.9 s]: loss = 1.686928 [79.41 s], NDCG = 
[ 0.6754978   0.67253202  0.67369116  0.67733663  0.68440605  0.69200245
  0.70101004  0.71224637  0.72405624  0.73689646]
Iteration 7 [114.7 s]: loss = 1.669442 [79.66 s], NDCG = 
[ 0.6792182   0.67498059  0.67501058  0.67926474  0.68535394  0.69301463
  0.70212657  0.71266434  0.72438091  0.73753791]
Iteration 8 [115.2 s]: loss = 1.658563 [78.53 s], NDCG = 
[ 0.67732395  0.67475116  0.67404042  0.67873171  0.68520006  0.69319853
  0.70230773  0.71268714  0.72449201  0.73766216]
Iteration 9 [111.6 s]: loss = 1.642993 [71.69 s], NDCG = 
[ 0.67879711  0.675489    0.67547311  0.67919735  0.68581455  0.69376477
  0.70284839  0.71355366  0.72494622  0.73790279]
Iteration 10 [112.7 s]: loss = 1.633837 [71.83 s], NDCG = 
[ 0.68177998  0.67710066  0.67636938  0.67980655  0.68609568  0.69366604
  0.70279684  0.71371187  0.72545346  0.73846646]
Iteration 11 [111.2 s]: loss = 1.618133 [72.16 s], NDCG = 
[ 0.68245714  0.67656939  0.67609128  0.68036379  0.6860376   0.69333639
  0.70250138  0.71345323  0.7251693   0.73818215]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [230.1 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [77.4], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [121.7 s]: loss = 0.998686 [81.65 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [133.1 s]: loss = 0.981132 [80.12 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [131.6 s]: loss = 0.904670 [83.26 s], NDCG = 
[ 0.63894585  0.64217495  0.64623906  0.65219765  0.66079267  0.66929421
  0.67926179  0.69102162  0.7043823   0.71818645]
Iteration 3 [146.8 s]: loss = 0.845266 [78.82 s], NDCG = 
[ 0.65169139  0.654036    0.65828759  0.66452224  0.67254913  0.68156637
  0.69177185  0.7029323   0.71534718  0.72884542]
Iteration 4 [142.0 s]: loss = 0.825068 [80.67 s], NDCG = 
[ 0.66238311  0.66162422  0.66372856  0.67048588  0.67825519  0.68689328
  0.697107    0.70807946  0.72006234  0.73347719]
Iteration 5 [143.1 s]: loss = 0.817402 [78.07 s], NDCG = 
[ 0.66999559  0.66412618  0.6676437   0.67337916  0.68105893  0.6895521
  0.69917987  0.7101827   0.72225105  0.7352074 ]
Iteration 6 [146.1 s]: loss = 0.812675 [81.96 s], NDCG = 
[ 0.67214844  0.66722709  0.67001874  0.67546209  0.68241798  0.69114594
  0.7006485   0.71155697  0.72383605  0.73683192]
Iteration 7 [144.4 s]: loss = 0.809661 [74.36 s], NDCG = 
[ 0.67379563  0.66838661  0.67132509  0.67585953  0.68320904  0.69198983
  0.70198281  0.71248948  0.7243025   0.7373154 ]
Iteration 8 [149.5 s]: loss = 0.808468 [80.41 s], NDCG = 
[ 0.67905818  0.67243909  0.67370187  0.67833239  0.68562996  0.69374828
  0.70362867  0.71391792  0.72545105  0.73821718]
Iteration 9 [155.7 s]: loss = 0.804734 [83.31 s], NDCG = 
[ 0.67857728  0.67273497  0.6737296   0.6795036   0.68621147  0.69414589
  0.70315099  0.7140493   0.72543118  0.7384854 ]
Iteration 10 [154.0 s]: loss = 0.803770 [76.46 s], NDCG = 
[ 0.67882612  0.67373446  0.67549252  0.67974769  0.68710196  0.6951719
  0.70426945  0.71464342  0.72607935  0.73940894]
Iteration 11 [149.5 s]: loss = 0.801162 [81.06 s], NDCG = 
[ 0.67847053  0.67359619  0.67503559  0.67977612  0.68711362  0.69453717
  0.70413502  0.71431553  0.72577179  0.73889371]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [583.4 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [82.7], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [106.7 s]: loss = 2.023601 [76.67 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [115.1 s]: loss = 1.993769 [75.76 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [113.7 s]: loss = 1.895569 [76.12 s], NDCG = 
[ 0.60639539  0.60524272  0.60900379  0.61477345  0.62398099  0.63417958
  0.646392    0.66003731  0.67425477  0.68991266]
Iteration 3 [112.8 s]: loss = 1.777622 [76.11 s], NDCG = 
[ 0.65607565  0.6538704   0.6578565   0.66391445  0.67128385  0.68056908
  0.69011481  0.70201069  0.7144302   0.72802575]
Iteration 4 [113.8 s]: loss = 1.727024 [75.53 s], NDCG = 
[ 0.66420292  0.66184078  0.66508631  0.67088855  0.67830783  0.68724727
  0.69686032  0.70759578  0.72018733  0.73322482]
Iteration 5 [113.6 s]: loss = 1.705708 [76.33 s], NDCG = 
[ 0.66660398  0.66551319  0.66859651  0.67338524  0.68044183  0.68903519
  0.69905661  0.70983023  0.72218622  0.73535927]
Iteration 6 [114.6 s]: loss = 1.688747 [77.00 s], NDCG = 
[ 0.67309547  0.67072198  0.67221186  0.67717751  0.68338008  0.6916576
  0.70149827  0.71205212  0.7235945   0.73703487]
Iteration 7 [117.3 s]: loss = 1.674569 [73.50 s], NDCG = 
[ 0.6772269   0.67271611  0.67320586  0.67805059  0.68435588  0.69234803
  0.70150117  0.71195049  0.72411163  0.73748442]
Iteration 8 [113.2 s]: loss = 1.657788 [74.59 s], NDCG = 
[ 0.67797513  0.67403082  0.6748457   0.67923581  0.68552896  0.69353802
  0.70217083  0.71288566  0.72457951  0.73783623]
Iteration 9 [112.7 s]: loss = 1.648461 [75.21 s], NDCG = 
[ 0.68007299  0.67486374  0.67508418  0.68011517  0.68620694  0.69395941
  0.70319546  0.71324877  0.72484443  0.7381766 ]
Iteration 10 [113.4 s]: loss = 1.630615 [75.38 s], NDCG = 
[ 0.68020445  0.67457684  0.67523781  0.67967679  0.68598077  0.69372446
  0.70265303  0.71302811  0.72476273  0.73784409]
Iteration 11 [113.6 s]: loss = 1.615431 [77.10 s], NDCG = 
[ 0.68100754  0.67525039  0.67626573  0.67995993  0.68621285  0.69423084
  0.70316626  0.71326154  0.72485822  0.73807243]
Iteration 12 [115.9 s]: loss = 1.596647 [81.80 s], NDCG = 
[ 0.68360271  0.67703142  0.67718867  0.68115636  0.68658629  0.69418002
  0.70336356  0.71343542  0.72503747  0.73814644]
Iteration 13 [117.9 s]: loss = 1.578092 [78.08 s], NDCG = 
[ 0.68375621  0.67711043  0.67746661  0.68122832  0.68625092  0.69363309
  0.70285392  0.71281172  0.7247918   0.73796795]
Iteration 14 [112.7 s]: loss = 1.554248 [78.24 s], NDCG = 
[ 0.6862839   0.67716469  0.67707396  0.68020154  0.68644927  0.69420565
  0.70258834  0.71270705  0.7247437   0.73808603]
Iteration 15 [115.8 s]: loss = 1.530198 [80.99 s], NDCG = 
[ 0.68362583  0.67677241  0.67677756  0.68058483  0.68694791  0.69403751
  0.70190696  0.71234828  0.72408911  0.73767981]
Iteration 16 [113.9 s]: loss = 1.502813 [76.52 s], NDCG = 
[ 0.68198517  0.67604476  0.6753267   0.67933435  0.68612811  0.69291627
  0.70128342  0.71173242  0.72354347  0.73665179]
Iteration 17 [113.4 s]: loss = 1.472522 [76.92 s], NDCG = 
[ 0.68331111  0.67628831  0.67554112  0.67926064  0.68494451  0.69194752
  0.70088362  0.71123223  0.72307406  0.73664449]
Iteration 18 [113.0 s]: loss = 1.437347 [76.59 s], NDCG = 
[ 0.68324769  0.67532277  0.67522859  0.67918367  0.68456588  0.69180079
  0.70067238  0.71118214  0.72279774  0.73630888]
Iteration 19 [112.5 s]: loss = 1.403657 [76.47 s], NDCG = 
[ 0.68136966  0.67439725  0.67414131  0.67803154  0.68371772  0.6907169
  0.69927606  0.70994084  0.72194471  0.73571145]
Iteration 20 [112.7 s]: loss = 1.364613 [76.39 s], NDCG = 
[ 0.68403951  0.67415495  0.67438658  0.67812207  0.6833864   0.6903659
  0.69929627  0.70989521  0.72160194  0.73508235]
Iteration 21 [112.9 s]: loss = 1.323948 [76.40 s], NDCG = 
[ 0.67987787  0.67142438  0.67146293  0.67541622  0.68138155  0.68929357
  0.69820246  0.7089947   0.72066543  0.733992  ]
Iteration 22 [112.9 s]: loss = 1.285220 [76.48 s], NDCG = 
[ 0.67840763  0.6716492   0.67105178  0.67499977  0.6802649   0.68770686
  0.69755821  0.70787938  0.71979881  0.73313449]
Iteration 23 [112.7 s]: loss = 1.246053 [76.54 s], NDCG = 
[ 0.67830988  0.66886163  0.67048584  0.67377287  0.67873944  0.68666112
  0.69602099  0.70678545  0.71840866  0.73217879]
Iteration 24 [113.0 s]: loss = 1.203172 [80.76 s], NDCG = 
[ 0.67632288  0.66690829  0.66639169  0.67090158  0.67751698  0.6853194
  0.69531321  0.7058321   0.7176299   0.73082477]
Iteration 25 [111.5 s]: loss = 1.169933 [70.73 s], NDCG = 
[ 0.67565694  0.66441937  0.66490328  0.66925151  0.6757254   0.68344319
  0.69336371  0.70431843  0.71625882  0.72997348]
Iteration 26 [109.8 s]: loss = 1.127737 [72.18 s], NDCG = 
[ 0.67202256  0.66268231  0.66219457  0.66643482  0.6737169   0.68159638
  0.69116659  0.70210247  0.71458138  0.72848923]
Iteration 27 [109.5 s]: loss = 1.087839 [71.09 s], NDCG = 
[ 0.66875556  0.6601592   0.66080255  0.66543323  0.67131929  0.67986003
  0.69000939  0.70111574  0.71342365  0.72693404]
Iteration 28 [109.6 s]: loss = 1.058331 [70.87 s], NDCG = 
[ 0.6674034   0.65912758  0.65971631  0.66400434  0.67087181  0.67940174
  0.68886118  0.70023982  0.71283503  0.72632991]
Iteration 29 [110.1 s]: loss = 1.022944 [71.36 s], NDCG = 
[ 0.66477872  0.65736229  0.65766231  0.66251161  0.66925557  0.67756954
  0.68732984  0.69853752  0.71120513  0.72513164]
Iteration 30 [109.6 s]: loss = 0.990859 [71.30 s], NDCG = 
[ 0.6604103   0.65399722  0.65611717  0.6604227   0.66742334  0.67610572
  0.68629887  0.69774902  0.71020896  0.72414012]
Iteration 31 [110.0 s]: loss = 0.961707 [71.46 s], NDCG = 
[ 0.65840385  0.65128504  0.65398806  0.65893181  0.66578571  0.67474531
  0.68491381  0.69653732  0.70891736  0.72307202]
Iteration 32 [110.0 s]: loss = 0.927451 [71.70 s], NDCG = 
[ 0.65506318  0.64915446  0.65167366  0.65672355  0.66424189  0.67347955
  0.68354591  0.69481077  0.70781667  0.7218759 ]
Iteration 33 [110.2 s]: loss = 0.902405 [72.57 s], NDCG = 
[ 0.65088684  0.64487439  0.64816614  0.65403127  0.66253696  0.67182438
  0.68217716  0.69387911  0.70644268  0.72030154]
Iteration 34 [110.3 s]: loss = 0.871057 [71.34 s], NDCG = 
[ 0.64865981  0.64391653  0.6474363   0.6528859   0.66119027  0.67060773
  0.68096416  0.69215139  0.70496656  0.71928706]
Iteration 35 [109.7 s]: loss = 0.846164 [71.50 s], NDCG = 
[ 0.64500877  0.6418791   0.64400287  0.65105174  0.6591687   0.66840006
  0.67905625  0.69069902  0.70373678  0.71804072]
Iteration 36 [109.6 s]: loss = 0.818746 [71.33 s], NDCG = 
[ 0.64085005  0.63956972  0.64232543  0.64989631  0.65802886  0.66792617
  0.67817744  0.68993598  0.70279174  0.71690505]
Iteration 37 [109.7 s]: loss = 0.793567 [71.50 s], NDCG = 
[ 0.63823404  0.63590431  0.63987359  0.64638581  0.65554725  0.66605563
  0.67632405  0.68812078  0.70079024  0.71500807]
Iteration 38 [109.8 s]: loss = 0.769580 [71.41 s], NDCG = 
[ 0.63852678  0.63617064  0.63973299  0.64654229  0.65502297  0.66535774
  0.67561338  0.68781775  0.70051087  0.71492547]
Iteration 39 [109.9 s]: loss = 0.746325 [71.86 s], NDCG = 
[ 0.63823543  0.63442996  0.63636005  0.64418798  0.6537373   0.66393455
  0.67460135  0.68664199  0.69932999  0.71383876]
Iteration 40 [109.6 s]: loss = 0.727932 [71.57 s], NDCG = 
[ 0.63622215  0.63276675  0.63643959  0.64412804  0.65294077  0.66286959
  0.67422574  0.68618606  0.69894999  0.71320302]
Iteration 41 [109.5 s]: loss = 0.707456 [71.59 s], NDCG = 
[ 0.63277003  0.6300822   0.63377815  0.64177585  0.65051646  0.66151065
  0.67219144  0.68467991  0.69772551  0.71186807]
Iteration 42 [109.7 s]: loss = 0.683857 [71.38 s], NDCG = 
[ 0.62754163  0.62779657  0.6324702   0.64021713  0.64900982  0.65941467
  0.6711528   0.68354901  0.69664493  0.71088079]




liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [220.2 s]. #user=12922, #item=13679, #train=20462698, #test=316795
Time: [82.2], Init: NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 0 [119.6 s]: loss = 0.999206 [77.45 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 1 [131.3 s]: loss = 0.987937 [80.27 s], NDCG = 
[ 0.51777574  0.53551241  0.54887414  0.56170929  0.57552412  0.58983295
  0.6043928   0.62045936  0.63661848  0.65434552]
Iteration 2 [138.8 s]: loss = 0.935010 [71.08 s], NDCG = 
[ 0.51785556  0.53559738  0.5489142   0.56174149  0.57554512  0.58984666
  0.60439795  0.62045348  0.63662713  0.65434431]
Iteration 3 [129.0 s]: loss = 0.863045 [71.90 s], NDCG = 
[ 0.6450369   0.64648543  0.65117164  0.65693049  0.6648926   0.67411515
  0.68372478  0.6955009   0.70788309  0.72202642]
Iteration 4 [132.8 s]: loss = 0.830931 [72.02 s], NDCG = 
[ 0.65211623  0.65326     0.6575467   0.66433173  0.67261768  0.68234095
  0.69191583  0.70318732  0.71551645  0.72875746]
Iteration 5 [137.1 s]: loss = 0.821048 [70.29 s], NDCG = 
[ 0.65971173  0.65852845  0.66140293  0.66832959  0.67634067  0.68562193
  0.69586961  0.7068164   0.71899201  0.73238777]
Iteration 6 [137.3 s]: loss = 0.814907 [70.30 s], NDCG = 
[ 0.66495737  0.66253929  0.66591191  0.67187983  0.67928775  0.68827531
  0.6985526   0.70915603  0.72135709  0.73484835]
Iteration 7 [137.4 s]: loss = 0.811189 [70.85 s], NDCG = 
[ 0.66491853  0.66375556  0.66654285  0.6727169   0.680468    0.68963785
  0.69961106  0.71078374  0.72225579  0.73547073]
Iteration 8 [138.4 s]: loss = 0.807298 [71.68 s], NDCG = 
[ 0.67180819  0.66672658  0.66985366  0.67510029  0.6827398   0.691996
  0.70159951  0.71247874  0.72405319  0.7369075 ]
Iteration 9 [139.3 s]: loss = 0.806430 [71.49 s], NDCG = 
[ 0.67334552  0.66701709  0.66980297  0.6757876   0.68403763  0.69217411
  0.70156484  0.71234806  0.72401485  0.73738688]
Iteration 10 [139.7 s]: loss = 0.803808 [71.40 s], NDCG = 
[ 0.67644173  0.66946419  0.67220792  0.67771632  0.6850937   0.69350176
  0.70263271  0.71388039  0.72528981  0.73815394]
Iteration 11 [141.8 s]: loss = 0.802434 [71.63 s], NDCG = 
[ 0.6797703   0.67103882  0.67375932  0.67857464  0.68601429  0.69440034
  0.70369776  0.71465189  0.72623507  0.73937122]
Iteration 12 [141.5 s]: loss = 0.799690 [71.38 s], NDCG = 
[ 0.68145265  0.67162344  0.67405684  0.6785957   0.68603883  0.69432602
  0.70379609  0.7146633   0.7265557   0.73911058]
Iteration 13 [141.2 s]: loss = 0.797823 [71.56 s], NDCG = 
[ 0.67851862  0.67182457  0.67398762  0.67904727  0.68593899  0.69429244
  0.70325002  0.7142697   0.72631431  0.7391205 ]
Iteration 14 [142.9 s]: loss = 0.795967 [71.66 s], NDCG = 
[ 0.677421    0.67157723  0.67397015  0.67909231  0.6862866   0.69419149
  0.70392776  0.71450607  0.72625651  0.73935873]
Iteration 15 [141.3 s]: loss = 0.794616 [71.59 s], NDCG = 
[ 0.6777261   0.6723856   0.67432614  0.67900773  0.68632128  0.6940503
  0.70368646  0.71465607  0.72602783  0.73918105]
Iteration 16 [142.5 s]: loss = 0.792504 [71.46 s], NDCG = 
[ 0.68034458  0.67390092  0.67477111  0.67973526  0.68678325  0.69472388
  0.70398472  0.71469349  0.72651809  0.73935382]
Iteration 17 [143.0 s]: loss = 0.790259 [71.85 s], NDCG = 
[ 0.68026583  0.67433904  0.67417225  0.67935364  0.68665732  0.69439314
  0.70390171  0.71474048  0.72653388  0.73945935]
Iteration 18 [143.3 s]: loss = 0.787462 [71.73 s], NDCG = 
[ 0.67960717  0.67377875  0.67388441  0.67950493  0.68649373  0.69491813
  0.70399817  0.71481311  0.7261229   0.73932265]
Iteration 19 [143.6 s]: loss = 0.784849 [71.55 s], NDCG = 
[ 0.6805867   0.67392582  0.67393168  0.67933397  0.68706257  0.69480252
  0.70447406  0.71498242  0.72628805  0.73923947]
Iteration 20 [143.7 s]: loss = 0.781774 [73.07 s], NDCG = 
[ 0.6823808   0.67479127  0.67416255  0.68015247  0.68682952  0.69502885
  0.70398184  0.71481268  0.7264657   0.73914496]
Iteration 21 [146.3 s]: loss = 0.777665 [71.45 s], NDCG = 
[ 0.6824401   0.67529871  0.67481006  0.68038352  0.68701792  0.69513015
  0.70415572  0.71519109  0.72639987  0.73960049]
Iteration 22 [144.0 s]: loss = 0.774389 [71.32 s], NDCG = 
[ 0.68227108  0.6758996   0.67541597  0.68017684  0.68734898  0.69522257
  0.70457707  0.71491441  0.72650092  0.73952505]
Iteration 23 [144.6 s]: loss = 0.770174 [71.49 s], NDCG = 
[ 0.68384889  0.67704656  0.6761966   0.68114668  0.68815551  0.6958091
  0.70471657  0.71492959  0.72680723  0.7397037 ]
Iteration 24 [144.7 s]: loss = 0.766099 [71.21 s], NDCG = 
[ 0.68164219  0.67508844  0.67545151  0.68021145  0.68769744  0.6954035
  0.70461905  0.71494093  0.72647346  0.73928383]
Iteration 25 [146.8 s]: loss = 0.759303 [71.82 s], NDCG = 
[ 0.68258023  0.67557841  0.67536072  0.68035179  0.68742167  0.69571089
  0.70472801  0.71491737  0.72635254  0.73946359]
Iteration 26 [146.3 s]: loss = 0.752295 [72.06 s], NDCG = 
[ 0.68330654  0.67578197  0.67631147  0.68076009  0.68737461  0.69563886
  0.70462534  0.71494273  0.72645509  0.73955786]
Iteration 27 [146.0 s]: loss = 0.745909 [71.73 s], NDCG = 
[ 0.68263141  0.67634795  0.67652049  0.68084139  0.68758671  0.69541515
  0.70479832  0.715409    0.72678534  0.7397592 ]
Iteration 28 [146.0 s]: loss = 0.737809 [71.30 s], NDCG = 
[ 0.68324725  0.67593129  0.67707288  0.68071996  0.68707562  0.69530985
  0.70465611  0.71548564  0.7267655   0.73924977]
Iteration 29 [146.2 s]: loss = 0.729366 [71.45 s], NDCG = 
[ 0.68260753  0.67634598  0.67726444  0.68013608  0.68649819  0.69497045
  0.7042376   0.7147571   0.72672659  0.73953503]
Iteration 30 [147.9 s]: loss = 0.719986 [71.40 s], NDCG = 
[ 0.68487182  0.67610134  0.67578675  0.68012988  0.68651789  0.69468893
  0.70454434  0.71502827  0.72661877  0.73979233]
Iteration 31 [148.3 s]: loss = 0.710321 [71.58 s], NDCG = 
[ 0.68389932  0.67615647  0.67636919  0.68048963  0.68716641  0.69466342
  0.70413085  0.71487826  0.72640627  0.7399388 ]
Iteration 32 [148.9 s]: loss = 0.702183 [71.38 s], NDCG = 
[ 0.68364743  0.6771349   0.67603186  0.68110631  0.68724752  0.69498331
  0.70443425  0.71518562  0.72705858  0.73991312]
Iteration 33 [150.3 s]: loss = 0.693548 [71.42 s], NDCG = 
[ 0.68489969  0.67742045  0.6774051   0.68057419  0.68684854  0.69439593
  0.70353125  0.7150063   0.72677742  0.74008306]
Iteration 34 [151.2 s]: loss = 0.684392 [72.13 s], NDCG = 
[ 0.68475202  0.67703902  0.67681124  0.68040556  0.68643369  0.69404558
  0.70360144  0.71457219  0.72691997  0.7401246 ]
Iteration 35 [152.5 s]: loss = 0.675619 [71.65 s], NDCG = 
[ 0.6819343   0.67584243  0.67541119  0.6795153   0.68570886  0.69331222
  0.70298331  0.71394709  0.72601654  0.73933954]
Iteration 36 [152.3 s]: loss = 0.666680 [71.71 s], NDCG = 
[ 0.6817203   0.67679402  0.67524439  0.67909652  0.68502066  0.69335495
  0.70242281  0.71358912  0.72583731  0.73908228]
Iteration 37 [153.7 s]: loss = 0.657456 [71.61 s], NDCG = 
[ 0.68366897  0.67711368  0.67664757  0.6798561   0.68600155  0.69381619
  0.70304579  0.71371381  0.72581806  0.73897294]
Iteration 38 [154.6 s]: loss = 0.649950 [71.65 s], NDCG = 
[ 0.68341088  0.67651731  0.67607897  0.67989062  0.6855812   0.69382705
  0.70331287  0.71379697  0.72589148  0.73864739]
Iteration 39 [156.2 s]: loss = 0.641626 [71.55 s], NDCG = 
[ 0.68424319  0.67808078  0.67567893  0.67958396  0.68528723  0.6931801
  0.70297965  0.71346462  0.72564985  0.73853352]
Iteration 40 [156.9 s]: loss = 0.633930 [71.47 s], NDCG = 
[ 0.6837372   0.67640277  0.67471995  0.67890909  0.68405168  0.69179247
  0.70128595  0.71244155  0.72469997  0.73760329]
Iteration 41 [158.1 s]: loss = 0.626709 [71.55 s], NDCG = 
[ 0.68318326  0.67440304  0.67380649  0.67717053  0.68318182  0.69066158
  0.70019338  0.71128808  0.723336    0.73646323]
Iteration 42 [160.2 s]: loss = 0.618417 [71.92 s], NDCG = 
[ 0.67912349  0.67273169  0.671982    0.67524167  0.68124321  0.68913385
  0.69864204  0.70953746  0.7215909   0.73489672]
Iteration 43 [161.3 s]: loss = 0.611400 [71.68 s], NDCG = 
[ 0.6786272   0.67225484  0.67065368  0.67481423  0.68092956  0.68887741
  0.69884416  0.70977346  0.72151994  0.73454528]
Iteration 44 [163.2 s]: loss = 0.604529 [71.55 s], NDCG = 
[ 0.67913946  0.6706977   0.66965483  0.673874    0.67971783  0.68779148
  0.69725655  0.70830474  0.72043303  0.73383427]
Iteration 45 [163.7 s]: loss = 0.597565 [71.75 s], NDCG = 
[ 0.67883626  0.67069792  0.66975372  0.67302696  0.67862897  0.68719593
  0.69691662  0.70822744  0.72053253  0.73381802]
Iteration 46 [165.0 s]: loss = 0.590768 [71.42 s], NDCG = 
[ 0.67727631  0.66797971  0.66744657  0.67090126  0.67679541  0.68485166
  0.69518645  0.70634286  0.71824068  0.73180896]
Iteration 47 [166.8 s]: loss = 0.584244 [71.52 s], NDCG = 
[ 0.6773657   0.66846696  0.66716087  0.67109415  0.67716275  0.68484133
  0.6949527   0.70594829  0.71813587  0.73141654]
Iteration 48 [168.1 s]: loss = 0.577382 [71.47 s], NDCG = 
[ 0.67794092  0.66805722  0.66739018  0.67098397  0.67711497  0.68438282
  0.69425285  0.70508301  0.71754477  0.73127659]
Iteration 49 [169.8 s]: loss = 0.572195 [71.57 s], NDCG = 
[ 0.67555114  0.66787761  0.66551122  0.67005054  0.67583181  0.68372597
  0.693553    0.70464587  0.71701627  0.7306592 ]
Iteration 50 [170.8 s]: loss = 0.566653 [71.51 s], NDCG = 
[ 0.67329319  0.66504935  0.6641987   0.66818922  0.67481631  0.68259479
  0.69236708  0.70342456  0.71554908  0.72910042]
Iteration 51 [172.2 s]: loss = 0.560153 [71.52 s], NDCG = 
[ 0.67256326  0.66359495  0.66281989  0.66765611  0.67391644  0.68197514
  0.69189537  0.70293479  0.71503593  0.72884048]
Iteration 52 [174.2 s]: loss = 0.554489 [71.68 s], NDCG = 
[ 0.67315173  0.66393152  0.66311289  0.66707665  0.67352855  0.6815087
  0.69170362  0.702499    0.71437751  0.72822761]
Iteration 53 [175.8 s]: loss = 0.550596 [71.51 s], NDCG = 
[ 0.67277923  0.66297142  0.66228866  0.6657384   0.67213109  0.68040044
  0.68970199  0.70105458  0.71338484  0.72700323]
Iteration 54 [177.1 s]: loss = 0.545758 [71.38 s], NDCG = 
[ 0.67323123  0.66238539  0.66197948  0.66617724  0.67211296  0.68001439
  0.69011099  0.70153672  0.71355685  0.72708506]
Iteration 55 [178.8 s]: loss = 0.541110 [71.66 s], NDCG = 
[ 0.66987953  0.66084537  0.66135706  0.66539034  0.671247    0.67942786
  0.68914958  0.7005483   0.71308891  0.72659632]
Iteration 56 [180.2 s]: loss = 0.536349 [71.62 s], NDCG = 
[ 0.66915892  0.65956611  0.65993744  0.66454323  0.67087785  0.67829824
  0.68813045  0.6995847   0.71204478  0.72611302]
Iteration 57 [181.7 s]: loss = 0.530644 [71.67 s], NDCG = 
[ 0.6701387   0.65959985  0.65914607  0.66348734  0.67051096  0.67854394
  0.68837127  0.69944502  0.7118031   0.72563751]
Iteration 58 [182.9 s]: loss = 0.528110 [71.54 s], NDCG = 
[ 0.66710477  0.65806634  0.6586123   0.66230814  0.66928574  0.67717933
  0.68731056  0.69849946  0.71100352  0.72500368]
Iteration 59 [185.0 s]: loss = 0.524486 [71.47 s], NDCG = 
[ 0.6671194   0.65941525  0.65851602  0.66275481  0.66947338  0.67726689
  0.68745793  0.69902329  0.71159393  0.72509567]
Iteration 60 [187.2 s]: loss = 0.519220 [71.36 s], NDCG = 
[ 0.66648032  0.65829104  0.65864226  0.66273604  0.66927159  0.67744459
  0.68690821  0.69825681  0.71096666  0.72474066]
Iteration 61 [188.7 s]: loss = 0.515483 [71.55 s], NDCG = 
[ 0.6646631   0.65872083  0.65713991  0.66176561  0.66813314  0.67644878
  0.68599914  0.69737457  0.7102213   0.72433789]
Iteration 62 [189.7 s]: loss = 0.512272 [71.36 s], NDCG = 
[ 0.66457973  0.65846218  0.65813628  0.66156991  0.66802436  0.67648102
  0.6865308   0.69736145  0.7101466   0.72402133]
Iteration 63 [192.0 s]: loss = 0.507965 [71.55 s], NDCG = 
[ 0.66428667  0.65733796  0.65672388  0.66048906  0.66705557  0.67565687
  0.68493572  0.69680508  0.7091945   0.72333723]
Iteration 64 [192.8 s]: loss = 0.505848 [71.39 s], NDCG = 
[ 0.66581419  0.65803146  0.65761901  0.6611511   0.66745219  0.67607831
  0.68588099  0.69735197  0.70976096  0.72398857]
Iteration 65 [194.2 s]: loss = 0.501066 [71.50 s], NDCG = 
[ 0.66210234  0.6560461   0.65554632  0.65963595  0.66656094  0.67485857
  0.68535318  0.69616976  0.70848882  0.72256935]
Iteration 66 [195.6 s]: loss = 0.498078 [71.89 s], NDCG = 
[ 0.66310974  0.65505537  0.65399773  0.65872371  0.66591293  0.67436588
  0.68460576  0.69545909  0.70804987  0.72207109]
Iteration 67 [197.2 s]: loss = 0.494728 [71.64 s], NDCG = 
[ 0.66008069  0.65293286  0.65332707  0.65835018  0.66500007  0.67344243
  0.68367512  0.69464442  0.7076053   0.72144896]
Iteration 68 [198.3 s]: loss = 0.492073 [71.42 s], NDCG = 
[ 0.65822723  0.6506469   0.65337673  0.65834987  0.66487982  0.6733019
  0.68336806  0.69418399  0.70725893  0.72114093]
Iteration 69 [200.1 s]: loss = 0.489218 [71.34 s], NDCG = 
[ 0.66040998  0.65335786  0.65369364  0.65790062  0.66509491  0.6737918
  0.68295991  0.69453655  0.70724736  0.72102818]
Iteration 70 [202.1 s]: loss = 0.486701 [71.53 s], NDCG = 
[ 0.66016178  0.65358254  0.6530778   0.65778353  0.66430466  0.67311179
  0.68281745  0.6940071   0.70675323  0.72073573]
Iteration 71 [202.0 s]: loss = 0.483026 [71.60 s], NDCG = 
[ 0.6619152   0.65246897  0.65296962  0.65681063  0.6638193   0.67169419
  0.68187874  0.69343655  0.70627785  0.72029363]
Iteration 72 [204.7 s]: loss = 0.481164 [71.87 s], NDCG = 
[ 0.66177703  0.65225042  0.65230783  0.65662557  0.66410565  0.6721392
  0.68241877  0.69371093  0.70627189  0.72034006]
Iteration 73 [206.8 s]: loss = 0.478715 [71.99 s], NDCG = 
[ 0.65900613  0.65182179  0.65194112  0.6564327   0.66312765  0.67152432
  0.6816752   0.69342919  0.70606641  0.72018388]
Iteration 74 [207.4 s]: loss = 0.474807 [72.13 s], NDCG = 
[ 0.65550866  0.6503598   0.65055117  0.65571317  0.66232198  0.67128722
  0.68102552  0.69263959  0.70539996  0.71936241]
Iteration 75 [207.2 s]: loss = 0.472982 [71.48 s], NDCG = 
[ 0.6557995   0.65045203  0.6510358   0.65475902  0.66205365  0.67105376
  0.68094973  0.69244517  0.70497286  0.71889561]
Iteration 76 [209.8 s]: loss = 0.469618 [71.67 s], NDCG = 
[ 0.65536409  0.65137135  0.6507079   0.65528891  0.66223352  0.67089616
  0.68105515  0.69261333  0.70472446  0.71860807]
Iteration 77 [210.8 s]: loss = 0.467484 [71.78 s], NDCG = 
[ 0.65955316  0.65164725  0.6510945   0.65575365  0.66256122  0.6719151
  0.68167504  0.69282938  0.70537725  0.7194107 ]
Iteration 78 [212.2 s]: loss = 0.465144 [71.73 s], NDCG = 
[ 0.65687748  0.64986995  0.64892353  0.65426019  0.66158932  0.6708912
  0.68032329  0.69180136  0.70437817  0.71839913]
Iteration 79 [213.4 s]: loss = 0.463377 [71.67 s], NDCG = 
[ 0.65729724  0.64974851  0.64859786  0.65404565  0.66146657  0.67078218
  0.6803686   0.69174715  0.70445082  0.71820243]
Iteration 80 [214.9 s]: loss = 0.460424 [71.49 s], NDCG = 
[ 0.65459324  0.648534    0.64832799  0.65406507  0.66103186  0.67013071
  0.6797585   0.69121556  0.70406174  0.7181402 ]
Iteration 81 [216.7 s]: loss = 0.459697 [71.66 s], NDCG = 
[ 0.6536917   0.64764822  0.64800917  0.65421453  0.66116571  0.66980834
  0.67974204  0.69109605  0.70378779  0.71789862]
Iteration 82 [216.8 s]: loss = 0.457039 [71.62 s], NDCG = 
[ 0.65426648  0.64825532  0.64817266  0.65387539  0.66083324  0.66987231
  0.67948983  0.69084208  0.7039131   0.71826386]
Iteration 83 [218.0 s]: loss = 0.455141 [71.75 s], NDCG = 
[ 0.65420902  0.64798583  0.64870799  0.65336926  0.65983889  0.66929323
  0.67944902  0.69064152  0.70359526  0.71755713]
Iteration 84 [219.3 s]: loss = 0.452883 [71.38 s], NDCG = 
[ 0.65320047  0.64688114  0.64736319  0.65354873  0.66052836  0.66939966
  0.67881641  0.6902653   0.70310951  0.71733552]
Iteration 85 [221.0 s]: loss = 0.451057 [71.75 s], NDCG = 
[ 0.65341549  0.64734     0.6483373   0.65338013  0.66068538  0.66918816
  0.67894009  0.69053976  0.70316249  0.71751904]
Iteration 86 [221.9 s]: loss = 0.449831 [71.60 s], NDCG = 
[ 0.65445621  0.64703751  0.64803772  0.65317968  0.65996271  0.66911091
  0.67887817  0.69012134  0.70274747  0.71685066]
Iteration 87 [223.5 s]: loss = 0.447529 [71.91 s], NDCG = 
[ 0.65315752  0.64745481  0.64831361  0.65315978  0.65967531  0.6685858
  0.67811055  0.68935967  0.70242783  0.71654871]
Iteration 88 [224.9 s]: loss = 0.446243 [72.15 s], NDCG = 
[ 0.65177267  0.64642201  0.64692721  0.6519622   0.65870767  0.66763998
  0.67710443  0.6887656   0.70168274  0.71607896]
Iteration 89 [226.1 s]: loss = 0.443846 [72.06 s], NDCG = 
[ 0.65165003  0.64652086  0.647902    0.65265228  0.65872991  0.66785154
  0.67792076  0.68926496  0.7026243   0.71670201]
Iteration 90 [228.8 s]: loss = 0.441606 [71.35 s], NDCG = 
[ 0.65406109  0.64629152  0.64707108  0.65180579  0.65812788  0.66755352
  0.67729936  0.68911495  0.70213595  0.71619053]
Iteration 91 [229.0 s]: loss = 0.441053 [71.68 s], NDCG = 
[ 0.65181062  0.64524593  0.64679486  0.65252482  0.65824588  0.6668864
  0.67679805  0.68857952  0.7016901   0.7158605 ]
Iteration 92 [228.8 s]: loss = 0.439126 [71.51 s], NDCG = 
[ 0.64928869  0.64505272  0.64653092  0.65130465  0.65795021  0.66714942
  0.67692804  0.68856022  0.70158458  0.7159824 ]
Iteration 93 [230.5 s]: loss = 0.436626 [71.80 s], NDCG = 
[ 0.65246022  0.64600078  0.6467      0.65164068  0.65881654  0.66759746
  0.67763291  0.68939992  0.70234651  0.71631951]
Iteration 94 [232.0 s]: loss = 0.435342 [74.66 s], NDCG = 
[ 0.65085954  0.64552257  0.64674974  0.65174149  0.65900008  0.66755547
  0.67786544  0.68942773  0.7020497   0.71630223]
Iteration 95 [238.6 s]: loss = 0.434276 [72.99 s], NDCG = 
[ 0.65067506  0.64391891  0.64485585  0.65072617  0.65777265  0.66639143
  0.67697668  0.68843142  0.70100506  0.71546165]
Iteration 96 [234.4 s]: loss = 0.431623 [72.50 s], NDCG = 
[ 0.64935521  0.64344792  0.64510658  0.65006055  0.65755687  0.66631268
  0.67622238  0.68777536  0.7011577   0.71535375]
Iteration 97 [235.0 s]: loss = 0.430893 [72.22 s], NDCG = 
[ 0.65014551  0.64491737  0.64634808  0.6513555   0.65801084  0.66689371
  0.67713631  0.68893403  0.70189485  0.71586973]
Iteration 98 [236.5 s]: loss = 0.428995 [72.41 s], NDCG = 
[ 0.64878587  0.64240474  0.6440759   0.6501136   0.65710332  0.66578364
  0.67596592  0.68809317  0.70092075  0.71496311]
Iteration 99 [237.3 s]: loss = 0.427404 [72.39 s], NDCG = 
[ 0.64773647  0.64408529  0.64572283  0.65116354  0.65727348  0.66588485
  0.6762419   0.68833968  0.70109486  0.71536193]
End. Best Iteration 34: NDCG = 0.7401. 

