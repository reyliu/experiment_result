liurui@ubuntu:~$ KERAS_BACKEND=theano python DeepRec/v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.000001,0.001,0.001,0.001] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.000001,0.001,0.001,0.001]', verbose=1) 
Traceback (most recent call last):
  File "DeepRec/v3_random_train/MLP.py", line 136, in <module>
    train = np.loadtxt(path + 'train_pair_all.txt')
  File "/home/liurui/.local/lib/python2.7/site-packages/numpy/lib/npyio.py", line 896, in loadtxt
    fh = iter(open(fname, 'U'))
IOError: [Errno 2] No such file or directory: 'Data/yelp/train_pair_all.txt'
liurui@ubuntu:~$ cd DeepRec
liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.000001,0.001,0.001,0.001] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.000001,0.001,0.001,0.001]', verbose=1) 
Load data done [192.6 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Init: NDCG = 0.689855 [68.7]
Iteration 0 [95.4 s]: NDCG = 0.745680, loss = 0.810463 [66.09 s]
Iteration 1 [88.3 s]: NDCG = 0.777062, loss = 0.760155 [67.81 s]
Iteration 2 [104.1 s]: NDCG = 0.787577, loss = 0.727629 [78.77 s]
Iteration 3 [121.8 s]: NDCG = 0.793671, loss = 0.705997 [166.73 s]
Iteration 4 [162.8 s]: NDCG = 0.796198, loss = 0.688934 [263.07 s]
Iteration 5 [143.2 s]: NDCG = 0.798181, loss = 0.678312 [336.18 s]
Iteration 6 [280.7 s]: NDCG = 0.799533, loss = 0.673433 [406.77 s]
Iteration 7 [422.6 s]: NDCG = 0.800459, loss = 0.670534 [431.04 s]
Iteration 8 [418.4 s]: NDCG = 0.801285, loss = 0.669308 [394.18 s]
Iteration 9 [427.2 s]: NDCG = 0.802017, loss = 0.667970 [361.02 s]
Iteration 10 [474.5 s]: NDCG = 0.802260, loss = 0.667375 [345.59 s]
Iteration 11 [509.3 s]: NDCG = 0.802596, loss = 0.666398 [343.77 s]
Iteration 12 [522.3 s]: NDCG = 0.802943, loss = 0.666003 [354.99 s]
Iteration 13 [533.0 s]: NDCG = 0.802967, loss = 0.665137 [359.23 s]
Iteration 14 [547.8 s]: NDCG = 0.803152, loss = 0.665175 [364.00 s]
Iteration 15 [544.7 s]: NDCG = 0.803342, loss = 0.664655 [352.80 s]
Iteration 16 [544.2 s]: NDCG = 0.803336, loss = 0.664147 [363.73 s]
Iteration 17 [556.7 s]: NDCG = 0.803424, loss = 0.663929 [368.25 s]
Iteration 18 [556.6 s]: NDCG = 0.803731, loss = 0.663782 [357.67 s]
Iteration 19 [552.6 s]: NDCG = 0.803810, loss = 0.663239 [363.23 s]
Iteration 20 [556.1 s]: NDCG = 0.803953, loss = 0.663094 [364.51 s]
Iteration 21 [552.4 s]: NDCG = 0.804094, loss = 0.662989 [367.47 s]
Iteration 22 [557.6 s]: NDCG = 0.803983, loss = 0.662817 [363.67 s]
Iteration 23 [551.3 s]: NDCG = 0.804018, loss = 0.662416 [361.31 s]
Iteration 24 [552.2 s]: NDCG = 0.804009, loss = 0.662317 [365.01 s]
Iteration 25 [552.0 s]: NDCG = 0.803906, loss = 0.662143 [344.21 s]
Iteration 26 [549.6 s]: NDCG = 0.803940, loss = 0.662426 [340.24 s]
Iteration 27 [545.5 s]: NDCG = 0.803940, loss = 0.662021 [343.94 s]
Iteration 28 [530.6 s]: NDCG = 0.803979, loss = 0.661723 [329.58 s]
Iteration 29 [526.1 s]: NDCG = 0.803996, loss = 0.661861 [328.99 s]
Iteration 30 [514.4 s]: NDCG = 0.803986, loss = 0.661910 [329.70 s]
Iteration 31 [514.5 s]: NDCG = 0.804006, loss = 0.661510 [325.75 s]
Iteration 32 [522.4 s]: NDCG = 0.803971, loss = 0.661447 [325.41 s]
Iteration 33 [515.0 s]: NDCG = 0.803909, loss = 0.661007 [328.71 s]
Iteration 34 [514.3 s]: NDCG = 0.803892, loss = 0.661002 [326.54 s]
Iteration 35 [517.2 s]: NDCG = 0.803865, loss = 0.660619 [325.76 s]
Iteration 36 [513.1 s]: NDCG = 0.804020, loss = 0.660547 [325.96 s]
Iteration 37 [517.7 s]: NDCG = 0.803973, loss = 0.660519 [325.75 s]
Iteration 38 [514.2 s]: NDCG = 0.804137, loss = 0.660675 [328.74 s]
Iteration 39 [513.1 s]: NDCG = 0.804103, loss = 0.660886 [324.98 s]
Iteration 40 [529.4 s]: NDCG = 0.804272, loss = 0.660487 [325.16 s]
Iteration 41 [513.5 s]: NDCG = 0.804295, loss = 0.660065 [332.62 s]
Iteration 42 [514.5 s]: NDCG = 0.804203, loss = 0.659971 [324.76 s]
Iteration 43 [520.7 s]: NDCG = 0.804213, loss = 0.660386 [324.78 s]
Iteration 44 [518.6 s]: NDCG = 0.804268, loss = 0.659600 [335.24 s]
Iteration 45 [520.2 s]: NDCG = 0.804258, loss = 0.660119 [332.78 s]
Iteration 46 [523.2 s]: NDCG = 0.804180, loss = 0.660233 [332.64 s]
Iteration 47 [525.1 s]: NDCG = 0.804261, loss = 0.659965 [334.84 s]
Iteration 48 [535.8 s]: NDCG = 0.804388, loss = 0.659830 [358.49 s]
Iteration 49 [531.8 s]: NDCG = 0.804342, loss = 0.659915 [356.54 s]
Iteration 50 [525.7 s]: NDCG = 0.804322, loss = 0.659727 [359.46 s]
Iteration 51 [526.0 s]: NDCG = 0.804353, loss = 0.659868 [355.46 s]
Iteration 52 [531.0 s]: NDCG = 0.804164, loss = 0.659597 [356.30 s]
Iteration 53 [535.6 s]: NDCG = 0.804327, loss = 0.659537 [357.95 s]
Iteration 54 [526.1 s]: NDCG = 0.804380, loss = 0.659649 [355.83 s]
Iteration 55 [525.6 s]: NDCG = 0.804176, loss = 0.659703 [328.15 s]
Iteration 56 [518.4 s]: NDCG = 0.804207, loss = 0.659121 [338.52 s]
Iteration 57 [530.3 s]: NDCG = 0.804040, loss = 0.659247 [328.68 s]
Iteration 58 [517.7 s]: NDCG = 0.804397, loss = 0.659291 [331.92 s]
Iteration 59 [518.5 s]: NDCG = 0.804278, loss = 0.659183 [328.40 s]
Iteration 60 [525.5 s]: NDCG = 0.804425, loss = 0.658908 [324.03 s]
Iteration 61 [514.2 s]: NDCG = 0.804560, loss = 0.659225 [328.30 s]
Iteration 62 [516.5 s]: NDCG = 0.804634, loss = 0.658979 [327.94 s]
Iteration 63 [528.1 s]: NDCG = 0.804396, loss = 0.659100 [325.01 s]
Iteration 64 [514.5 s]: NDCG = 0.804498, loss = 0.659254 [328.31 s]
Iteration 65 [513.9 s]: NDCG = 0.804476, loss = 0.659060 [324.62 s]
Iteration 66 [520.9 s]: NDCG = 0.804430, loss = 0.658944 [324.83 s]
Iteration 67 [517.9 s]: NDCG = 0.804276, loss = 0.659117 [328.01 s]
Iteration 68 [516.4 s]: NDCG = 0.804228, loss = 0.659096 [325.00 s]
Iteration 69 [520.0 s]: NDCG = 0.804126, loss = 0.658976 [324.65 s]
Iteration 70 [513.8 s]: NDCG = 0.804302, loss = 0.658710 [328.30 s]
Iteration 71 [516.0 s]: NDCG = 0.804298, loss = 0.658632 [324.96 s]
Iteration 72 [517.6 s]: NDCG = 0.804144, loss = 0.658720 [325.31 s]
Iteration 73 [514.1 s]: NDCG = 0.804255, loss = 0.658629 [326.07 s]
Iteration 74 [520.5 s]: NDCG = 0.804424, loss = 0.658540 [325.28 s]
Iteration 75 [518.4 s]: NDCG = 0.804054, loss = 0.658611 [326.94 s]
Iteration 76 [513.0 s]: NDCG = 0.804138, loss = 0.658784 [325.56 s]
Iteration 77 [521.4 s]: NDCG = 0.804146, loss = 0.658981 [325.42 s]
Iteration 78 [515.0 s]: NDCG = 0.804313, loss = 0.658518 [328.18 s]
Iteration 79 [515.5 s]: NDCG = 0.804105, loss = 0.658377 [325.90 s]
Iteration 80 [519.8 s]: NDCG = 0.804105, loss = 0.658630 [325.46 s]
Iteration 81 [514.6 s]: NDCG = 0.804266, loss = 0.658552 [328.45 s]
Iteration 82 [514.8 s]: NDCG = 0.804230, loss = 0.658718 [325.61 s]
Iteration 83 [516.1 s]: NDCG = 0.804401, loss = 0.658605 [325.47 s]
Iteration 84 [515.9 s]: NDCG = 0.804610, loss = 0.658634 [329.02 s]
Iteration 85 [516.2 s]: NDCG = 0.804379, loss = 0.658471 [325.17 s]
Iteration 86 [517.5 s]: NDCG = 0.804403, loss = 0.658432 [325.32 s]
Iteration 87 [512.6 s]: NDCG = 0.804349, loss = 0.658414 [328.77 s]
Iteration 88 [516.4 s]: NDCG = 0.804472, loss = 0.658303 [325.60 s]
Iteration 89 [517.9 s]: NDCG = 0.804218, loss = 0.658401 [325.12 s]
Iteration 90 [517.3 s]: NDCG = 0.804192, loss = 0.657972 [329.28 s]
Iteration 91 [522.0 s]: NDCG = 0.804209, loss = 0.658486 [325.33 s]
Iteration 92 [519.0 s]: NDCG = 0.804297, loss = 0.658581 [325.30 s]
Iteration 93 [512.8 s]: NDCG = 0.804168, loss = 0.657790 [325.44 s]
Iteration 94 [520.0 s]: NDCG = 0.804145, loss = 0.658467 [335.04 s]
Iteration 95 [533.6 s]: NDCG = 0.804238, loss = 0.657901 [337.51 s]
Iteration 96 [520.3 s]: NDCG = 0.804345, loss = 0.658182 [332.76 s]
Iteration 97 [524.4 s]: NDCG = 0.804313, loss = 0.657947 [327.24 s]
Iteration 98 [521.9 s]: NDCG = 0.804346, loss = 0.658035 [330.38 s]
Iteration 99 [520.9 s]: NDCG = 0.804315, loss = 0.657864 [327.79 s]
End. Best Iteration 62: NDCG = 0.8046. 
liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.000001,0.001,0.002,0.004] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.000001,0.001,0.002,0.004]', verbose=1) 
Load data done [152.4 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Init: NDCG = 0.689205 [62.1]
Iteration 0 [91.2 s]: NDCG = 0.728712, loss = 0.890927 [62.14 s]
Iteration 1 [86.6 s]: NDCG = 0.769092, loss = 0.814880 [62.24 s]
Iteration 2 [101.5 s]: NDCG = 0.780372, loss = 0.763369 [66.69 s]
Iteration 3 [116.5 s]: NDCG = 0.787728, loss = 0.730794 [146.58 s]
Iteration 4 [155.6 s]: NDCG = 0.793646, loss = 0.711772 [260.12 s]
Iteration 5 [136.7 s]: NDCG = 0.795286, loss = 0.700656 [331.84 s]
Iteration 6 [276.3 s]: NDCG = 0.796280, loss = 0.691078 [388.57 s]
Iteration 7 [411.8 s]: NDCG = 0.797389, loss = 0.682323 [436.22 s]
Iteration 8 [402.1 s]: NDCG = 0.798568, loss = 0.677613 [427.69 s]
Iteration 9 [408.2 s]: NDCG = 0.799702, loss = 0.675365 [372.47 s]
Iteration 10 [468.2 s]: NDCG = 0.800347, loss = 0.674073 [371.06 s]
Iteration 11 [511.6 s]: NDCG = 0.800988, loss = 0.672668 [372.13 s]
Iteration 12 [509.5 s]: NDCG = 0.801474, loss = 0.671766 [376.38 s]
Iteration 13 [519.4 s]: NDCG = 0.801710, loss = 0.670962 [372.22 s]
Iteration 14 [528.8 s]: NDCG = 0.802107, loss = 0.670369 [372.64 s]
Iteration 15 [520.1 s]: NDCG = 0.802271, loss = 0.669335 [376.47 s]
Iteration 16 [520.3 s]: NDCG = 0.802632, loss = 0.668826 [377.06 s]
Iteration 17 [530.8 s]: NDCG = 0.802939, loss = 0.668206 [371.20 s]
Iteration 18 [525.3 s]: NDCG = 0.803241, loss = 0.667935 [373.57 s]
Iteration 19 [522.5 s]: NDCG = 0.803321, loss = 0.667558 [373.72 s]
Iteration 20 [525.0 s]: NDCG = 0.803493, loss = 0.667284 [371.61 s]
Iteration 21 [527.1 s]: NDCG = 0.803591, loss = 0.666658 [372.41 s]
Iteration 22 [522.4 s]: NDCG = 0.803781, loss = 0.666579 [376.01 s]
Iteration 23 [522.7 s]: NDCG = 0.803789, loss = 0.666231 [372.50 s]
Iteration 24 [527.8 s]: NDCG = 0.803971, loss = 0.666025 [372.49 s]
Iteration 25 [523.2 s]: NDCG = 0.804105, loss = 0.665209 [376.61 s]
Iteration 26 [521.9 s]: NDCG = 0.804258, loss = 0.665871 [368.44 s]
Iteration 27 [525.6 s]: NDCG = 0.804289, loss = 0.665142 [369.25 s]
Iteration 28 [519.0 s]: NDCG = 0.804265, loss = 0.665188 [372.35 s]
Iteration 29 [517.3 s]: NDCG = 0.804203, loss = 0.664838 [368.02 s]
Iteration 30 [530.1 s]: NDCG = 0.803985, loss = 0.664456 [370.80 s]
Iteration 31 [519.5 s]: NDCG = 0.804289, loss = 0.664512 [370.13 s]
Iteration 32 [514.6 s]: NDCG = 0.804292, loss = 0.664257 [364.98 s]
Iteration 33 [517.8 s]: NDCG = 0.804319, loss = 0.664094 [363.88 s]
Iteration 34 [516.3 s]: NDCG = 0.804236, loss = 0.663966 [364.06 s]
Iteration 35 [517.9 s]: NDCG = 0.804111, loss = 0.663517 [370.91 s]
Iteration 36 [517.2 s]: NDCG = 0.804290, loss = 0.663399 [363.41 s]
Iteration 37 [514.5 s]: NDCG = 0.804172, loss = 0.663230 [364.49 s]
Iteration 38 [513.5 s]: NDCG = 0.804327, loss = 0.663268 [364.27 s]
Iteration 39 [521.2 s]: NDCG = 0.804155, loss = 0.663329 [363.10 s]
Iteration 40 [520.5 s]: NDCG = 0.804445, loss = 0.662973 [363.35 s]
Iteration 41 [518.0 s]: NDCG = 0.804209, loss = 0.662822 [365.84 s]
Iteration 42 [514.1 s]: NDCG = 0.804435, loss = 0.662732 [362.39 s]
Iteration 43 [518.8 s]: NDCG = 0.804572, loss = 0.662445 [362.72 s]
Iteration 44 [518.2 s]: NDCG = 0.804488, loss = 0.662573 [365.72 s]
Iteration 45 [515.0 s]: NDCG = 0.804531, loss = 0.662607 [362.77 s]
Iteration 46 [519.7 s]: NDCG = 0.804580, loss = 0.662315 [362.78 s]
Iteration 47 [513.7 s]: NDCG = 0.804554, loss = 0.662073 [366.33 s]
Iteration 48 [514.0 s]: NDCG = 0.804624, loss = 0.662119 [362.64 s]
Iteration 49 [521.6 s]: NDCG = 0.804695, loss = 0.662008 [362.57 s]
Iteration 50 [513.2 s]: NDCG = 0.804737, loss = 0.662356 [366.07 s]
Iteration 51 [515.3 s]: NDCG = 0.804770, loss = 0.662159 [363.06 s]
Iteration 52 [518.3 s]: NDCG = 0.804760, loss = 0.661889 [362.62 s]
Iteration 53 [516.5 s]: NDCG = 0.804475, loss = 0.661822 [366.68 s]
Iteration 54 [512.7 s]: NDCG = 0.804537, loss = 0.662011 [362.58 s]
Iteration 55 [518.1 s]: NDCG = 0.804500, loss = 0.661610 [361.87 s]
Iteration 56 [515.5 s]: NDCG = 0.804564, loss = 0.661994 [366.61 s]
Iteration 57 [516.8 s]: NDCG = 0.804558, loss = 0.661838 [362.40 s]
Iteration 58 [520.0 s]: NDCG = 0.804591, loss = 0.661950 [363.00 s]
Iteration 59 [514.4 s]: NDCG = 0.804598, loss = 0.661739 [366.21 s]
Iteration 60 [514.6 s]: NDCG = 0.804456, loss = 0.661323 [362.78 s]
Iteration 61 [522.4 s]: NDCG = 0.804564, loss = 0.661542 [370.08 s]
Iteration 62 [526.3 s]: NDCG = 0.804542, loss = 0.661078 [372.33 s]
Iteration 63 [512.6 s]: NDCG = 0.804597, loss = 0.661236 [362.89 s]
Iteration 64 [518.8 s]: NDCG = 0.804616, loss = 0.661612 [362.64 s]
Iteration 65 [513.9 s]: NDCG = 0.804485, loss = 0.661227 [366.39 s]
Iteration 66 [515.1 s]: NDCG = 0.804546, loss = 0.661166 [362.01 s]
Iteration 67 [513.6 s]: NDCG = 0.804474, loss = 0.661070 [363.00 s]
Iteration 68 [516.2 s]: NDCG = 0.804416, loss = 0.661028 [362.92 s]
Iteration 69 [513.3 s]: NDCG = 0.804362, loss = 0.661139 [363.13 s]
Iteration 70 [516.4 s]: NDCG = 0.804484, loss = 0.660892 [363.53 s]
Iteration 71 [517.2 s]: NDCG = 0.804422, loss = 0.660862 [362.53 s]
Iteration 72 [514.1 s]: NDCG = 0.804537, loss = 0.660983 [362.97 s]
Iteration 73 [513.8 s]: NDCG = 0.804734, loss = 0.660609 [362.10 s]
Iteration 74 [514.0 s]: NDCG = 0.804586, loss = 0.660772 [362.76 s]
Iteration 75 [515.0 s]: NDCG = 0.804627, loss = 0.660580 [363.14 s]
Iteration 76 [514.2 s]: NDCG = 0.804658, loss = 0.661026 [363.82 s]
Iteration 77 [517.2 s]: NDCG = 0.804683, loss = 0.660847 [363.24 s]
Iteration 78 [512.8 s]: NDCG = 0.804698, loss = 0.660888 [362.91 s]
Iteration 79 [513.1 s]: NDCG = 0.804589, loss = 0.660514 [362.78 s]
Iteration 80 [513.5 s]: NDCG = 0.804472, loss = 0.660727 [362.24 s]
Iteration 81 [513.0 s]: NDCG = 0.804535, loss = 0.660652 [361.94 s]
Iteration 82 [513.1 s]: NDCG = 0.804573, loss = 0.660696 [361.90 s]
Iteration 83 [514.8 s]: NDCG = 0.804685, loss = 0.660544 [361.96 s]
Iteration 84 [514.6 s]: NDCG = 0.804802, loss = 0.660588 [362.16 s]
Iteration 85 [512.9 s]: NDCG = 0.804903, loss = 0.660419 [362.20 s]
Iteration 86 [512.9 s]: NDCG = 0.804747, loss = 0.660645 [362.67 s]
Iteration 87 [514.4 s]: NDCG = 0.804779, loss = 0.660535 [362.31 s]
Iteration 88 [513.7 s]: NDCG = 0.804758, loss = 0.660592 [362.28 s]
Iteration 89 [512.6 s]: NDCG = 0.804727, loss = 0.660401 [362.17 s]
Iteration 90 [512.4 s]: NDCG = 0.804768, loss = 0.660269 [362.29 s]
Iteration 91 [512.9 s]: NDCG = 0.804724, loss = 0.660400 [362.16 s]
Iteration 92 [518.2 s]: NDCG = 0.804750, loss = 0.660307 [362.42 s]
Iteration 93 [513.5 s]: NDCG = 0.804708, loss = 0.660086 [362.18 s]
Iteration 94 [517.7 s]: NDCG = 0.804745, loss = 0.660393 [362.36 s]
Iteration 95 [514.9 s]: NDCG = 0.804666, loss = 0.660372 [362.31 s]
Iteration 96 [512.9 s]: NDCG = 0.804608, loss = 0.660386 [362.19 s]
Iteration 97 [513.4 s]: NDCG = 0.804556, loss = 0.660115 [362.15 s]
Iteration 98 [513.3 s]: NDCG = 0.804712, loss = 0.660480 [362.52 s]
Iteration 99 [513.1 s]: NDCG = 0.804535, loss = 0.660194 [362.07 s]
End. Best Iteration 85: NDCG = 0.8049. 


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.000005,0.0002,0.001,0.005] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.000005,0.0002,0.001,0.005]', verbose=1) 
Load data done [146.7 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Init: NDCG = 0.689153 [62.2]
Iteration 0 [100.1 s]: NDCG = 0.765244, loss = 0.838421 [61.35 s]
Iteration 1 [194.0 s]: NDCG = 0.784889, loss = 0.791637 [88.28 s]
Iteration 2 [344.5 s]: NDCG = 0.792662, loss = 0.755994 [141.64 s]
Iteration 3 [365.5 s]: NDCG = 0.795929, loss = 0.728508 [224.20 s]
Iteration 4 [370.5 s]: NDCG = 0.797237, loss = 0.706689 [295.29 s]
Iteration 5 [376.1 s]: NDCG = 0.798950, loss = 0.690748 [334.36 s]
Iteration 6 [381.9 s]: NDCG = 0.800328, loss = 0.680878 [391.36 s]
Iteration 7 [383.9 s]: NDCG = 0.801071, loss = 0.675329 [366.72 s]
Iteration 8 [392.0 s]: NDCG = 0.801716, loss = 0.672095 [322.14 s]
Iteration 9 [414.6 s]: NDCG = 0.801997, loss = 0.670034 [319.09 s]
Iteration 10 [453.6 s]: NDCG = 0.802305, loss = 0.668727 [318.90 s]
Iteration 11 [509.3 s]: NDCG = 0.802556, loss = 0.668135 [319.02 s]
Iteration 12 [518.0 s]: NDCG = 0.802989, loss = 0.667090 [318.96 s]
Iteration 13 [517.6 s]: NDCG = 0.802938, loss = 0.666853 [318.94 s]
Iteration 14 [517.6 s]: NDCG = 0.803008, loss = 0.665871 [318.91 s]
Iteration 15 [518.5 s]: NDCG = 0.803176, loss = 0.665611 [318.92 s]
Iteration 16 [519.4 s]: NDCG = 0.803149, loss = 0.665158 [319.28 s]
Iteration 17 [522.2 s]: NDCG = 0.803304, loss = 0.664888 [319.15 s]
Iteration 18 [522.8 s]: NDCG = 0.803341, loss = 0.664744 [318.95 s]
Iteration 19 [522.8 s]: NDCG = 0.803509, loss = 0.664125 [318.99 s]
Iteration 20 [523.3 s]: NDCG = 0.803559, loss = 0.663985 [318.78 s]
Iteration 21 [522.6 s]: NDCG = 0.803597, loss = 0.663527 [318.80 s]
Iteration 22 [523.0 s]: NDCG = 0.803515, loss = 0.663677 [318.78 s]
Iteration 23 [522.7 s]: NDCG = 0.803626, loss = 0.663322 [319.65 s]
Iteration 24 [522.7 s]: NDCG = 0.803470, loss = 0.663305 [318.86 s]
Iteration 25 [525.9 s]: NDCG = 0.803730, loss = 0.662907 [318.89 s]
Iteration 26 [523.2 s]: NDCG = 0.803786, loss = 0.662899 [318.43 s]
Iteration 27 [523.1 s]: NDCG = 0.803600, loss = 0.662532 [318.49 s]
Iteration 28 [523.0 s]: NDCG = 0.803755, loss = 0.662264 [318.46 s]
Iteration 29 [523.8 s]: NDCG = 0.803682, loss = 0.662096 [318.56 s]
Iteration 30 [523.1 s]: NDCG = 0.803546, loss = 0.661983 [318.41 s]
Iteration 31 [523.7 s]: NDCG = 0.803812, loss = 0.662141 [317.35 s]
Iteration 32 [525.8 s]: NDCG = 0.803595, loss = 0.661601 [317.38 s]
Iteration 33 [522.7 s]: NDCG = 0.803507, loss = 0.661830 [317.13 s]
Iteration 34 [522.9 s]: NDCG = 0.803668, loss = 0.661763 [317.06 s]
Iteration 35 [536.0 s]: NDCG = 0.803654, loss = 0.661780 [317.34 s]
Iteration 36 [522.5 s]: NDCG = 0.803588, loss = 0.661617 [317.22 s]
Iteration 37 [526.1 s]: NDCG = 0.803698, loss = 0.661123 [317.13 s]
Iteration 38 [523.0 s]: NDCG = 0.803572, loss = 0.660891 [316.98 s]
Iteration 39 [522.8 s]: NDCG = 0.803509, loss = 0.661206 [317.38 s]
Iteration 40 [522.2 s]: NDCG = 0.803599, loss = 0.660864 [317.24 s]
Iteration 41 [523.9 s]: NDCG = 0.803720, loss = 0.660659 [317.29 s]
Iteration 42 [522.4 s]: NDCG = 0.803634, loss = 0.660563 [316.94 s]
Iteration 43 [524.7 s]: NDCG = 0.803589, loss = 0.660797 [316.72 s]
Iteration 44 [524.7 s]: NDCG = 0.803388, loss = 0.660504 [316.61 s]
Iteration 45 [522.7 s]: NDCG = 0.803371, loss = 0.660733 [316.75 s]
Iteration 46 [524.2 s]: NDCG = 0.803491, loss = 0.660419 [316.59 s]
Iteration 47 [522.3 s]: NDCG = 0.803481, loss = 0.660296 [317.14 s]
Iteration 48 [523.5 s]: NDCG = 0.803672, loss = 0.660293 [325.61 s]
Iteration 49 [529.2 s]: NDCG = 0.803625, loss = 0.659988 [351.39 s]
Iteration 50 [541.7 s]: NDCG = 0.803668, loss = 0.659898 [352.27 s]
Iteration 51 [532.2 s]: NDCG = 0.803412, loss = 0.660103 [337.78 s]
Iteration 52 [532.0 s]: NDCG = 0.803586, loss = 0.659677 [337.52 s]
Iteration 53 [536.5 s]: NDCG = 0.803572, loss = 0.659591 [339.27 s]
Iteration 54 [535.5 s]: NDCG = 0.803589, loss = 0.659650 [343.36 s]
Iteration 55 [554.7 s]: NDCG = 0.803530, loss = 0.659763 [345.14 s]
Iteration 56 [534.1 s]: NDCG = 0.803439, loss = 0.659226 [327.24 s]
Iteration 57 [527.5 s]: NDCG = 0.803267, loss = 0.659494 [320.05 s]
Iteration 58 [542.2 s]: NDCG = 0.803121, loss = 0.659409 [324.13 s]
Iteration 59 [547.1 s]: NDCG = 0.803179, loss = 0.659626 [322.87 s]
Iteration 60 [532.2 s]: NDCG = 0.802990, loss = 0.659081 [319.38 s]
Iteration 61 [525.6 s]: NDCG = 0.803039, loss = 0.659056 [339.23 s]
Iteration 62 [534.5 s]: NDCG = 0.802984, loss = 0.658993 [318.34 s]
Iteration 63 [523.8 s]: NDCG = 0.802891, loss = 0.659160 [317.17 s]
Iteration 64 [522.2 s]: NDCG = 0.802921, loss = 0.658865 [334.57 s]
Iteration 65 [539.1 s]: NDCG = 0.802673, loss = 0.658789 [351.34 s]
Iteration 66 [538.6 s]: NDCG = 0.802612, loss = 0.658524 [343.69 s]
Iteration 67 [536.2 s]: NDCG = 0.802662, loss = 0.658941 [322.00 s]
Iteration 68 [523.7 s]: NDCG = 0.802540, loss = 0.658577 [319.19 s]
Iteration 69 [539.9 s]: NDCG = 0.802901, loss = 0.658457 [318.62 s]
Iteration 70 [524.3 s]: NDCG = 0.802631, loss = 0.658541 [318.51 s]
Iteration 71 [523.9 s]: NDCG = 0.802656, loss = 0.658387 [318.91 s]
Iteration 72 [542.5 s]: NDCG = 0.802649, loss = 0.658482 [353.80 s]
Iteration 73 [544.3 s]: NDCG = 0.802456, loss = 0.657974 [342.19 s]
Iteration 74 [533.2 s]: NDCG = 0.802507, loss = 0.658085 [353.31 s]
Iteration 75 [548.7 s]: NDCG = 0.802672, loss = 0.658473 [347.10 s]
Iteration 76 [542.4 s]: NDCG = 0.802483, loss = 0.658051 [351.97 s]
Iteration 77 [537.5 s]: NDCG = 0.802189, loss = 0.658190 [339.02 s]
Iteration 78 [524.0 s]: NDCG = 0.802260, loss = 0.657777 [323.86 s]
Iteration 79 [536.4 s]: NDCG = 0.802367, loss = 0.657727 [351.03 s]
Iteration 80 [533.1 s]: NDCG = 0.802081, loss = 0.657817 [322.37 s]
Iteration 81 [525.6 s]: NDCG = 0.802067, loss = 0.657479 [318.51 s]
Iteration 82 [524.4 s]: NDCG = 0.802292, loss = 0.658068 [343.43 s]
Iteration 83 [537.3 s]: NDCG = 0.802306, loss = 0.657676 [349.34 s]
Iteration 84 [537.0 s]: NDCG = 0.802125, loss = 0.657536 [352.19 s]
Iteration 85 [542.1 s]: NDCG = 0.801969, loss = 0.657473 [330.34 s]
Iteration 86 [524.6 s]: NDCG = 0.801825, loss = 0.657866 [319.60 s]
Iteration 87 [524.9 s]: NDCG = 0.801883, loss = 0.657173 [319.55 s]
Iteration 88 [525.6 s]: NDCG = 0.801756, loss = 0.657412 [318.31 s]
Iteration 89 [524.1 s]: NDCG = 0.801530, loss = 0.657637 [318.16 s]
Iteration 90 [524.9 s]: NDCG = 0.801718, loss = 0.657092 [326.92 s]
Iteration 91 [525.2 s]: NDCG = 0.801885, loss = 0.657324 [324.21 s]
Iteration 92 [529.2 s]: NDCG = 0.801832, loss = 0.657345 [352.02 s]
Iteration 93 [535.9 s]: NDCG = 0.801831, loss = 0.657096 [351.23 s]
Iteration 94 [535.3 s]: NDCG = 0.801734, loss = 0.657262 [351.83 s]
Iteration 95 [527.4 s]: NDCG = 0.801844, loss = 0.657120 [348.53 s]
Iteration 96 [535.7 s]: NDCG = 0.801630, loss = 0.656997 [350.87 s]
Iteration 97 [535.6 s]: NDCG = 0.801830, loss = 0.657034 [350.93 s]
Iteration 98 [529.6 s]: NDCG = 0.801918, loss = 0.657348 [346.21 s]
Iteration 99 [527.1 s]: NDCG = 0.801711, loss = 0.656988 [339.12 s]
End. Best Iteration 31: NDCG = 0.8038. 


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.0000001,0.001,0.002,0.004] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.0000001,0.001,0.002,0.004]', verbose=1) 
Load data done [145.0 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Init: NDCG = 0.690499 [62.1]
Iteration 0 [91.7 s]: NDCG = 0.710852, loss = 0.894917 [63.02 s]
Iteration 1 [86.7 s]: NDCG = 0.751785, loss = 0.817874 [64.48 s]
Iteration 2 [86.9 s]: NDCG = 0.780424, loss = 0.765643 [79.08 s]
Iteration 3 [90.6 s]: NDCG = 0.788890, loss = 0.732620 [208.77 s]
Iteration 4 [96.0 s]: NDCG = 0.794448, loss = 0.713426 [321.53 s]
Iteration 5 [100.1 s]: NDCG = 0.795618, loss = 0.702101 [386.50 s]
Iteration 6 [104.5 s]: NDCG = 0.796134, loss = 0.692350 [441.69 s]
Iteration 7 [109.5 s]: NDCG = 0.796958, loss = 0.683094 [441.57 s]
Iteration 8 [113.0 s]: NDCG = 0.798199, loss = 0.678020 [445.70 s]
Iteration 9 [114.3 s]: NDCG = 0.798948, loss = 0.675122 [445.80 s]
Iteration 10 [114.9 s]: NDCG = 0.799793, loss = 0.673844 [447.49 s]
Iteration 11 [114.8 s]: NDCG = 0.800363, loss = 0.672734 [447.82 s]
Iteration 12 [115.7 s]: NDCG = 0.800763, loss = 0.671529 [447.33 s]
Iteration 13 [115.2 s]: NDCG = 0.801071, loss = 0.670703 [446.94 s]
Iteration 14 [115.6 s]: NDCG = 0.801496, loss = 0.669896 [442.62 s]
Iteration 15 [114.9 s]: NDCG = 0.801669, loss = 0.669424 [436.68 s]
Iteration 16 [118.9 s]: NDCG = 0.802121, loss = 0.668691 [437.22 s]
Iteration 17 [123.0 s]: NDCG = 0.802431, loss = 0.668369 [431.23 s]
Iteration 18 [127.6 s]: NDCG = 0.802800, loss = 0.667849 [419.78 s]
Iteration 19 [132.5 s]: NDCG = 0.802813, loss = 0.667169 [395.88 s]
Iteration 20 [137.4 s]: NDCG = 0.802882, loss = 0.667190 [372.85 s]
Iteration 21 [147.5 s]: NDCG = 0.803245, loss = 0.666701 [366.56 s]
Iteration 22 [165.2 s]: NDCG = 0.803255, loss = 0.666120 [365.81 s]
Iteration 23 [192.7 s]: NDCG = 0.803295, loss = 0.666020 [365.82 s]
Iteration 24 [227.4 s]: NDCG = 0.803538, loss = 0.665384 [365.77 s]
Iteration 25 [245.3 s]: NDCG = 0.803686, loss = 0.665479 [364.74 s]
Iteration 26 [245.8 s]: NDCG = 0.803595, loss = 0.665165 [362.82 s]
Iteration 27 [245.1 s]: NDCG = 0.803819, loss = 0.665018 [362.25 s]
Iteration 28 [248.9 s]: NDCG = 0.803848, loss = 0.664634 [365.25 s]
Iteration 29 [245.4 s]: NDCG = 0.804149, loss = 0.664465 [362.12 s]
Iteration 30 [247.4 s]: NDCG = 0.804013, loss = 0.664293 [362.06 s]
Iteration 31 [248.2 s]: NDCG = 0.803891, loss = 0.664135 [362.31 s]
Iteration 32 [247.5 s]: NDCG = 0.803933, loss = 0.664008 [361.90 s]
Iteration 33 [240.6 s]: NDCG = 0.804050, loss = 0.663852 [362.13 s]
Iteration 34 [239.5 s]: NDCG = 0.804008, loss = 0.663337 [362.28 s]
Iteration 35 [237.5 s]: NDCG = 0.804075, loss = 0.663257 [362.91 s]
Iteration 36 [236.1 s]: NDCG = 0.804205, loss = 0.663175 [362.50 s]
Iteration 37 [246.3 s]: NDCG = 0.804186, loss = 0.662902 [362.56 s]
Iteration 38 [265.0 s]: NDCG = 0.804182, loss = 0.663041 [362.27 s]
Iteration 39 [290.6 s]: NDCG = 0.804290, loss = 0.662893 [362.28 s]
Iteration 40 [305.9 s]: NDCG = 0.804184, loss = 0.662875 [363.29 s]
Iteration 41 [307.3 s]: NDCG = 0.804248, loss = 0.662504 [363.60 s]
Iteration 42 [307.4 s]: NDCG = 0.804138, loss = 0.662626 [362.64 s]
Iteration 43 [313.4 s]: NDCG = 0.804177, loss = 0.662128 [362.24 s]
Iteration 44 [305.6 s]: NDCG = 0.804276, loss = 0.662156 [362.15 s]
Iteration 45 [301.5 s]: NDCG = 0.804117, loss = 0.661918 [362.15 s]
Iteration 46 [289.7 s]: NDCG = 0.804186, loss = 0.662202 [361.94 s]
Iteration 47 [268.7 s]: NDCG = 0.804158, loss = 0.661669 [362.24 s]
Iteration 48 [247.2 s]: NDCG = 0.804189, loss = 0.661839 [362.36 s]
Iteration 49 [240.7 s]: NDCG = 0.804207, loss = 0.661575 [362.44 s]
Iteration 50 [239.7 s]: NDCG = 0.804181, loss = 0.661395 [362.70 s]
Iteration 51 [234.6 s]: NDCG = 0.804002, loss = 0.661570 [362.67 s]
Iteration 52 [233.6 s]: NDCG = 0.804038, loss = 0.661096 [362.42 s]
Iteration 53 [242.3 s]: NDCG = 0.804150, loss = 0.661354 [362.83 s]
Iteration 54 [265.8 s]: NDCG = 0.804319, loss = 0.661193 [362.13 s]
Iteration 55 [316.0 s]: NDCG = 0.804294, loss = 0.661287 [362.52 s]
Iteration 56 [382.9 s]: NDCG = 0.804297, loss = 0.661234 [362.41 s]
Iteration 57 [415.7 s]: NDCG = 0.804400, loss = 0.661125 [362.03 s]
Iteration 58 [414.3 s]: NDCG = 0.804220, loss = 0.661074 [362.10 s]
Iteration 59 [411.8 s]: NDCG = 0.804294, loss = 0.660925 [361.94 s]
Iteration 60 [415.6 s]: NDCG = 0.804297, loss = 0.660879 [362.10 s]
Iteration 61 [418.2 s]: NDCG = 0.804354, loss = 0.660612 [361.35 s]
Iteration 62 [422.6 s]: NDCG = 0.804213, loss = 0.660774 [361.38 s]
Iteration 63 [434.4 s]: NDCG = 0.804317, loss = 0.660610 [360.37 s]
Iteration 64 [444.4 s]: NDCG = 0.804491, loss = 0.660312 [361.07 s]
Iteration 65 [460.7 s]: NDCG = 0.804368, loss = 0.660624 [361.62 s]
Iteration 66 [496.2 s]: NDCG = 0.804471, loss = 0.660322 [360.42 s]
Iteration 67 [554.4 s]: NDCG = 0.804270, loss = 0.660293 [360.85 s]
Iteration 68 [602.5 s]: NDCG = 0.804275, loss = 0.660334 [383.77 s]
Iteration 69 [638.3 s]: NDCG = 0.804219, loss = 0.660134 [373.70 s]
Iteration 70 [604.7 s]: NDCG = 0.804227, loss = 0.660140 [361.02 s]
Iteration 71 [583.0 s]: NDCG = 0.804196, loss = 0.659955 [361.32 s]
Iteration 72 [545.2 s]: NDCG = 0.804220, loss = 0.659921 [361.86 s]
Iteration 73 [525.4 s]: NDCG = 0.804178, loss = 0.659585 [398.54 s]
Iteration 74 [529.3 s]: NDCG = 0.804422, loss = 0.659640 [394.28 s]
Iteration 75 [530.1 s]: NDCG = 0.804391, loss = 0.659721 [398.81 s]
Iteration 76 [532.6 s]: NDCG = 0.804291, loss = 0.659749 [400.06 s]
Iteration 77 [526.6 s]: NDCG = 0.804427, loss = 0.660056 [374.23 s]
Iteration 78 [530.8 s]: NDCG = 0.804386, loss = 0.659850 [378.79 s]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0,0,0,0] --lr 0.00001Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [555.2 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [69.3], Init: NDCG = 
[ 0.5595902   0.56518256  0.57256333  0.58288201  0.59470351  0.60933644
  0.62604618  0.64517868  0.66550665  0.68769883]
Iteration 0 [99.5 s]: loss = 0.692808 [68.54 s], NDCG = 
[ 0.587371    0.58866342  0.59639719  0.60652584  0.6177791   0.63147441
  0.64713017  0.66463375  0.68326196  0.70384562]
Iteration 1 [125.5 s]: loss = 0.687194 [65.78 s], NDCG = 
[ 0.6774106   0.67366121  0.67555743  0.68101216  0.68958391  0.70004316
  0.71262471  0.72676709  0.74200611  0.75816057]
Iteration 2 [96.8 s]: loss = 0.671273 [65.35 s], NDCG = 
[ 0.69406393  0.6906631   0.69263249  0.69843751  0.7065182   0.71657128
  0.72856293  0.74222645  0.75683733  0.77215272]
Iteration 3 [96.4 s]: loss = 0.660526 [68.51 s], NDCG = 
[ 0.69190092  0.68969364  0.69174906  0.69716715  0.70490018  0.71470586
  0.72698437  0.74029751  0.75454512  0.77018923]
Iteration 4 [99.6 s]: loss = 0.656935 [66.29 s], NDCG = 
[ 0.68204357  0.68011525  0.6818065   0.6869959   0.69445722  0.70523966
  0.71751455  0.73108163  0.74588718  0.76189078]
Iteration 5 [91.1 s]: loss = 0.654830 [63.54 s], NDCG = 
[ 0.6718501   0.6684566   0.67084109  0.67572918  0.68327406  0.69478511
  0.70734494  0.72159503  0.73694455  0.75357104]
Iteration 6 [90.8 s]: loss = 0.653200 [63.47 s], NDCG = 
[ 0.66252993  0.65907519  0.66137945  0.66696017  0.67561172  0.68679676
  0.69980861  0.71445186  0.7301349   0.74696249]
Iteration 7 [90.9 s]: loss = 0.652059 [64.02 s], NDCG = 
[ 0.65727896  0.6532534   0.65570641  0.66096851  0.6696859   0.68066267
  0.69471058  0.70923316  0.72511318  0.74259519]
Iteration 8 [91.0 s]: loss = 0.650734 [64.36 s], NDCG = 
[ 0.6493434   0.64545348  0.64845616  0.65471786  0.66326627  0.67467255
  0.68863224  0.70373449  0.72019561  0.73789527]
Iteration 9 [93.7 s]: loss = 0.649742 [72.41 s], NDCG = 
[ 0.64470934  0.64113383  0.64449384  0.65029634  0.65967945  0.67129426
  0.68521356  0.7003066   0.71712959  0.73497209]
Iteration 10 [99.2 s]: loss = 0.648783 [73.87 s], NDCG = 
[ 0.63947284  0.63671855  0.64003407  0.64607338  0.65582679  0.66762535
  0.68154216  0.69697802  0.71383713  0.731805  ]
Iteration 11 [99.1 s]: loss = 0.648074 [73.91 s], NDCG = 
[ 0.63456995  0.63198011  0.63546163  0.6424346   0.65204642  0.66385764
  0.67767358  0.69343223  0.71086437  0.72892816]
Iteration 12 [99.3 s]: loss = 0.646921 [74.03 s], NDCG = 
[ 0.62861875  0.62762621  0.63153586  0.63893734  0.6487785   0.66068589
  0.67469384  0.69086339  0.70806784  0.72653829]
Iteration 13 [99.6 s]: loss = 0.646205 [74.07 s], NDCG = 
[ 0.62396394  0.62332373  0.62819983  0.63527777  0.64485084  0.65688287
  0.67135367  0.6879702   0.7049499   0.72373281]
Iteration 14 [99.0 s]: loss = 0.644416 [74.34 s], NDCG = 
[ 0.61966853  0.62049637  0.62499831  0.63262114  0.64227771  0.65459552
  0.66925621  0.68588121  0.70309733  0.72194839]
Iteration 15 [99.1 s]: loss = 0.643586 [74.55 s], NDCG = 
[ 0.61658775  0.61671619  0.62142889  0.62970506  0.63880829  0.65156486
  0.66647994  0.68321395  0.7008734   0.71953068]
Iteration 16 [99.4 s]: loss = 0.642117 [74.47 s], NDCG = 
[ 0.61310325  0.61318369  0.61823501  0.62652332  0.63611013  0.64879543
  0.66367435  0.6805419   0.69824098  0.71737715]
Iteration 17 [99.2 s]: loss = 0.640762 [74.90 s], NDCG = 
[ 0.61028816  0.60972977  0.61517678  0.62363373  0.63333004  0.64663591
  0.66184254  0.67872044  0.6964477   0.71579212]
Iteration 18 [99.7 s]: loss = 0.638669 [74.19 s], NDCG = 
[ 0.60646471  0.6063404   0.61268664  0.62134214  0.63164929  0.64472772
  0.65985769  0.67674606  0.69463675  0.71404851]
Iteration 19 [99.2 s]: loss = 0.637016 [74.07 s], NDCG = 
[ 0.60226456  0.6035049   0.60948862  0.61808154  0.6284706   0.64210142
  0.65722748  0.67423561  0.69224527  0.71198102]
Iteration 20 [100.5 s]: loss = 0.635094 [72.56 s], NDCG = 
[ 0.59766289  0.59990153  0.60643935  0.61454034  0.62582226  0.63954967
  0.65465523  0.672038    0.690218    0.71002645]


