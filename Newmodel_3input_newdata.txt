liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0,0,0,0] --lr 0.00001Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [34.1 s]. #user=13679, #item=12922, #train=3914489, #test=316795
Time: [75.0], Init: NDCG = 
[ 0.56024489  0.56505958  0.57290191  0.58420038  0.59637996  0.61086083
  0.62820626  0.64637612  0.66642834  0.68888093]
Iteration 0 [105.5 s]: loss = 0.621487 [74.64 s], NDCG = 
[ 0.62682718  0.62511986  0.62843825  0.63456777  0.64425489  0.65624899
  0.67004719  0.68634783  0.70411161  0.72362705]
Iteration 1 [103.8 s]: loss = 0.398330 [73.40 s], NDCG = 
[ 0.64158365  0.63980445  0.64223778  0.64954288  0.65939389  0.67145542
  0.68512562  0.70078622  0.71768815  0.73613272]
Iteration 2 [101.0 s]: loss = 0.345164 [70.61 s], NDCG = 
[ 0.67841587  0.67342875  0.67544218  0.68049866  0.68928177  0.69895647
  0.71175886  0.72564962  0.74115557  0.75791551]
Iteration 3 [98.1 s]: loss = 0.326184 [70.19 s], NDCG = 
[ 0.70932506  0.70511536  0.70507721  0.70743638  0.71313357  0.72161522
  0.73321495  0.74599212  0.75990298  0.77573155]
Iteration 4 [100.7 s]: loss = 0.315916 [72.37 s], NDCG = 
[ 0.71330479  0.70921918  0.70920167  0.71197748  0.71797802  0.72684505
  0.7378137   0.75017972  0.76413497  0.7795069 ]
Iteration 5 [100.2 s]: loss = 0.310616 [72.81 s], NDCG = 
[ 0.7110082   0.70715393  0.70718913  0.71078542  0.71709132  0.72615643
  0.7375167   0.75012634  0.7639884   0.77912031]
Iteration 6 [99.8 s]: loss = 0.308103 [72.59 s], NDCG = 
[ 0.70753564  0.70439852  0.70467983  0.70900292  0.71544419  0.7246809
  0.73607288  0.74881608  0.7630316   0.77836331]
Iteration 7 [100.5 s]: loss = 0.306076 [72.61 s], NDCG = 
[ 0.70657475  0.70272699  0.70281421  0.70732294  0.71406469  0.72332506
  0.73484407  0.7477657   0.76204002  0.77758333]
Iteration 8 [100.4 s]: loss = 0.304194 [72.54 s], NDCG = 
[ 0.70556826  0.70055895  0.70122254  0.70547826  0.71188571  0.72136852
  0.73289899  0.74637385  0.76081748  0.7761841 ]
Iteration 9 [100.9 s]: loss = 0.302906 [72.67 s], NDCG = 
[ 0.70316665  0.69806718  0.69910877  0.70313469  0.71025221  0.71976508
  0.73149092  0.74492226  0.7594836   0.77494397]
Iteration 10 [102.9 s]: loss = 0.301357 [75.21 s], NDCG = 
[ 0.70216362  0.69628385  0.69684801  0.7011673   0.70844673  0.71828329
  0.72961915  0.74341489  0.75794611  0.77376492]
Iteration 11 [102.2 s]: loss = 0.300214 [75.26 s], NDCG = 
[ 0.70167468  0.69504273  0.69564886  0.70011214  0.70720909  0.71660708
  0.72858024  0.74225467  0.75715349  0.7728931 ]
Iteration 12 [102.9 s]: loss = 0.299347 [74.97 s], NDCG = 
[ 0.70024717  0.69368832  0.69434639  0.69863465  0.70612597  0.71568464
  0.72739104  0.74094733  0.755781    0.77172046]
Iteration 13 [102.7 s]: loss = 0.298111 [75.03 s], NDCG = 
[ 0.69810654  0.69157923  0.69256875  0.69652057  0.70409612  0.71377958
  0.72565182  0.7396384   0.75454234  0.77054655]
Iteration 14 [101.9 s]: loss = 0.297077 [74.90 s], NDCG = 
[ 0.69699975  0.69069826  0.69170188  0.69547121  0.70276144  0.71240253
  0.72451561  0.73838204  0.75348351  0.76957262]
Iteration 15 [102.6 s]: loss = 0.296466 [74.79 s], NDCG = 
[ 0.69601968  0.68959391  0.69046987  0.6939243   0.70140793  0.71144531
  0.72336801  0.73756258  0.75259429  0.7688694 ]
Iteration 16 [102.6 s]: loss = 0.295214 [74.80 s], NDCG = 
[ 0.69281187  0.6885814   0.68975481  0.6927173   0.70046806  0.71036398
  0.72231478  0.73628357  0.75144287  0.76790242]
Iteration 17 [103.0 s]: loss = 0.293610 [74.94 s], NDCG = 
[ 0.69305367  0.68762723  0.68814017  0.69166996  0.69970712  0.70952592
  0.72179484  0.73557192  0.75082737  0.76715096]
Iteration 18 [103.1 s]: loss = 0.293135 [74.85 s], NDCG = 
[ 0.69142272  0.68750395  0.68797947  0.69093583  0.69870742  0.70884727
  0.72090845  0.73489266  0.7503045   0.76649155]
Iteration 19 [102.5 s]: loss = 0.292460 [75.26 s], NDCG = 
[ 0.6905681   0.68672255  0.6870471   0.68972253  0.69813044  0.70804048
  0.72004847  0.73424978  0.74962812  0.76599197]
Iteration 20 [101.9 s]: loss = 0.291415 [74.98 s], NDCG = 
[ 0.68983769  0.68524887  0.68550657  0.68896594  0.69701025  0.70710904
  0.71940879  0.73343987  0.74880671  0.76529945]
Iteration 21 [102.8 s]: loss = 0.290088 [74.92 s], NDCG = 
[ 0.68989617  0.68505257  0.68502531  0.68851682  0.6963659   0.70631268
  0.71881297  0.73274337  0.7482912   0.76473435]
Iteration 22 [103.3 s]: loss = 0.289704 [75.62 s], NDCG = 
[ 0.68812656  0.68393886  0.68380282  0.68781307  0.69545957  0.70551404
  0.71814861  0.73196842  0.74762973  0.76418995]
Iteration 23 [103.1 s]: loss = 0.288769 [75.86 s], NDCG = 
[ 0.68828441  0.6828988   0.68347334  0.68706398  0.69451151  0.7049165
  0.71748535  0.73140414  0.74692617  0.76371535]
Iteration 24 [103.4 s]: loss = 0.287768 [75.31 s], NDCG = 
[ 0.68708895  0.68135108  0.68218799  0.68597156  0.69333675  0.70395019
  0.71672703  0.73047775  0.74600374  0.762947  ]
Iteration 25 [101.5 s]: loss = 0.287432 [73.01 s], NDCG = 
[ 0.68673962  0.68069198  0.68137286  0.68541116  0.69297886  0.70352693
  0.71648486  0.72999266  0.74570165  0.76242756]
Iteration 26 [100.4 s]: loss = 0.286411 [72.73 s], NDCG = 
[ 0.6863601   0.67966966  0.68011083  0.68464665  0.69202272  0.70261147
  0.71580713  0.72933816  0.74506868  0.76190911]
Iteration 27 [100.9 s]: loss = 0.285916 [72.64 s], NDCG = 
[ 0.68474865  0.67847216  0.67876485  0.6837259   0.69140035  0.70179396
  0.71495777  0.72873587  0.74440455  0.76125904]
Iteration 28 [100.8 s]: loss = 0.285522 [72.69 s], NDCG = 
[ 0.682758    0.67814778  0.67797325  0.68291734  0.69041424  0.7010668
  0.71410305  0.72769701  0.74370716  0.76066288]
Iteration 29 [100.7 s]: loss = 0.285216 [72.46 s], NDCG = 
[ 0.680873    0.67584842  0.67688503  0.6815398   0.68902947  0.69996349
  0.71294998  0.72686734  0.74288606  0.75995183]
Iteration 30 [101.1 s]: loss = 0.284479 [74.81 s], NDCG = 
[ 0.67974954  0.67531753  0.67595379  0.68072792  0.68828654  0.69945961
  0.71234248  0.72628749  0.74245682  0.75949463]
Iteration 31 [102.3 s]: loss = 0.284165 [72.82 s], NDCG = 
[ 0.67860376  0.67395393  0.67532418  0.67972272  0.68780408  0.69872591
  0.71156013  0.72562924  0.74179087  0.75889254]
Iteration 32 [101.4 s]: loss = 0.283199 [73.74 s], NDCG = 
[ 0.67719009  0.67284327  0.67435381  0.67913941  0.68695995  0.69803991
  0.71092019  0.72493203  0.74109011  0.75839274]
Iteration 33 [101.1 s]: loss = 0.282967 [73.28 s], NDCG = 
[ 0.67658104  0.67214494  0.6737528   0.67851325  0.68623545  0.69745346
  0.71034399  0.72435268  0.74064145  0.75793791]
Iteration 34 [101.5 s]: loss = 0.282732 [72.40 s], NDCG = 
[ 0.67681561  0.67180737  0.6735135   0.67797863  0.68566346  0.69696112
  0.7098178   0.72373922  0.74016987  0.75747009]
Iteration 35 [102.2 s]: loss = 0.281889 [73.51 s], NDCG = 
[ 0.67611223  0.67130096  0.67297242  0.67754705  0.68498553  0.69633949
  0.70906236  0.72322032  0.73968838  0.75703205]
Iteration 36 [101.0 s]: loss = 0.281490 [73.05 s], NDCG = 
[ 0.67604243  0.67045735  0.67217884  0.67678158  0.68478315  0.69595314
  0.70883127  0.72303347  0.73935699  0.75674871]
Iteration 37 [101.3 s]: loss = 0.280526 [73.06 s], NDCG = 
[ 0.67513687  0.6694347   0.67150159  0.67631752  0.68440181  0.6954158
  0.70815081  0.72239599  0.73885881  0.75610942]
Iteration 38 [101.4 s]: loss = 0.280339 [72.74 s], NDCG = 
[ 0.67521736  0.66860136  0.67045411  0.67557708  0.68379798  0.69472286
  0.70756552  0.72197608  0.73835797  0.75576788]
Iteration 39 [101.7 s]: loss = 0.279702 [74.89 s], NDCG = 
[ 0.67503625  0.66810773  0.67037546  0.6748602   0.68294619  0.69396327
  0.70712581  0.7215446   0.73800166  0.75532724]
Iteration 40 [102.8 s]: loss = 0.278458 [72.69 s], NDCG = 
[ 0.67444292  0.66831169  0.66980617  0.67457551  0.68259848  0.69347534
  0.70692793  0.72119748  0.73769641  0.75514917]
Iteration 41 [101.4 s]: loss = 0.277884 [73.13 s], NDCG = 
[ 0.67346474  0.66764683  0.66879503  0.67386008  0.68187982  0.69291564
  0.70593181  0.72068609  0.73729111  0.75454334]
Iteration 42 [101.9 s]: loss = 0.277791 [75.14 s], NDCG = 
[ 0.67310912  0.66742877  0.66845851  0.67337689  0.68148562  0.69267635
  0.70557374  0.72038492  0.73714783  0.75424454]
Iteration 43 [104.2 s]: loss = 0.276939 [73.03 s], NDCG = 
[ 0.67214696  0.66639477  0.66774588  0.67283984  0.68117409  0.69220068
  0.70512651  0.7200034   0.73654103  0.75391083]
Iteration 44 [101.9 s]: loss = 0.276158 [72.78 s], NDCG = 
[ 0.67259597  0.66577912  0.6672459   0.6723761   0.68095827  0.69195549
  0.70476823  0.7197497   0.73634284  0.75366428]
Iteration 45 [102.0 s]: loss = 0.275487 [75.45 s], NDCG = 
[ 0.67265162  0.66572616  0.6672305   0.67217324  0.68038419  0.69166812
  0.70453429  0.71953552  0.73612198  0.75342339]
Iteration 46 [102.8 s]: loss = 0.274840 [73.08 s], NDCG = 
[ 0.67241234  0.66538527  0.66671002  0.6719065   0.68001102  0.69123632
  0.70426544  0.71916613  0.73574985  0.7530191 ]
Iteration 47 [102.6 s]: loss = 0.273762 [73.61 s], NDCG = 
[ 0.67139265  0.66462725  0.66629181  0.67121219  0.67963146  0.69076912
  0.70364371  0.71861242  0.73524084  0.75264788]
Iteration 48 [101.7 s]: loss = 0.273227 [74.00 s], NDCG = 
[ 0.66975887  0.66462945  0.66597085  0.67102744  0.67941491  0.69050392
  0.70344551  0.71822791  0.7349768   0.75233064]
Iteration 49 [100.9 s]: loss = 0.272734 [65.34 s], NDCG = 
[ 0.66917246  0.66425265  0.66556548  0.67055328  0.67875626  0.68997221
  0.70319884  0.71798948  0.73483689  0.75204899]
Iteration 50 [99.1 s]: loss = 0.271570 [65.82 s], NDCG = 
[ 0.66927025  0.66418379  0.66571412  0.67051755  0.6788993   0.68995316
  0.70312994  0.71791859  0.73449696  0.75195425]
Iteration 51 [98.9 s]: loss = 0.270657 [65.79 s], NDCG = 
[ 0.66768113  0.66341305  0.66451566  0.66981646  0.67811376  0.68940042
  0.70281108  0.71758663  0.73400586  0.75152533]
Iteration 52 [98.9 s]: loss = 0.270049 [65.67 s], NDCG = 
[ 0.66907059  0.66378018  0.66481507  0.67014314  0.6786145   0.68953254
  0.70272842  0.71746367  0.73403406  0.7515583 ]
Iteration 53 [99.3 s]: loss = 0.268980 [65.81 s], NDCG = 
[ 0.66821597  0.66355079  0.6648299   0.66997717  0.67852164  0.68926537
  0.70261899  0.71726156  0.73369389  0.75139277]
Iteration 54 [99.9 s]: loss = 0.268349 [65.56 s], NDCG = 
[ 0.6680509   0.66263061  0.66481897  0.66976933  0.6782728   0.68939925
  0.70265837  0.71708782  0.733642    0.75106215]
Iteration 55 [99.8 s]: loss = 0.267178 [65.77 s], NDCG = 
[ 0.66665766  0.66219947  0.66404119  0.66935259  0.67766055  0.68889077
  0.70193897  0.71650167  0.73313305  0.75068473]
Iteration 56 [100.4 s]: loss = 0.266829 [65.49 s], NDCG = 
[ 0.6666781   0.66244677  0.66400996  0.66953543  0.67780389  0.68910302
  0.7020056   0.71657291  0.73327318  0.75087344]
Iteration 57 [100.7 s]: loss = 0.265937 [65.38 s], NDCG = 
[ 0.66596214  0.6619223   0.66384676  0.66906834  0.67742857  0.68884564
  0.70186836  0.71658451  0.73312762  0.75061212]
Iteration 58 [101.5 s]: loss = 0.264866 [65.36 s], NDCG = 
[ 0.66539994  0.66124549  0.6635963   0.66855671  0.67704327  0.68842023
  0.7015335   0.71624204  0.73265534  0.75022447]
Iteration 59 [101.3 s]: loss = 0.264469 [65.81 s], NDCG = 
[ 0.6649654   0.6603798   0.66314296  0.66817026  0.67664655  0.68797675
  0.7011008   0.71582465  0.73220249  0.74988377]
Iteration 60 [101.0 s]: loss = 0.263160 [65.75 s], NDCG = 
[ 0.66323793  0.65975116  0.66270015  0.66806544  0.6762881   0.68757401
  0.70090221  0.71544786  0.73198582  0.74957113]
Iteration 61 [101.9 s]: loss = 0.262424 [65.18 s], NDCG = 
[ 0.66320491  0.65979392  0.66231789  0.66803819  0.67607368  0.6875933
  0.70062567  0.71538449  0.73197151  0.74959495]
Iteration 62 [102.0 s]: loss = 0.261632 [65.42 s], NDCG = 
[ 0.66335112  0.65959274  0.66253517  0.66776645  0.6759563   0.68772804
  0.70054301  0.71536969  0.73189064  0.74940895]
Iteration 63 [102.3 s]: loss = 0.261221 [65.46 s], NDCG = 
[ 0.6620714   0.65885675  0.66204795  0.66713147  0.67539157  0.68712744
  0.70014171  0.71493251  0.73144557  0.74910151]
Iteration 64 [102.6 s]: loss = 0.259970 [65.46 s], NDCG = 
[ 0.66155196  0.65894514  0.6615518   0.66679071  0.67536632  0.68704805
  0.69971095  0.71460762  0.73118747  0.74889926]



liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3_random_train/MLP.py --learner adam --batch_size_random 1280000 --reg_layers [0.000001,0.001,0.002,0.004] --lr 0.00001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0.000001,0.001,0.002,0.004]', verbose=1) 
Load data done [32.9 s]. #user=13679, #item=12922, #train=3914489, #test=316795
Time: [71.5], Init: NDCG = 
[ 0.56192354  0.56679954  0.57549852  0.58647089  0.59865636  0.61252891
  0.62876945  0.64733796  0.66733852  0.68909811]
Iteration 0 [97.9 s]: loss = 0.840954 [74.09 s], NDCG = 
[ 0.63255828  0.63113818  0.63459014  0.64128844  0.65153178  0.66342155
  0.67680388  0.69279206  0.70991784  0.72930041]
Iteration 1 [93.3 s]: loss = 0.579005 [86.94 s], NDCG = 
[ 0.63703763  0.63481264  0.63801656  0.64615047  0.65649548  0.66943911
  0.68365353  0.69971447  0.71705067  0.73583372]
Iteration 2 [104.8 s]: loss = 0.473807 [241.68 s], NDCG = 
[ 0.64234614  0.64102533  0.64412704  0.65174195  0.66174945  0.6748849
  0.68964869  0.7054021   0.72278634  0.74118584]
Iteration 3 [120.7 s]: loss = 0.427822 [344.70 s], NDCG = 
[ 0.64914189  0.65014486  0.65224122  0.6594021   0.66980896  0.68271936
  0.69691367  0.71245043  0.72921089  0.74718924]
Iteration 4 [151.6 s]: loss = 0.398586 [394.22 s], NDCG = 
[ 0.66660807  0.66645787  0.66988713  0.67753125  0.68728712  0.69862795
  0.71161438  0.72596773  0.7416305   0.75845772]
Iteration 5 [137.0 s]: loss = 0.377673 [421.59 s], NDCG = 
[ 0.70482962  0.70101363  0.70249517  0.70698516  0.71390776  0.72299576
  0.73444081  0.74710641  0.76124518  0.77681058]
Iteration 6 [227.8 s]: loss = 0.361846 [397.45 s], NDCG = 
[ 0.73582761  0.72546221  0.72413957  0.72600016  0.7314528   0.73903638
  0.74902415  0.76102109  0.7742142   0.78934271]
Iteration 7 [330.2 s]: loss = 0.351689 [402.88 s], NDCG = 
[ 0.74864311  0.73694137  0.73405134  0.73457151  0.73797316  0.74561702
  0.75467635  0.76646945  0.77981889  0.79430501]
Iteration 8 [318.1 s]: loss = 0.345267 [416.00 s], NDCG = 
[ 0.7522908   0.73948789  0.73519963  0.73606445  0.739473    0.74669417
  0.75592355  0.76747904  0.78060235  0.79496452]
Iteration 9 [325.9 s]: loss = 0.340597 [452.22 s], NDCG = 
[ 0.75329886  0.74079827  0.73617399  0.73662588  0.73998881  0.74753682
  0.75670559  0.7683044   0.7812377   0.79555236]
Iteration 10 [374.4 s]: loss = 0.337527 [479.74 s], NDCG = 
[ 0.75335892  0.74093917  0.7364076   0.73684993  0.74043585  0.7480352
  0.75718644  0.76876294  0.78167995  0.79619613]
Iteration 11 [398.7 s]: loss = 0.335204 [448.91 s], NDCG = 
[ 0.75488861  0.7418943   0.73716086  0.73716892  0.74082613  0.74835752
  0.75770655  0.76925792  0.7822813   0.79677605]
Iteration 12 [396.6 s]: loss = 0.333314 [464.56 s], NDCG = 
[ 0.75479208  0.74195038  0.73707642  0.73735097  0.74090649  0.74838516
  0.75792154  0.76959446  0.78250117  0.79696321]
Iteration 13 [397.4 s]: loss = 0.331324 [428.90 s], NDCG = 
[ 0.75481158  0.74154392  0.73713919  0.73744939  0.74112173  0.74850381
  0.7581798   0.76970449  0.78265012  0.7971402 ]
Iteration 14 [389.0 s]: loss = 0.330617 [363.46 s], NDCG = 
[ 0.75468738  0.7418681   0.73755796  0.73760456  0.74108957  0.7485885
  0.75824451  0.76980724  0.7826613   0.79735994]
Iteration 15 [404.1 s]: loss = 0.329751 [427.14 s], NDCG = 
[ 0.75556778  0.74248332  0.73762564  0.73783628  0.74138759  0.7486238
  0.75872245  0.77002522  0.78297539  0.79750578]
Iteration 16 [411.8 s]: loss = 0.327961 [401.02 s], NDCG = 
[ 0.75496848  0.74196697  0.73731188  0.73751031  0.74136814  0.74872478
  0.75840256  0.77003707  0.78280153  0.79743448]
Iteration 17 [469.6 s]: loss = 0.326896 [400.41 s], NDCG = 
[ 0.75480246  0.74177201  0.7369741   0.7373348   0.74112481  0.74874306
  0.75852471  0.77007901  0.78295231  0.79736522]
Iteration 18 [526.7 s]: loss = 0.326228 [400.96 s], NDCG = 
[ 0.75409342  0.74108507  0.73641114  0.73727811  0.7409072   0.74843373
  0.7585452   0.7699372   0.78299491  0.79747101]
Iteration 19 [515.1 s]: loss = 0.325352 [399.44 s], NDCG = 
[ 0.75403085  0.74109849  0.73653939  0.73709095  0.74101176  0.74845937
  0.75857189  0.77001711  0.78314956  0.79749601]
Iteration 20 [528.1 s]: loss = 0.325095 [387.71 s], NDCG = 
[ 0.75336709  0.7409198   0.73614417  0.73665268  0.74101102  0.74834645
  0.75847182  0.76990279  0.78303236  0.79749292]
Iteration 21 [543.0 s]: loss = 0.323869 [390.53 s], NDCG = 
[ 0.75251405  0.74039566  0.73601271  0.73666246  0.74079763  0.74807024
  0.7584095   0.76973436  0.78291122  0.7973673 ]
Iteration 22 [536.4 s]: loss = 0.323522 [382.74 s], NDCG = 
[ 0.75316743  0.74033372  0.73631594  0.73656725  0.74088196  0.74836122
  0.75845229  0.76977608  0.78308165  0.79741159]
Iteration 23 [534.2 s]: loss = 0.323010 [398.28 s], NDCG = 
[ 0.75246122  0.74032774  0.73568435  0.73646951  0.7407205   0.74832249
  0.75842138  0.76982796  0.78310103  0.79728778]
Iteration 24 [537.2 s]: loss = 0.322621 [389.89 s], NDCG = 
[ 0.75244173  0.74020147  0.7356043   0.73606139  0.74075825  0.74830656
  0.75828911  0.76983599  0.7829032   0.79732386]
Iteration 25 [536.1 s]: loss = 0.321863 [398.11 s], NDCG = 
[ 0.75152328  0.74003169  0.73566182  0.73608523  0.7405352   0.74809571
  0.75830951  0.76971128  0.78291807  0.79731151]
Iteration 26 [534.4 s]: loss = 0.320991 [398.06 s], NDCG = 
[ 0.75172074  0.74001304  0.73567838  0.73631956  0.74053148  0.74819015
  0.75840061  0.76974718  0.78291023  0.79730136]
Iteration 27 [526.0 s]: loss = 0.321097 [360.11 s], NDCG = 
[ 0.75162578  0.73987142  0.73539483  0.73610756  0.7402604   0.74799226
  0.7581384   0.76970537  0.78281663  0.79723333]
Iteration 28 [520.3 s]: loss = 0.320685 [393.22 s], NDCG = 
[ 0.75103277  0.73938697  0.73523004  0.73589946  0.73998205  0.747736
  0.75819688  0.76938117  0.78260907  0.79711144]


