liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 5120000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=5120000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [228.0 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [65.0], Init: NDCG = 
[ 0.55743012  0.56341126  0.5722085   0.58286198  0.59551643  0.61030699
  0.62664981  0.64491293  0.66565507  0.68768817]
Iteration 0 [415.0 s]: loss = 0.664367 [65.59 s], NDCG = 
[ 0.74446731  0.73403173  0.73212857  0.73429852  0.7397727   0.74830071
  0.75851632  0.77055841  0.78381707  0.79827806]
Iteration 1 [422.2 s]: loss = 0.614839 [63.82 s], NDCG = 
[ 0.75962091  0.74511091  0.74208819  0.74207559  0.74610336  0.75429002
  0.76446321  0.77584846  0.78900832  0.80313041]
Iteration 2 [420.0 s]: loss = 0.605881 [72.60 s], NDCG = 
[ 0.76191781  0.74636142  0.74244627  0.74212433  0.746298    0.75428273
  0.76448336  0.77588491  0.78909398  0.80323672]
Iteration 3 [410.3 s]: loss = 0.598931 [63.75 s], NDCG = 
[ 0.76133392  0.74673165  0.74223352  0.74197694  0.74639395  0.75405958
  0.76362452  0.77513487  0.78824041  0.8025188 ]
Iteration 4 [404.7 s]: loss = 0.591955 [63.70 s], NDCG = 
[ 0.76203163  0.74744599  0.74218607  0.74229432  0.74710102  0.75446488
  0.7640392   0.77567343  0.78854668  0.80283624]
Iteration 5 [406.0 s]: loss = 0.584734 [63.56 s], NDCG = 
[ 0.76439273  0.74961907  0.74370141  0.74333882  0.74751462  0.7545797
  0.76433935  0.77561979  0.78865115  0.80319399]
Iteration 6 [405.7 s]: loss = 0.577304 [63.41 s], NDCG = 
[ 0.76269697  0.7476818   0.74114087  0.7408918   0.74513167  0.75207722
  0.76188806  0.77359365  0.78693107  0.80116052]
Iteration 7 [414.5 s]: loss = 0.569834 [72.62 s], NDCG = 
[ 0.76037931  0.74491739  0.73747693  0.73765619  0.74129332  0.7483762
  0.75819202  0.77009537  0.783501    0.79795954]
Iteration 8 [426.4 s]: loss = 0.562217 [72.15 s], NDCG = 
[ 0.75322793  0.73824804  0.73222746  0.7322518   0.73590442  0.74272891
  0.7530513   0.76555233  0.77908175  0.79387677]
Iteration 9 [421.4 s]: loss = 0.554752 [72.44 s], NDCG = 
[ 0.74605864  0.73098831  0.7267696   0.72682367  0.73076606  0.73851309
  0.74883009  0.76138247  0.77520014  0.79007409]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 256000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=256000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [149.5 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [70.1], Init: NDCG = 
[ 0.55919528  0.56443465  0.57201347  0.58146901  0.59399706  0.60935778
  0.62552283  0.64405773  0.66447554  0.68680781]
Iteration 0 [19.2 s]: loss = 0.693118 [72.50 s], NDCG = 
[ 0.57414072  0.57907444  0.58614357  0.59476895  0.60639286  0.6201398
  0.6362097   0.65392012  0.67363818  0.6955117 ]
Iteration 1 [21.2 s]: loss = 0.693016 [72.68 s], NDCG = 
[ 0.59124001  0.59342811  0.59864644  0.60753578  0.61767211  0.63180402
  0.64743014  0.6648961   0.68403166  0.70530165]
Iteration 2 [21.9 s]: loss = 0.692857 [73.05 s], NDCG = 
[ 0.6084453   0.60993004  0.61308989  0.62032691  0.63100145  0.64345042
  0.65869774  0.67526148  0.69378403  0.71444254]
Iteration 3 [21.7 s]: loss = 0.692522 [73.16 s], NDCG = 
[ 0.63213128  0.63410774  0.63759883  0.64316015  0.65224234  0.66440633
  0.67796525  0.6935429   0.71114332  0.73077653]
Iteration 4 [21.7 s]: loss = 0.691850 [73.06 s], NDCG = 
[ 0.65778807  0.65772494  0.66037801  0.66604265  0.67479012  0.6855859
  0.69873043  0.71344306  0.72966446  0.74765803]
Iteration 5 [21.8 s]: loss = 0.690600 [73.25 s], NDCG = 
[ 0.6767865   0.67369018  0.67623966  0.68186845  0.68986139  0.70004876
  0.71248759  0.72667576  0.74265226  0.76020855]
Iteration 6 [21.6 s]: loss = 0.688584 [73.63 s], NDCG = 
[ 0.68661587  0.68299223  0.6861254   0.69248508  0.70046891  0.71078977
  0.72279925  0.73681156  0.75197127  0.76845505]
Iteration 7 [21.6 s]: loss = 0.685704 [73.34 s], NDCG = 
[ 0.69335565  0.69009691  0.69180709  0.69822164  0.70670234  0.71687283
  0.72936867  0.7427539   0.75766312  0.77425029]
Iteration 8 [21.5 s]: loss = 0.681873 [73.23 s], NDCG = 
[ 0.69856197  0.69345228  0.69688884  0.70324693  0.71147368  0.72179499
  0.73419997  0.74746561  0.7623052   0.77825472]
Iteration 9 [21.3 s]: loss = 0.677178 [72.85 s], NDCG = 
[ 0.69606383  0.69508759  0.69997049  0.70535331  0.71395721  0.72412053
  0.73616343  0.74944278  0.76405204  0.77988527]
Iteration 10 [21.3 s]: loss = 0.672073 [73.04 s], NDCG = 
[ 0.69708694  0.69661044  0.70052601  0.70742492  0.71547222  0.7261059
  0.73829469  0.75185893  0.76585323  0.78146911]
Iteration 11 [21.3 s]: loss = 0.666654 [73.32 s], NDCG = 
[ 0.69824781  0.69806491  0.70177556  0.70854294  0.71725013  0.72778477
  0.73970175  0.75323235  0.76748628  0.78284545]
Iteration 12 [21.3 s]: loss = 0.660795 [73.48 s], NDCG = 
[ 0.70052333  0.70080991  0.7052254   0.7113983   0.71959966  0.73005225
  0.74224743  0.75526558  0.76932757  0.78471948]
Iteration 13 [21.4 s]: loss = 0.654560 [65.04 s], NDCG = 
[ 0.70328308  0.70305022  0.70688387  0.71342269  0.72206911  0.73204398
  0.74382761  0.75701531  0.77092321  0.78632361]
Iteration 14 [20.2 s]: loss = 0.649026 [63.91 s], NDCG = 
[ 0.71175631  0.70909593  0.71178645  0.71724325  0.72526216  0.73455947
  0.74654028  0.75950634  0.77328161  0.78834547]
Iteration 15 [20.1 s]: loss = 0.643958 [63.98 s], NDCG = 
[ 0.71443871  0.7127616   0.71509892  0.71996649  0.72733754  0.73698927
  0.74851391  0.76106466  0.77487184  0.78992838]
Iteration 16 [20.1 s]: loss = 0.639630 [64.15 s], NDCG = 
[ 0.72032985  0.71727958  0.71851806  0.72314486  0.73007554  0.73928076
  0.75020132  0.76272497  0.77663475  0.79131962]
Iteration 17 [20.3 s]: loss = 0.634761 [64.02 s], NDCG = 
[ 0.72561163  0.72139617  0.72153199  0.72559793  0.73280285  0.74134091
  0.75264195  0.76451302  0.77833666  0.79302617]
Iteration 18 [20.2 s]: loss = 0.631847 [64.45 s], NDCG = 
[ 0.72659611  0.72323066  0.72319849  0.72743043  0.73359546  0.74218038
  0.75323571  0.76554133  0.77920312  0.79390852]
Iteration 19 [20.0 s]: loss = 0.628869 [64.00 s], NDCG = 
[ 0.7317518   0.7273501   0.72709079  0.73020668  0.73592116  0.74416303
  0.75498251  0.76709278  0.78059165  0.79534142]
Iteration 20 [20.1 s]: loss = 0.625435 [64.26 s], NDCG = 
[ 0.73418202  0.72913493  0.72862416  0.73103882  0.73634662  0.74470957
  0.75591849  0.76780101  0.78148618  0.79598618]
Iteration 21 [19.9 s]: loss = 0.624223 [63.67 s], NDCG = 
[ 0.73718325  0.73179762  0.73008596  0.73202048  0.73751053  0.74603015
  0.75694996  0.76899779  0.7824352   0.79689605]
Iteration 22 [20.1 s]: loss = 0.621722 [63.55 s], NDCG = 
[ 0.7387337   0.73375539  0.73224636  0.73354032  0.73904908  0.7472061
  0.75826692  0.77003125  0.78321582  0.79739942]
Iteration 23 [20.0 s]: loss = 0.620257 [63.66 s], NDCG = 
[ 0.74246157  0.73485003  0.73263072  0.73452111  0.73976381  0.7481202
  0.75877093  0.77064783  0.78388751  0.79825053]
Iteration 24 [20.0 s]: loss = 0.618963 [64.08 s], NDCG = 
[ 0.74376928  0.73476577  0.7330234   0.73506706  0.74057366  0.74836016
  0.75898814  0.77119834  0.78416595  0.7984552 ]
Iteration 25 [20.0 s]: loss = 0.618895 [63.90 s], NDCG = 
[ 0.74587533  0.73649324  0.7342053   0.7359218   0.74101141  0.74884984
  0.75982679  0.77174115  0.78477985  0.79903486]
Iteration 26 [20.0 s]: loss = 0.617568 [63.68 s], NDCG = 
[ 0.74905263  0.73786214  0.73538522  0.73673385  0.74216805  0.74981116
  0.76030697  0.77226924  0.78564488  0.79982483]
Iteration 27 [20.0 s]: loss = 0.617494 [63.86 s], NDCG = 
[ 0.75209096  0.7392225   0.73620736  0.73749888  0.74291556  0.75041658
  0.7606606   0.77258386  0.78600519  0.8003673 ]
Iteration 28 [19.9 s]: loss = 0.616168 [63.66 s], NDCG = 
[ 0.75315373  0.73987017  0.73738021  0.73828239  0.74316296  0.75087954
  0.76129275  0.77309345  0.78629952  0.80070558]
Iteration 29 [20.0 s]: loss = 0.616600 [63.70 s], NDCG = 
[ 0.75245758  0.73952201  0.73719494  0.73774666  0.74290083  0.750669
  0.76131462  0.77285626  0.78630952  0.80060622]
Iteration 30 [19.9 s]: loss = 0.615481 [63.89 s], NDCG = 
[ 0.75353828  0.73981154  0.73717225  0.73804812  0.74301473  0.75096034
  0.76150265  0.77298073  0.78633086  0.80066104]
Iteration 31 [20.2 s]: loss = 0.614298 [63.96 s], NDCG = 
[ 0.75338138  0.74056778  0.73802796  0.73894888  0.74351266  0.75139868
  0.76135976  0.77337998  0.78656448  0.80083721]
Iteration 32 [19.9 s]: loss = 0.613442 [63.73 s], NDCG = 
[ 0.75507112  0.74101747  0.73837207  0.73934374  0.74354123  0.75113486
  0.76150944  0.77356911  0.78661345  0.8009196 ]
Iteration 33 [20.0 s]: loss = 0.613273 [64.03 s], NDCG = 
[ 0.75518431  0.74193208  0.73893897  0.73938228  0.74405303  0.75194546
  0.7620753   0.77410019  0.78708604  0.80136941]
Iteration 34 [20.0 s]: loss = 0.613443 [63.66 s], NDCG = 
[ 0.75453282  0.74218813  0.73877793  0.73953727  0.74415416  0.75211305
  0.76219415  0.7743155   0.78730992  0.80140746]
Iteration 35 [20.0 s]: loss = 0.612700 [63.95 s], NDCG = 
[ 0.75517456  0.74226663  0.73971834  0.74022815  0.7442648   0.75216501
  0.76259601  0.77456463  0.78738404  0.80165502]
Iteration 36 [20.2 s]: loss = 0.612699 [63.49 s], NDCG = 
[ 0.75480637  0.74242872  0.73927777  0.74008224  0.74445959  0.75250956
  0.76251095  0.77421539  0.78750999  0.80175998]
Iteration 37 [20.0 s]: loss = 0.610378 [63.71 s], NDCG = 
[ 0.75675017  0.74393681  0.74011051  0.74024703  0.74469425  0.75291381
  0.76284383  0.77470477  0.78771508  0.80204015]
Iteration 38 [20.0 s]: loss = 0.611149 [63.68 s], NDCG = 
[ 0.75868454  0.74447641  0.74099487  0.74077291  0.74507782  0.75302619
  0.76310231  0.77486294  0.78794448  0.80218474]
Iteration 39 [20.0 s]: loss = 0.611622 [63.89 s], NDCG = 
[ 0.75859713  0.74458915  0.7407583   0.7404651   0.74510256  0.75311578
  0.76352759  0.77496865  0.7880728   0.80218184]
Iteration 40 [20.0 s]: loss = 0.610557 [63.68 s], NDCG = 
[ 0.75792079  0.74491941  0.74066143  0.74095359  0.74512965  0.75328297
  0.76344138  0.77520304  0.78804601  0.80222633]
Iteration 41 [20.0 s]: loss = 0.610764 [63.79 s], NDCG = 
[ 0.75828961  0.74464321  0.74100949  0.74142005  0.74535786  0.75311333
  0.76344773  0.7754605   0.7881876   0.80225625]
Iteration 42 [20.0 s]: loss = 0.611054 [63.70 s], NDCG = 
[ 0.75900588  0.74547127  0.74094295  0.74132203  0.74532934  0.75346644
  0.76375269  0.77515925  0.78831516  0.80255036]
Iteration 43 [19.9 s]: loss = 0.609862 [63.74 s], NDCG = 
[ 0.75810284  0.74507328  0.74105276  0.74088947  0.74553131  0.75347469
  0.76348382  0.77529984  0.78834469  0.80237732]
Iteration 44 [20.0 s]: loss = 0.608533 [63.43 s], NDCG = 
[ 0.757435    0.74439658  0.74115606  0.74121078  0.7455404   0.75344895
  0.76356332  0.77521     0.78817064  0.80230906]
Iteration 45 [20.2 s]: loss = 0.608775 [63.72 s], NDCG = 
[ 0.75828144  0.74504041  0.74103086  0.74115864  0.74593524  0.75353174
  0.76358715  0.77519765  0.78838701  0.80248864]
Iteration 46 [19.9 s]: loss = 0.607777 [63.35 s], NDCG = 
[ 0.75782049  0.74505959  0.7411194   0.74109769  0.74578607  0.75382498
  0.76373402  0.77541998  0.78845193  0.80236148]
Iteration 47 [19.9 s]: loss = 0.608893 [63.87 s], NDCG = 
[ 0.75707372  0.74523988  0.7411102   0.74121852  0.74558451  0.75351671
  0.76372008  0.77547781  0.78846142  0.80237523]
Iteration 48 [20.0 s]: loss = 0.608395 [64.11 s], NDCG = 
[ 0.75770352  0.74528044  0.74109588  0.74104182  0.74559454  0.75373007
  0.76379065  0.77536609  0.78836337  0.80244307]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 15651858
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=15651858, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [140.4 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [68.3], Init: NDCG = 
[ 0.56358336  0.56662693  0.5739292   0.58397537  0.59582851  0.61031139
  0.62748303  0.64517293  0.66560437  0.68803401]
Iteration 0 [1292.1 s]: loss = 0.628549 [71.28 s], NDCG = 
[ 0.75971146  0.74483198  0.74036646  0.74120951  0.74545547  0.75353705
  0.76352037  0.77511611  0.78852322  0.80272307]
Iteration 1 [1267.3 s]: loss = 0.594737 [70.53 s], NDCG = 
[ 0.76264886  0.74762084  0.7404256   0.74073471  0.74505223  0.75279322
  0.76275525  0.77428904  0.78758338  0.80174053]
Iteration 2 [1265.2 s]: loss = 0.569755 [66.49 s], NDCG = 
[ 0.74938718  0.73301906  0.72671235  0.72774972  0.73247249  0.74032764
  0.7505089   0.76254786  0.77611648  0.79142302]



