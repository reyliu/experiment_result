liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [228.7 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [66.2], Init: NDCG = 
[ 0.5493594   0.5589967   0.567387    0.578249    0.59095474  0.60661471
  0.62362996  0.64233799  0.66313625  0.68624923]
Iteration 0 [104.0 s]: loss = 1.995198 [69.23 s], NDCG = 
[ 0.63525608  0.64032565  0.64752085  0.657353    0.66836114  0.68154813
  0.69615676  0.71182053  0.72899671  0.74716733]
Iteration 1 [104.8 s]: loss = 1.940192 [69.34 s], NDCG = 
[ 0.67001805  0.67306084  0.68015817  0.69023121  0.70116256  0.71334849
  0.72648371  0.74039904  0.75561643  0.77180102]
Iteration 2 [104.6 s]: loss = 1.760773 [70.34 s], NDCG = 
[ 0.69480796  0.69932025  0.7045763   0.71056655  0.71928901  0.7298378
  0.74197088  0.75515854  0.76929755  0.78449953]
Iteration 3 [106.5 s]: loss = 1.622040 [70.85 s], NDCG = 
[ 0.71860206  0.72015851  0.72162016  0.72588159  0.73340233  0.74183253
  0.75301376  0.76551373  0.77910308  0.79378082]
Iteration 4 [107.0 s]: loss = 1.571595 [71.80 s], NDCG = 
[ 0.73597207  0.73285938  0.73234158  0.73480924  0.74048656  0.74876846
  0.75947216  0.7714336   0.78448943  0.79908304]
Iteration 5 [102.7 s]: loss = 1.552692 [64.46 s], NDCG = 
[ 0.74680761  0.73940182  0.73760745  0.73922581  0.74454313  0.75213518
  0.7626825   0.77450183  0.78726938  0.80131598]
Iteration 6 [100.3 s]: loss = 1.537946 [63.66 s], NDCG = 
[ 0.75289904  0.74296308  0.73989829  0.74112936  0.74604417  0.75388952
  0.76383948  0.77567627  0.78851152  0.80262477]
Iteration 7 [99.6 s]: loss = 1.524536 [61.31 s], NDCG = 
[ 0.7593156   0.74633345  0.74317589  0.74313355  0.74730106  0.7553003
  0.76501189  0.77672328  0.78982034  0.80378108]
Iteration 8 [98.9 s]: loss = 1.518449 [61.81 s], NDCG = 
[ 0.7567574   0.74652446  0.74270106  0.74306678  0.74759582  0.75519601
  0.76493747  0.77655644  0.78958118  0.80352671]
Iteration 9 [98.6 s]: loss = 1.508633 [61.61 s], NDCG = 
[ 0.75830659  0.7470451   0.74318335  0.743586    0.7479142   0.75569637
  0.76529922  0.77700097  0.79005596  0.80371135]
Iteration 10 [99.6 s]: loss = 1.498393 [63.13 s], NDCG = 
[ 0.75839746  0.7469083   0.74278038  0.74344217  0.74746037  0.75499192
  0.76500014  0.77663119  0.78968432  0.80354   ]
Iteration 11 [102.5 s]: loss = 1.490866 [61.67 s], NDCG = 
[ 0.76041798  0.74647914  0.74221986  0.7429558   0.74738996  0.75469524
  0.76450741  0.77640203  0.78978611  0.8035483 ]
Iteration 12 [97.6 s]: loss = 1.480629 [61.43 s], NDCG = 
[ 0.75844588  0.74677474  0.74200095  0.74269599  0.74684583  0.75480206
  0.76478688  0.7764278   0.78937283  0.80342446]
Iteration 13 [97.9 s]: loss = 1.476230 [61.39 s], NDCG = 
[ 0.75837199  0.74631837  0.74207996  0.7426995   0.7467365   0.75476807
  0.76468107  0.77604358  0.78921854  0.80343024]
Iteration 14 [98.2 s]: loss = 1.460600 [61.38 s], NDCG = 
[ 0.75934735  0.74714187  0.7425589   0.74317634  0.74752899  0.75521001
  0.76486428  0.77650052  0.78949539  0.80351849]
Iteration 15 [98.3 s]: loss = 1.456882 [61.48 s], NDCG = 
[ 0.758638    0.74687553  0.74248491  0.74288836  0.74657233  0.75462799
  0.76455703  0.77624601  0.78925943  0.80337631]
Iteration 16 [99.0 s]: loss = 1.447820 [61.73 s], NDCG = 
[ 0.76047835  0.74666729  0.74241279  0.74231957  0.74698684  0.75436748
  0.7643414   0.77604254  0.78898863  0.80297783]
Iteration 17 [98.0 s]: loss = 1.432995 [61.58 s], NDCG = 
[ 0.76050288  0.74609775  0.74176875  0.74175014  0.74668944  0.75422852
  0.7646464   0.77602768  0.78908997  0.80311234]
Iteration 18 [98.1 s]: loss = 1.423794 [61.52 s], NDCG = 
[ 0.75953098  0.74606003  0.74133572  0.74164416  0.74628828  0.75370566
  0.76367943  0.77549154  0.78860244  0.80290462]
Iteration 19 [97.7 s]: loss = 1.409324 [61.55 s], NDCG = 
[ 0.76189486  0.74672989  0.74160703  0.74237526  0.74691115  0.75413687
  0.76450916  0.77605003  0.78903598  0.8030059 ]
Iteration 20 [97.9 s]: loss = 1.397496 [61.66 s], NDCG = 
[ 0.76038434  0.74519415  0.74058752  0.74106917  0.74586285  0.75325764
  0.7631657   0.77502809  0.78789973  0.80209455]
Iteration 21 [97.7 s]: loss = 1.379331 [61.48 s], NDCG = 
[ 0.75797078  0.74473398  0.73904952  0.7404445   0.74498645  0.75227752
  0.76192306  0.77405546  0.78768833  0.80156489]
Iteration 22 [98.1 s]: loss = 1.363213 [61.57 s], NDCG = 
[ 0.75837105  0.74308549  0.73836462  0.73897724  0.74386867  0.75166314
  0.761542    0.77331945  0.78663166  0.80082786]
Iteration 23 [97.8 s]: loss = 1.347511 [61.57 s], NDCG = 
[ 0.75821231  0.74254511  0.73714259  0.73738631  0.74256871  0.75042327
  0.76063788  0.77234039  0.78578707  0.79986207]
Iteration 24 [97.8 s]: loss = 1.330291 [61.61 s], NDCG = 
[ 0.75266133  0.73907762  0.73415499  0.73479931  0.74017287  0.74814024
  0.75842129  0.77060003  0.78406282  0.79847752]
Iteration 25 [97.8 s]: loss = 1.306618 [61.45 s], NDCG = 
[ 0.74900677  0.73667716  0.73201239  0.73247662  0.73776429  0.74632102
  0.75685063  0.76886307  0.78257064  0.79700942]
Iteration 26 [97.9 s]: loss = 1.288025 [61.54 s], NDCG = 
[ 0.74433247  0.73240104  0.72815172  0.7295424   0.73509213  0.74369907
  0.75423343  0.76668204  0.78048225  0.79509523]
Iteration 27 [97.9 s]: loss = 1.257414 [61.57 s], NDCG = 
[ 0.74275021  0.72940539  0.72665069  0.72791431  0.7336038   0.74210415
  0.75249466  0.76493624  0.77899526  0.79377412]
Iteration 28 [97.6 s]: loss = 1.237046 [61.61 s], NDCG = 
[ 0.74003579  0.7261066   0.72356368  0.72559837  0.73165146  0.74013796
  0.75083633  0.76301104  0.77728704  0.79212832]
Iteration 29 [97.5 s]: loss = 1.214525 [61.53 s], NDCG = 
[ 0.73816238  0.72500915  0.7225335   0.72442464  0.73007229  0.73851162
  0.74958397  0.76225567  0.77616827  0.79113682]
Iteration 30 [97.6 s]: loss = 1.188541 [61.58 s], NDCG = 
[ 0.73376168  0.72149492  0.7189834   0.72119896  0.72712589  0.73627593
  0.74736145  0.75989661  0.77441223  0.7892308 ]
Iteration 31 [98.1 s]: loss = 1.167811 [61.52 s], NDCG = 
[ 0.73132189  0.72014265  0.71756593  0.71983418  0.72591928  0.73436457
  0.74594265  0.75860502  0.77310634  0.78844987]
Iteration 32 [97.4 s]: loss = 1.142155 [61.68 s], NDCG = 
[ 0.72918094  0.71775829  0.71554837  0.71786393  0.72322862  0.73230297
  0.743526    0.75682859  0.77132029  0.78656412]
Iteration 33 [97.8 s]: loss = 1.118073 [61.58 s], NDCG = 
[ 0.72676204  0.71588762  0.71323394  0.71597624  0.72198343  0.73122346
  0.74274581  0.75582212  0.77033546  0.78544013]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [170.7 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [62.3], Init: NDCG = 
[ 0.55191845  0.55837367  0.56779111  0.57862885  0.5916157   0.60732464
  0.62415308  0.64261945  0.66316229  0.68566777]
Iteration 0 [116.6 s]: loss = 0.996572 [68.59 s], NDCG = 
[ 0.69235474  0.68824401  0.68819871  0.69255022  0.70019011  0.70986329
  0.72158999  0.73558602  0.75058045  0.76754682]
Iteration 1 [120.3 s]: loss = 0.942032 [63.63 s], NDCG = 
[ 0.72236106  0.71500549  0.71518392  0.7182032   0.72538778  0.73438847
  0.74576662  0.75855176  0.77220984  0.78773241]
Iteration 2 [128.5 s]: loss = 0.822159 [66.07 s], NDCG = 
[ 0.73451657  0.72540266  0.72452887  0.72698506  0.73332568  0.7419354
  0.75298646  0.76545952  0.77890855  0.79386698]
Iteration 3 [138.3 s]: loss = 0.776399 [66.12 s], NDCG = 
[ 0.74236598  0.73347651  0.7311211   0.7331276   0.73866582  0.74734555
  0.75777124  0.76972257  0.78324449  0.79782097]
Iteration 4 [143.5 s]: loss = 0.763694 [65.61 s], NDCG = 
[ 0.75052259  0.73900685  0.7359691   0.73829176  0.74299989  0.75122052
  0.76130353  0.77310394  0.78623231  0.8005672 ]
Iteration 5 [144.0 s]: loss = 0.756626 [62.75 s], NDCG = 
[ 0.75296822  0.74181374  0.73858633  0.73989725  0.74463731  0.75254422
  0.76299201  0.7745844   0.7873921   0.80172494]
Iteration 6 [146.6 s]: loss = 0.752691 [63.85 s], NDCG = 
[ 0.75650869  0.74350295  0.74071265  0.74157218  0.74558558  0.7535627
  0.76404155  0.77548412  0.78864065  0.80287203]
Iteration 7 [146.2 s]: loss = 0.749959 [64.41 s], NDCG = 
[ 0.75711554  0.74519966  0.74187762  0.74259011  0.7472513   0.75452814
  0.76428136  0.77607439  0.78910858  0.80338917]
Iteration 8 [147.1 s]: loss = 0.746189 [64.18 s], NDCG = 
[ 0.75919863  0.74647401  0.74308705  0.74354418  0.74834158  0.75551224
  0.76551801  0.77707534  0.78978123  0.80377035]
Iteration 9 [149.6 s]: loss = 0.743750 [64.29 s], NDCG = 
[ 0.75951652  0.74644164  0.74332112  0.74325438  0.74753329  0.75518749
  0.76537896  0.77684277  0.78963619  0.80369474]
Iteration 10 [151.4 s]: loss = 0.741745 [72.76 s], NDCG = 
[ 0.76126223  0.74765045  0.74395989  0.74384936  0.74794226  0.75584541
  0.76557933  0.77728582  0.78992624  0.80399281]
Iteration 11 [156.3 s]: loss = 0.739959 [72.16 s], NDCG = 
[ 0.76072172  0.74777862  0.74449988  0.74420129  0.74841107  0.75597789
  0.76576317  0.7775453   0.79001737  0.80396623]
Iteration 12 [155.2 s]: loss = 0.737480 [67.57 s], NDCG = 
[ 0.76212502  0.7484292   0.74542351  0.74456445  0.748845    0.75613097
  0.76609909  0.77755478  0.79037047  0.80430591]
Iteration 13 [156.7 s]: loss = 0.735922 [72.83 s], NDCG = 
[ 0.76338305  0.74949607  0.7451843   0.74462543  0.7486954   0.75635416
  0.76614764  0.7776963   0.79058659  0.80466577]
Iteration 14 [156.8 s]: loss = 0.734439 [71.29 s], NDCG = 
[ 0.76318433  0.74867953  0.74485069  0.74444814  0.74855638  0.75645796
  0.76594127  0.77738274  0.79036356  0.80450556]
Iteration 15 [155.9 s]: loss = 0.729885 [72.08 s], NDCG = 
[ 0.76531333  0.74920422  0.74516569  0.74516082  0.74894041  0.75634968
  0.76605183  0.77789501  0.79067706  0.80500509]
Iteration 16 [156.1 s]: loss = 0.727060 [71.23 s], NDCG = 
[ 0.76660815  0.74947092  0.74540539  0.74519094  0.74925783  0.75696404
  0.76667853  0.77800877  0.79101035  0.80510374]
Iteration 17 [154.7 s]: loss = 0.725856 [71.05 s], NDCG = 
[ 0.76436816  0.74936616  0.74491535  0.7443782   0.74882429  0.75664697
  0.76655264  0.77789441  0.79073107  0.80484431]
Iteration 18 [156.6 s]: loss = 0.723886 [70.33 s], NDCG = 
[ 0.76426125  0.74939863  0.74473675  0.74447357  0.74849564  0.75664759
  0.76632667  0.77808612  0.79070998  0.80486506]
Iteration 19 [154.0 s]: loss = 0.719233 [71.31 s], NDCG = 
[ 0.76404115  0.74943151  0.74477809  0.7445191   0.74860307  0.7563787
  0.76612098  0.77791086  0.79065144  0.80481068]
Iteration 20 [156.3 s]: loss = 0.716874 [71.40 s], NDCG = 
[ 0.76439551  0.74896486  0.74443743  0.74474996  0.74930355  0.75650002
  0.76632367  0.77799703  0.79094454  0.80496411]
Iteration 21 [156.2 s]: loss = 0.713094 [71.31 s], NDCG = 
[ 0.76537936  0.74907959  0.74492663  0.74507825  0.74870366  0.75665466
  0.76652878  0.77787068  0.79086018  0.80493789]
Iteration 22 [156.3 s]: loss = 0.709027 [66.11 s], NDCG = 
[ 0.76492501  0.74960802  0.7443882   0.745417    0.74927018  0.75661848
  0.76660308  0.77798712  0.79080228  0.80492567]
Iteration 23 [150.7 s]: loss = 0.707418 [71.82 s], NDCG = 
[ 0.76388394  0.74857061  0.74410351  0.74466986  0.74848546  0.75605837
  0.76611527  0.77764606  0.7903763   0.80432798]



