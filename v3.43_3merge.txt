liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.43_3merge/MLP.py --learner adam --reg_layers [0,0,0] --lr 0.0001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[96,32,10]', learner='adam', lr=0.0001, path='Data/yelp/', reg_layers='[0,0,0]', verbose=1) 
Load data done [132.4 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [62.5], Init: NDCG = 
[ 0.55925129  0.56384939  0.57182489  0.58147658  0.59441681  0.60879105
  0.62531534  0.64384617  0.66439887  0.68723793]
Iteration 0 [94.4 s]: loss = 0.636982 [60.56 s], NDCG = 
[ 0.75075652  0.7390078   0.73540477  0.73551383  0.7401314   0.74798198
  0.75844841  0.77013023  0.78347074  0.79810376]
Iteration 1 [103.6 s]: loss = 0.607418 [61.91 s], NDCG = 
[ 0.75558669  0.74092497  0.73717323  0.73723938  0.74247147  0.74995848
  0.75960096  0.77157471  0.7850172   0.79935911]
Iteration 2 [100.6 s]: loss = 0.595521 [61.19 s], NDCG = 
[ 0.75409953  0.7408055   0.73721827  0.73728146  0.7421722   0.75005592
  0.76017937  0.7721297   0.78505468  0.79937585]
Iteration 3 [98.8 s]: loss = 0.583772 [64.58 s], NDCG = 
[ 0.74759655  0.73632601  0.73257618  0.73374837  0.73840656  0.74554924
  0.75628769  0.76856489  0.7817258   0.79663507]
Iteration 4 [98.6 s]: loss = 0.574500 [61.34 s], NDCG = 
[ 0.74149003  0.72913157  0.72587111  0.72681138  0.73214293  0.74015973
  0.75095898  0.76330257  0.77709748  0.79194668]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.43_3merge/MLP.py --learner adam --reg_layers [0,0,0] --lr 0.00001 --batch_size_random 1280000
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[96,32,10]', learner='adam', lr=1e-05, path='Data/yelp/', reg_layers='[0,0,0]', verbose=1) 
Load data done [132.4 s]. #user=13679, #item=12922, #train=15651858, #test=316795
Time: [62.8], Init: NDCG = 
[ 0.56004415  0.56553117  0.57388105  0.58384277  0.59634564  0.61081842
  0.62745058  0.64536379  0.66535623  0.68762263]
Iteration 0 [96.9 s]: loss = 0.692124 [60.58 s], NDCG = 
[ 0.69032613  0.6853547   0.68750569  0.69168122  0.69935109  0.70923443
  0.72149319  0.73549264  0.75067373  0.76716395]
Iteration 1 [99.9 s]: loss = 0.679644 [65.75 s], NDCG = 
[ 0.72892477  0.72160085  0.7206249   0.72275844  0.72860783  0.73724176
  0.74832717  0.76109524  0.77523098  0.78996507]
Iteration 2 [101.8 s]: loss = 0.649030 [68.63 s], NDCG = 
[ 0.74377808  0.7330387   0.73108636  0.73237504  0.73793821  0.74588449
  0.75664922  0.76888037  0.78235474  0.7966827 ]
Iteration 3 [102.3 s]: loss = 0.626609 [69.57 s], NDCG = 
[ 0.74969564  0.73997031  0.73680644  0.73718363  0.74223437  0.75035836
  0.76062998  0.7724953   0.78560348  0.79969011]
Iteration 4 [101.4 s]: loss = 0.618358 [68.29 s], NDCG = 
[ 0.75606283  0.74516143  0.74153875  0.74053096  0.74570257  0.75326685
  0.76326696  0.77464435  0.78761569  0.80189723]
Iteration 5 [101.1 s]: loss = 0.615587 [67.30 s], NDCG = 
[ 0.75881345  0.74630263  0.7420431   0.74252802  0.74712025  0.7544471
  0.76430458  0.77574155  0.78867618  0.80293761]
Iteration 6 [99.9 s]: loss = 0.614022 [70.58 s], NDCG = 
[ 0.76007494  0.74656795  0.74282807  0.74278563  0.74669523  0.75474712
  0.76435937  0.77595721  0.78909355  0.80341812]
Iteration 7 [101.5 s]: loss = 0.612297 [67.96 s], NDCG = 
[ 0.76082926  0.74653907  0.74321993  0.74295307  0.74665945  0.7547963
  0.76472578  0.77639956  0.78914264  0.80326008]
Iteration 8 [102.1 s]: loss = 0.610586 [68.11 s], NDCG = 
[ 0.76007085  0.74569469  0.74255894  0.74268027  0.74661601  0.75463016
  0.76429752  0.77607572  0.78867627  0.80313176]

