liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.0001 --path Data_item/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data_item/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.4 s]. #user=13679, #item=12922, #train=323272, #test=316871
Init: NDCG = 
[ 0.56872151  0.57084691  0.5756905   0.58361577  0.59408922  0.60700414
  0.62144867  0.63736664  0.65508457  0.67444297]
Iteration 0 [15.0 s]: loss = 6.1997 [19.5 s], NDCG = 
[ 0.71543171  0.70486908  0.69978652  0.70027994  0.70550355  0.71301343
  0.72265148  0.73451764  0.74735993  0.76159891]
Iteration 1 [13.3 s]: loss = 1.0847 [19.4 s], NDCG = 
[ 0.75956298  0.74513543  0.74074817  0.74051307  0.74347493  0.75024314
  0.75907256  0.76909425  0.78077136  0.79331957]
Iteration 2 [13.3 s]: loss = 0.9988 [19.4 s], NDCG = 
[ 0.77168315  0.75835781  0.75056202  0.74811474  0.75181215  0.75830028
  0.7663409   0.77594272  0.78685758  0.79895827]
Iteration 3 [13.2 s]: loss = 0.9796 [19.4 s], NDCG = 
[ 0.77389812  0.76103229  0.75288795  0.7512203   0.75376106  0.75972079
  0.76817443  0.77722375  0.78815851  0.80007149]
Iteration 4 [13.2 s]: loss = 0.9715 [19.6 s], NDCG = 
[ 0.77230028  0.75958433  0.75244932  0.75081927  0.75365977  0.75959998
  0.76758771  0.7768741   0.78789299  0.79970896]
Iteration 5 [13.2 s]: loss = 0.9666 [19.4 s], NDCG = 
[ 0.77405379  0.75988057  0.75271844  0.75107024  0.75405998  0.75980087
  0.76779371  0.77712091  0.78804423  0.80010349]
Iteration 6 [13.2 s]: loss = 0.9629 [19.4 s], NDCG = 
[ 0.77514561  0.76086926  0.75270097  0.7508794   0.75387503  0.75980531
  0.7681396   0.77716449  0.78795138  0.79992612]
Iteration 7 [13.2 s]: loss = 0.9597 [19.4 s], NDCG = 
[ 0.77438499  0.76007786  0.7528181   0.75084672  0.75378331  0.76009123
  0.76803542  0.7772822   0.78787242  0.80005013]
Iteration 8 [13.3 s]: loss = 0.9567 [19.8 s], NDCG = 
[ 0.7744637   0.76068238  0.75266789  0.75132915  0.75419949  0.76020276
  0.76819363  0.77750017  0.78815465  0.80016728]
Iteration 9 [13.2 s]: loss = 0.9539 [19.5 s], NDCG = 
[ 0.77664837  0.76114046  0.7542054   0.75195156  0.75507864  0.76096706
  0.7687858   0.77806638  0.7885602   0.80061545]
Iteration 10 [13.2 s]: loss = 0.9508 [19.4 s], NDCG = 
[ 0.77619388  0.76148778  0.75293236  0.75141827  0.75476818  0.76052914
  0.76853749  0.77762136  0.78815777  0.80047597]
Iteration 11 [13.1 s]: loss = 0.9469 [19.2 s], NDCG = 
[ 0.77675598  0.7623976   0.75450628  0.75224254  0.75461428  0.76060047
  0.76868892  0.77788269  0.78859186  0.80071461]
Iteration 12 [13.0 s]: loss = 0.9420 [19.6 s], NDCG = 
[ 0.77435783  0.76103134  0.75322947  0.75172068  0.75427103  0.76015946
  0.7680991   0.7772978   0.78818224  0.80059592]
Iteration 13 [13.1 s]: loss = 0.9346 [19.0 s], NDCG = 
[ 0.77658638  0.7612502   0.75401294  0.75181904  0.75457323  0.76026233
  0.76885798  0.77754411  0.78829354  0.80051926]
Iteration 14 [13.1 s]: loss = 0.9237 [18.9 s], NDCG = 
[ 0.77669957  0.76160559  0.75436253  0.75183873  0.75504509  0.76022375
  0.7685106   0.77760149  0.78832035  0.80059516]
Iteration 15 [13.0 s]: loss = 0.9096 [18.9 s], NDCG = 
[ 0.77707708  0.76231697  0.75417807  0.75241687  0.75468706  0.7601868
  0.76799378  0.77739452  0.78843682  0.80049008]
Iteration 16 [13.0 s]: loss = 0.8940 [18.8 s], NDCG = 
[ 0.77708231  0.76144934  0.75348572  0.75136634  0.75390721  0.75965026
  0.76727686  0.77673988  0.78758165  0.79949068]
Iteration 17 [13.1 s]: loss = 0.8786 [19.0 s], NDCG = 
[ 0.77414504  0.75969085  0.75186638  0.75044138  0.7524864   0.75809378
  0.76608634  0.77533314  0.7863964   0.79846016]
Iteration 18 [13.8 s]: loss = 0.8633 [22.3 s], NDCG = 
[ 0.77507212  0.75877428  0.74982574  0.74865582  0.75130697  0.75691871
  0.76457379  0.77401692  0.78499558  0.79721058]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v3.03_eval_paircomp_rand/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [148.4 s]. #user=13679, #item=12922, #train=15898078, #test=316871
Time: [71.8], Init: NDCG = 
[ 0.55207896  0.55953165  0.56664762  0.57651665  0.58822162  0.60113444
  0.61628934  0.63343856  0.65127944  0.6708714 ]
Iteration 0 [106.0 s]: loss = 0.692371 [72.12 s], NDCG = 
[ 0.6843168   0.68170184  0.68188521  0.68560942  0.6918902   0.70041514
  0.7110258   0.72314939  0.73589573  0.7506098 ]
Iteration 1 [109.0 s]: loss = 0.681564 [72.23 s], NDCG = 
[ 0.72343519  0.71516469  0.71398079  0.71703888  0.72268535  0.73038102
  0.73968511  0.75007183  0.76169177  0.77536598]
Iteration 2 [107.3 s]: loss = 0.649874 [72.91 s], NDCG = 
[ 0.73691793  0.72647635  0.72548745  0.72682642  0.73166519  0.73894751
  0.74807733  0.75810052  0.76967561  0.7827    ]
Iteration 3 [107.2 s]: loss = 0.626707 [72.86 s], NDCG = 
[ 0.74738469  0.73412443  0.73119331  0.73314447  0.73715679  0.7437766
  0.75243137  0.76269431  0.77393902  0.78666672]
Iteration 4 [107.1 s]: loss = 0.618677 [71.53 s], NDCG = 
[ 0.75274171  0.74010556  0.73557838  0.73573572  0.7398328   0.74645848
  0.7549504   0.76511499  0.77606955  0.78901702]
Iteration 5 [106.3 s]: loss = 0.615419 [69.74 s], NDCG = 
[ 0.7547397   0.74120572  0.73696511  0.73685504  0.74108199  0.74744839
  0.75572321  0.76569332  0.77704725  0.78956614]
Iteration 6 [104.0 s]: loss = 0.612588 [68.52 s], NDCG = 
[ 0.75771946  0.74334647  0.73779221  0.73751333  0.74182455  0.74815228
  0.75638458  0.76636356  0.77789986  0.79019941]
Iteration 7 [104.2 s]: loss = 0.610312 [69.02 s], NDCG = 
[ 0.75751538  0.74353029  0.73799019  0.73723952  0.74123575  0.74779501
  0.75612064  0.76646285  0.7775247   0.79034973]
Iteration 8 [104.8 s]: loss = 0.609220 [71.48 s], NDCG = 
[ 0.757802    0.74320562  0.73726598  0.73717166  0.74093201  0.74763414
  0.75579636  0.76622213  0.77743392  0.79015664]
Iteration 9 [106.9 s]: loss = 0.606879 [72.95 s], NDCG = 
[ 0.75823176  0.74314617  0.73662978  0.73658059  0.7405281   0.74721259
  0.75582701  0.7660213   0.77712113  0.7898117 ]
Iteration 10 [109.4 s]: loss = 0.605477 [73.35 s], NDCG = 
[ 0.75927516  0.74433776  0.73740229  0.73682164  0.74062948  0.74748958
  0.75592381  0.76595956  0.77727624  0.79005512]
Iteration 11 [106.1 s]: loss = 0.603605 [72.76 s], NDCG = 
[ 0.76012179  0.74376027  0.73743668  0.73634538  0.74043314  0.74715543
  0.75593111  0.76612427  0.77715409  0.78979122]
Iteration 12 [105.8 s]: loss = 0.602260 [72.18 s], NDCG = 
[ 0.76041051  0.74426042  0.7376325   0.73655892  0.74105679  0.74741283
  0.75590752  0.76591759  0.77712895  0.78982157]
Iteration 13 [103.7 s]: loss = 0.600576 [71.69 s], NDCG = 
[ 0.76054842  0.74428523  0.73729419  0.73663822  0.74038266  0.74679568
  0.75553495  0.76596725  0.777134    0.78974724]
Iteration 14 [105.7 s]: loss = 0.599119 [72.90 s], NDCG = 
[ 0.76041434  0.74457497  0.73750227  0.73692096  0.74041571  0.74692418
  0.75550562  0.76549342  0.77690002  0.78977154]
Iteration 15 [105.9 s]: loss = 0.597434 [73.20 s], NDCG = 
[ 0.76075389  0.74443949  0.7374885   0.73673949  0.74035136  0.74677373
  0.75562628  0.76554547  0.77703721  0.78948587]
Iteration 16 [109.1 s]: loss = 0.596139 [74.32 s], NDCG = 
[ 0.75984527  0.7438008   0.73749152  0.7369132   0.74014923  0.74644576
  0.75522257  0.76532699  0.7767531   0.78938304]
Iteration 17 [109.4 s]: loss = 0.594556 [72.99 s], NDCG = 
[ 0.75951651  0.74479613  0.73864788  0.7370368   0.7408581   0.74687033
  0.7555276   0.76581969  0.77699737  0.78984436]
Iteration 18 [108.0 s]: loss = 0.592863 [73.43 s], NDCG = 
[ 0.76065011  0.74495558  0.73905128  0.73821952  0.74111654  0.74699428
  0.75578641  0.76605554  0.7774415   0.79015612]
Iteration 19 [109.4 s]: loss = 0.591873 [72.23 s], NDCG = 
[ 0.76178476  0.74590905  0.73840358  0.73746967  0.74104869  0.74701782
  0.75596908  0.76611615  0.7776429   0.79012009]
Iteration 20 [103.5 s]: loss = 0.590564 [71.16 s], NDCG = 
[ 0.76093708  0.74615102  0.73913838  0.73810657  0.74163703  0.74795954
  0.75613134  0.7662551   0.77767366  0.7901668 ]
Iteration 21 [105.6 s]: loss = 0.588766 [72.98 s], NDCG = 
[ 0.76068076  0.74623367  0.73959077  0.73856644  0.74205841  0.74799236
  0.75587416  0.76628298  0.77776137  0.79018012]
Iteration 22 [106.1 s]: loss = 0.587508 [63.40 s], NDCG = 
[ 0.76116659  0.74679613  0.73994579  0.73906604  0.74192862  0.74801904
  0.7559756   0.76630569  0.77786127  0.79027297]
Iteration 23 [100.7 s]: loss = 0.586297 [64.48 s], NDCG = 
[ 0.76195019  0.74705153  0.74025061  0.7395908   0.74237537  0.74883184
  0.75662768  0.76659114  0.77798347  0.79032294]
Iteration 24 [105.3 s]: loss = 0.584286 [72.83 s], NDCG = 
[ 0.76219258  0.74745547  0.74062043  0.73973144  0.74278036  0.74896264
  0.75670999  0.76680313  0.77801965  0.79035138]
Iteration 25 [106.2 s]: loss = 0.582751 [70.98 s], NDCG = 
[ 0.76277488  0.74771379  0.7404896   0.73866603  0.74208076  0.74826884
  0.75661823  0.76641394  0.77788055  0.79029823]
Iteration 26 [103.3 s]: loss = 0.581434 [69.13 s], NDCG = 
[ 0.76171267  0.74704751  0.73975543  0.73875495  0.74138339  0.74782421
  0.75589688  0.76549448  0.77710112  0.78952698]
Iteration 27 [103.3 s]: loss = 0.580039 [69.54 s], NDCG = 
[ 0.76189551  0.74712173  0.73943879  0.73848258  0.7413598   0.74731577
  0.75566304  0.76530182  0.77698334  0.789302  ]
Iteration 28 [106.0 s]: loss = 0.578244 [70.99 s], NDCG = 
[ 0.76136057  0.74602663  0.73868281  0.73754298  0.74046204  0.74667175
  0.7549741   0.765049    0.77638252  0.7887466 ]
Iteration 29 [110.4 s]: loss = 0.576908 [71.99 s], NDCG = 
[ 0.76192735  0.7462058   0.73903537  0.73751931  0.74037223  0.74642084
  0.75429451  0.76461064  0.77596887  0.78831889]

liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6_mseloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [170.4 s]. #user=13679, #item=12922, #train=15898078, #test=316871
Time: [68.7], Init: NDCG = 
[ 0.54265803  0.55328806  0.56226335  0.5726896   0.58468524  0.59784912
  0.61299762  0.63016762  0.64835111  0.66841673]
Iteration 0 [102.6 s]: loss = 1.996641 [70.96 s], NDCG = 
[ 0.54265803  0.55328806  0.56226335  0.5726896   0.58468524  0.59784912
  0.61299762  0.63016762  0.64835111  0.66841673]
Iteration 1 [107.5 s]: loss = 1.919572 [71.33 s], NDCG = 
[ 0.57314308  0.57576916  0.58126999  0.58974936  0.59952612  0.61126858
  0.62538102  0.64163773  0.65924799  0.67857514]
Iteration 2 [107.3 s]: loss = 1.721498 [62.54 s], NDCG = 
[ 0.72732879  0.71892792  0.71841354  0.72071261  0.72539259  0.73200415
  0.74076362  0.75097035  0.76235821  0.77571599]
Iteration 3 [101.0 s]: loss = 1.604295 [62.93 s], NDCG = 
[ 0.74217967  0.73046989  0.72654234  0.72782778  0.73299765  0.74008814
  0.74859714  0.7590067   0.77044816  0.78318597]
Iteration 4 [100.9 s]: loss = 1.569000 [63.59 s], NDCG = 
[ 0.7480241   0.73543957  0.73147617  0.73145007  0.73614135  0.74255256
  0.75140879  0.76107723  0.77283673  0.78563128]
Iteration 5 [104.0 s]: loss = 1.548258 [67.31 s], NDCG = 
[ 0.74941383  0.73850024  0.7334991   0.73340703  0.73731899  0.74362309
  0.75234276  0.76225391  0.77368969  0.78652986]
Iteration 6 [107.9 s]: loss = 1.535124 [62.63 s], NDCG = 
[ 0.75326968  0.7398539   0.73402366  0.73401939  0.73825167  0.74446515
  0.75307027  0.76296206  0.77435675  0.78719525]
Iteration 7 [101.2 s]: loss = 1.524855 [63.00 s], NDCG = 
[ 0.75531225  0.74111512  0.73538494  0.73497432  0.73831775  0.74510623
  0.75391323  0.76348672  0.77456466  0.7874835 ]
Iteration 8 [101.3 s]: loss = 1.516381 [63.18 s], NDCG = 
[ 0.74961811  0.73847519  0.73356725  0.73326167  0.73723274  0.74402481
  0.7525507   0.76239976  0.77383769  0.7866089 ]
Iteration 9 [101.2 s]: loss = 1.509664 [63.06 s], NDCG = 
[ 0.75411387  0.74013476  0.73432831  0.73468317  0.73840065  0.74503821
  0.75403511  0.76419637  0.77493238  0.78750721]
Iteration 10 [100.8 s]: loss = 1.501280 [63.42 s], NDCG = 
[ 0.7534163   0.73971153  0.73384107  0.73343388  0.73745129  0.74475342
  0.75353737  0.76339774  0.77441375  0.78695782]
Iteration 11 [101.3 s]: loss = 1.495167 [63.52 s], NDCG = 
[ 0.75566644  0.74040227  0.73551998  0.73478278  0.7381104   0.74468544
  0.75333257  0.76334405  0.77441461  0.78744581]
Iteration 12 [101.3 s]: loss = 1.482618 [63.55 s], NDCG = 
[ 0.75259125  0.73862147  0.73309655  0.73250054  0.73638088  0.74369958
  0.75236509  0.76224169  0.77336979  0.78622185]
Iteration 13 [100.7 s]: loss = 1.474210 [66.75 s], NDCG = 
[ 0.75348177  0.73824984  0.7336384   0.73301475  0.73689055  0.74322602
  0.75199469  0.76184657  0.77328921  0.78610378]
Iteration 14 [107.7 s]: loss = 1.465152 [72.48 s], NDCG = 
[ 0.75354376  0.73778748  0.73292174  0.73199142  0.73589657  0.74272763
  0.75162327  0.76147539  0.77309049  0.78580466]
Iteration 15 [107.5 s]: loss = 1.457094 [72.66 s], NDCG = 
[ 0.75211518  0.73758165  0.7324791   0.73193629  0.73556142  0.74228087
  0.75103638  0.76122349  0.77256591  0.78549261]
Iteration 16 [106.6 s]: loss = 1.442915 [69.24 s], NDCG = 
[ 0.74984901  0.73606633  0.73194134  0.73134082  0.73484953  0.7412498
  0.75007251  0.76001131  0.77161888  0.78431967]
Iteration 17 [103.8 s]: loss = 1.427412 [68.42 s], NDCG = 
[ 0.7523318   0.7380773   0.73230931  0.73120675  0.7345258   0.74110101
  0.75023634  0.76049174  0.7718325   0.78462112]


liurui@ubuntu:~/DeepRec$ KERAS_BACKEND=theano python v6.1_hingeloss_3Embed_2MLP/MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.00001 --batch_size_random 1280000 --path Data/yelpi/
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=1280000, epochs=100, layers='[64,32,16,8]', learner='adam', lr=1e-05, path='Data/yelpi/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [158.3 s]. #user=13679, #item=12922, #train=15898078, #test=316871
Time: [65.3], Init: NDCG = 
[ 0.54265803  0.55328806  0.56226335  0.5726896   0.58468524  0.59784912
  0.61299762  0.63016762  0.64835111  0.66841673]
Iteration 0 [112.8 s]: loss = 0.997420 [64.20 s], NDCG = 
[ 0.54265803  0.55328806  0.56226335  0.5726896   0.58468524  0.59784912
  0.61299762  0.63016762  0.64835111  0.66841673]
Iteration 1 [120.0 s]: loss = 0.955913 [64.09 s], NDCG = 
[ 0.58079621  0.58434262  0.58829086  0.59596101  0.60543981  0.61670459
  0.63062568  0.64640822  0.66355503  0.68270716]
Iteration 2 [123.3 s]: loss = 0.847467 [63.76 s], NDCG = 
[ 0.72039205  0.71056785  0.71041433  0.71372391  0.71957186  0.72753538
  0.73659876  0.74725351  0.75883729  0.77242331]
Iteration 3 [131.2 s]: loss = 0.788820 [64.18 s], NDCG = 
[ 0.73418091  0.7250507   0.72217886  0.7246524   0.72925128  0.73665274
  0.74547068  0.75551559  0.76660461  0.77985198]
Iteration 4 [136.2 s]: loss = 0.768836 [62.87 s], NDCG = 
[ 0.74105164  0.73146174  0.72814027  0.72944068  0.73395385  0.74080397
  0.74939999  0.75949054  0.77064084  0.7833436 ]
Iteration 5 [137.0 s]: loss = 0.759868 [62.78 s], NDCG = 
[ 0.74801923  0.73724626  0.73343061  0.7338119   0.737557    0.7441446
  0.75258364  0.76244966  0.77360317  0.78594236]
Iteration 6 [139.7 s]: loss = 0.755682 [63.16 s], NDCG = 
[ 0.74783918  0.73730183  0.7331437   0.73407215  0.73782462  0.74447105
  0.75303745  0.76308261  0.77409265  0.78643756]
Iteration 7 [141.2 s]: loss = 0.751230 [63.13 s], NDCG = 
[ 0.75004056  0.7379838   0.73400131  0.73483262  0.73892102  0.745228
  0.7536173   0.76330211  0.77469252  0.78728418]
Iteration 8 [142.6 s]: loss = 0.749732 [63.93 s], NDCG = 
[ 0.75201453  0.7389736   0.73472441  0.7353602   0.73927106  0.7461082
  0.75440629  0.76368378  0.77504007  0.78747257]
Iteration 9 [142.3 s]: loss = 0.747874 [63.49 s], NDCG = 
[ 0.75371963  0.74009303  0.73567858  0.73562603  0.73967178  0.74617094
  0.75461689  0.76412274  0.77489503  0.78761301]
Iteration 10 [143.9 s]: loss = 0.745718 [63.33 s], NDCG = 
[ 0.75364824  0.74172483  0.73680479  0.73691706  0.74021505  0.74663927
  0.7549281   0.76447579  0.77555789  0.78787055]
Iteration 11 [144.0 s]: loss = 0.744956 [63.64 s], NDCG = 
[ 0.75328082  0.74052369  0.73633191  0.73650978  0.73965194  0.74627921
  0.75464494  0.76417045  0.77493199  0.78742576]
Iteration 12 [145.0 s]: loss = 0.742066 [63.22 s], NDCG = 
[ 0.75634973  0.74253797  0.73725861  0.73675913  0.74021557  0.74711128
  0.75506249  0.76474974  0.77571668  0.78830665]
Iteration 13 [145.4 s]: loss = 0.738624 [63.29 s], NDCG = 
[ 0.7558956   0.74442402  0.73709743  0.73680009  0.73988917  0.74695834
  0.75540669  0.76468252  0.77540412  0.78796602]
Iteration 14 [145.0 s]: loss = 0.739509 [63.29 s], NDCG = 
[ 0.75560445  0.74362164  0.73798775  0.73645649  0.74061366  0.7471112
  0.75517183  0.76504544  0.7758221   0.78843675]
Iteration 15 [145.6 s]: loss = 0.736191 [63.15 s], NDCG = 
[ 0.75803464  0.74412662  0.73818886  0.73730305  0.74137186  0.74761782
  0.75558559  0.76545534  0.77632882  0.78866111]
Iteration 16 [145.3 s]: loss = 0.734343 [63.41 s], NDCG = 
[ 0.75841495  0.74410347  0.73778281  0.73747929  0.74060877  0.74708197
  0.75549372  0.76510457  0.77558549  0.78831192]
Iteration 17 [145.6 s]: loss = 0.732397 [63.18 s], NDCG = 
[ 0.7573005   0.74434266  0.73794447  0.73709085  0.7403847   0.74687717
  0.75554585  0.76498294  0.77571323  0.78834885]
Iteration 18 [146.4 s]: loss = 0.730024 [63.11 s], NDCG = 
[ 0.75714029  0.7443241   0.73826189  0.73783566  0.74094976  0.74716908
  0.75528187  0.76483012  0.7755205   0.78818409]
Iteration 19 [146.1 s]: loss = 0.728623 [63.28 s], NDCG = 
[ 0.75589803  0.74269475  0.73746696  0.7370715   0.74066377  0.74660421
  0.75489706  0.76442702  0.77533023  0.78796194]
Iteration 20 [146.8 s]: loss = 0.726822 [63.42 s], NDCG = 
[ 0.75751433  0.7438108   0.73710262  0.73668273  0.73986851  0.74657379
  0.75463784  0.76413186  0.77498024  0.78773833]
Iteration 21 [147.0 s]: loss = 0.725008 [63.17 s], NDCG = 
[ 0.75708005  0.74330775  0.73768669  0.73635951  0.74009526  0.74617352
  0.75445436  0.76428674  0.77498121  0.7878306 ]
Iteration 22 [146.8 s]: loss = 0.723252 [63.28 s], NDCG = 
[ 0.75671785  0.74393825  0.73772524  0.73672714  0.74070061  0.74669594
  0.75478744  0.7640925   0.77501184  0.78768586]
Iteration 23 [147.1 s]: loss = 0.720624 [63.42 s], NDCG = 
[ 0.75671715  0.74324553  0.73741728  0.73647441  0.74008836  0.74631679
  0.75400171  0.7637861   0.77464554  0.78764495]
Iteration 24 [148.2 s]: loss = 0.717737 [63.55 s], NDCG = 
[ 0.75769996  0.74302535  0.73737645  0.73636883  0.74007724  0.74605986
  0.75446225  0.76362176  0.77485836  0.78779734]
Iteration 25 [148.1 s]: loss = 0.715227 [63.21 s], NDCG = 
[ 0.75837037  0.74334016  0.73720323  0.73727753  0.74061136  0.74646653
  0.75428895  0.76389813  0.77485847  0.78765309]
Iteration 26 [148.1 s]: loss = 0.712883 [63.43 s], NDCG = 
[ 0.75709572  0.74304831  0.73704557  0.73654142  0.73991829  0.74561473
  0.75360315  0.7632778   0.77407073  0.78710863]

