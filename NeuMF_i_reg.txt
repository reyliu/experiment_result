liurui@ubuntu:~/NeuMF_Regression$ KERAS_BACKEND=theano python MLP.py --learner adam --reg_layers [0,0,0,0] --lr 0.0001
Using Theano backend.
MLP arguments: Namespace(batch_size=256, batch_size_random=0, dataset='yelp', epochs=100, layers='[64,32,16,8]', learner='adam', lr=0.0001, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [4.2 s]. #user=12922, #item=13679, #train=323348, #test=316795
Init: NDCG = 
[ 0.54211442  0.5500685   0.55737551  0.56872839  0.58029974  0.59340186
  0.60784154  0.62296204  0.6391779   0.65673122]
Iteration 0 [15.2 s]: loss = 7.2962 [18.1 s], NDCG = 
[ 0.58142346  0.57886762  0.58321692  0.59134467  0.60101868  0.61238094
  0.62597157  0.63994755  0.65557399  0.67216497]
Iteration 1 [13.5 s]: loss = 1.1254 [18.2 s], NDCG = 
[ 0.67427361  0.66954342  0.67074511  0.67573306  0.68233316  0.69143805
  0.70096289  0.71261706  0.72487421  0.73793229]
Iteration 2 [13.5 s]: loss = 1.0044 [18.8 s], NDCG = 
[ 0.69589245  0.68857446  0.68787576  0.69068238  0.69704123  0.70532928
  0.71472103  0.72562508  0.7370129   0.74962532]
Iteration 3 [13.6 s]: loss = 0.9785 [19.1 s], NDCG = 
[ 0.70250747  0.69415524  0.69208985  0.69462355  0.70020167  0.70817614
  0.71769919  0.72854909  0.73971263  0.751937  ]
Iteration 4 [13.4 s]: loss = 0.9692 [18.0 s], NDCG = 
[ 0.70331272  0.69375845  0.6921432   0.69521643  0.70094796  0.70944236
  0.71879661  0.72915453  0.74019464  0.75233208]
Iteration 5 [13.5 s]: loss = 0.9638 [18.6 s], NDCG = 
[ 0.70336638  0.69375427  0.69271282  0.69517165  0.70114937  0.70887873
  0.71826552  0.72891847  0.74030352  0.75220477]
Iteration 6 [13.4 s]: loss = 0.9604 [18.1 s], NDCG = 
[ 0.704078    0.6945381   0.69281365  0.69615548  0.70188454  0.70968296
  0.71860921  0.72921462  0.74040767  0.75261085]
Iteration 7 [13.8 s]: loss = 0.9576 [18.1 s], NDCG = 
[ 0.70354091  0.69509136  0.69347482  0.6966802   0.70179649  0.7098167
  0.71922645  0.72974376  0.74062767  0.75275504]
Iteration 8 [13.6 s]: loss = 0.9547 [18.6 s], NDCG = 
[ 0.70478101  0.6944391   0.69320663  0.69622542  0.70165053  0.7093292
  0.71926294  0.72952976  0.74061677  0.75287968]
Iteration 9 [13.8 s]: loss = 0.9522 [18.7 s], NDCG = 
[ 0.703203    0.69524475  0.69316019  0.69569281  0.70162804  0.70962055
  0.71916171  0.72934103  0.74054346  0.75288077]
Iteration 10 [13.7 s]: loss = 0.9493 [18.6 s], NDCG = 
[ 0.70550739  0.6957384   0.69339796  0.69652281  0.70197796  0.70988618
  0.71948931  0.72986473  0.74095274  0.75292703]
Iteration 11 [13.5 s]: loss = 0.9463 [17.9 s], NDCG = 
[ 0.70468813  0.6954828   0.69349081  0.69624885  0.70182095  0.71010068
  0.71940484  0.73001198  0.74081719  0.75294348]
Iteration 12 [13.6 s]: loss = 0.9417 [18.1 s], NDCG = 
[ 0.70378101  0.69558119  0.69333069  0.69654085  0.7019033   0.71017256
  0.71966752  0.7299547   0.74119363  0.75317728]
Iteration 13 [13.6 s]: loss = 0.9351 [18.8 s], NDCG = 
[ 0.70478411  0.69650488  0.69417612  0.69677456  0.70229728  0.71032237
  0.71996042  0.73031461  0.74156536  0.75339532]
Iteration 14 [13.6 s]: loss = 0.9249 [18.8 s], NDCG = 
[ 0.70575933  0.6953797   0.69382251  0.6971025   0.70265784  0.71067667
  0.71958832  0.72997284  0.74151178  0.75340016]
Iteration 15 [14.0 s]: loss = 0.9105 [19.0 s], NDCG = 
[ 0.70562294  0.69548146  0.69379592  0.69624936  0.70244883  0.70986842
  0.71899473  0.72931805  0.74094225  0.75273153]

